## Что такое Kafka Connect?

**Kafka Connect** — это фреймворк, встроенный в Apache Kafka, предназначенный для упрощения интеграции Kafka с внешними системами, такими
как базы данных, файловые системы, облачные сервисы, системы хранения и другие. Он позволяет создавать надежные и масштабируемые **конвейеры
данных** (data pipelines) для потоковой обработки данных, минимизируя необходимость написания сложного кода для интеграции.

Kafka Connect решает задачу передачи данных между Kafka и другими системами, предоставляя готовую инфраструктуру для чтения (извлечения)
данных из источников (Source Connectors) и записи данных в целевые системы (Sink Connectors). Это делает его идеальным инструментом для
сценариев, где нужно, например, загружать данные из базы данных в Kafka или отправлять сообщения из Kafka в облачное хранилище.

**Пример сценария**:

- Вы хотите в реальном времени собирать данные из базы данных MySQL (например, новые заказы в интернет-магазине) и отправлять их в Kafka для
  обработки аналитической системой.
- Или наоборот, вы хотите взять сообщения из Kafka и записать их в Elasticsearch для последующей визуализации в Kibana.

---

## Основные компоненты Kafka Connect

Kafka Connect состоит из нескольких ключевых компонентов, которые взаимодействуют для обеспечения передачи данных. Разберем каждый из них:

### 1. **Connectors (Коннекторы)**

Коннекторы — это плагины, которые определяют, как Kafka Connect взаимодействует с внешними системами. Они бывают двух типов:

- **Source Connectors** (Исходные коннекторы): Извлекают данные из внешних систем и отправляют их в топики Kafka.
    - Пример: JDBC Source Connector извлекает данные из базы данных (MySQL, PostgreSQL и т.д.) и публикует их в Kafka.
    - Реальный пример: Вы используете JDBC Source Connector для чтения новых строк из таблицы `orders` в MySQL и отправки их в
      топик `orders_topic`.

- **Sink Connectors** (Целевые коннекторы): Читают данные из топиков Kafka и отправляют их во внешние системы.
    - Пример: S3 Sink Connector записывает данные из Kafka в Amazon S3.
    - Реальный пример: Вы отправляете логи из топика `logs_topic` в Amazon S3 для долгосрочного хранения.

Коннекторы — это готовые или пользовательские реализации, которые можно найти в экосистеме Kafka (например, в Confluent Hub) или написать
самостоятельно, если требуется специфическая интеграция.

### 2. **Tasks (Задачи)**

Задачи — это единицы работы, которые выполняют фактическую передачу данных. Каждый коннектор разбивает свою работу на задачи, которые
выполняются параллельно для повышения производительности.

- **Source Tasks**: Извлекают данные из источника и отправляют их в Kafka.
- **Sink Tasks**: Читают данные из Kafka и отправляют их в целевую систему.
- Количество задач определяется конфигурацией коннектора (например, параметр `tasks.max`). Это позволяет масштабировать обработку данных.

**Пример**: Если у вас есть JDBC Source Connector, который читает данные из очень большой таблицы, вы можете настроить 4
задачи (`tasks.max=4`), чтобы параллельно читать разные партиции таблицы, что ускорит процесс.

### 3. **Workers (Работники)**

Работники — это процессы, которые запускают коннекторы и задачи. Они управляют выполнением и координацией работы.

- **Standalone Mode** (Одиночный режим):
    - Один процесс выполняет все коннекторы и задачи.
    - Подходит для тестирования, разработки или небольших систем.
    - Недостаток: нет отказоустойчивости и масштабируемости.
    - Пример: Вы запускаете Kafka Connect на локальной машине для тестирования передачи данных из файла в топик Kafka.

- **Distributed Mode** (Распределенный режим):
    - Несколько процессов (воркеров) работают вместе в кластере.
    - Подходит для продакшн-сред с высокой нагрузкой.
    - Преимущества: отказоустойчивость (если один воркер падает, другие продолжают работу) и масштабируемость (можно добавлять новые
      воркеры).
    - Пример: В кластере из трех воркеров вы распределяете задачи для обработки данных из 10 баз данных, обеспечивая высокую
      производительность и надежность.

Воркеры обмениваются информацией о состоянии через внутренние топики
Kafka (`config.storage.topic`, `offset.storage.topic`, `status.storage.topic`), что позволяет координировать работу в распределенном режиме.

### 4. **Converters (Конвертеры)**

Конвертеры отвечают за преобразование данных между форматами, чтобы Kafka Connect мог работать с различными типами данных. Например, данные
могут храниться в Kafka в виде JSON, Avro или простых строк.

- **Популярные конвертеры**:
    - **JsonConverter**: Для работы с JSON-данными.
    - **AvroConverter**: Для работы с данными в формате Apache Avro (рекомендуется для продакшн, так как Avro компактнее и поддерживает
      схемы).
    - **StringConverter**: Для работы с простыми строками.

**Пример**: Если вы используете AvroConverter с Confluent Schema Registry, Kafka Connect автоматически проверяет совместимость схем данных,
что предотвращает ошибки при изменении структуры данных.

### 5. **Transformations (Трансформации)**

Single Message Transforms (SMT) — это легковесные преобразования данных, которые можно применять к сообщениям на лету, без необходимости
писать сложный код. Трансформации выполняются в рамках коннектора, до или после передачи данных.

- **Примеры SMT**:
    - **Filter**: Удаляет сообщения, не соответствующие условиям (например, пропускать записи, где поле `status` равно `inactive`).
    - **Cast**: Изменяет тип данных поля (например, преобразует строку в число).
    - **ReplaceField**: Переименовывает или удаляет поля в сообщении.
    - **TimestampConverter**: Преобразует временные метки между форматами.

**Пример**: Вы используете JDBC Source Connector для чтения данных из MySQL, но хотите исключить поле `internal_id` из сообщений,
отправляемых в Kafka. Для этого вы добавляете SMT `ReplaceField` с параметром `exclude=internal_id`.

---

## Режимы работы Kafka Connect

Kafka Connect поддерживает два режима работы, которые определяют, как воркеры и задачи взаимодействуют:

### 1. **Standalone Mode (Одиночный режим)**

- **Описание**: Все коннекторы и задачи выполняются в одном процессе.
- **Когда использовать**:
    - Тестирование или разработка.
    - Небольшие системы с низкой нагрузкой.
    - Сценарии, где не требуется высокая отказоустойчивость.
- **Плюсы**:
    - Простота настройки.
    - Не требует дополнительных ресурсов.
- **Минусы**:
    - Нет отказоустойчивости: если процесс падает, работа останавливается.
    - Ограниченная масштабируемость.
- **Пример конфигурации**:
  ```properties
  bootstrap.servers=localhost:9092
  key.converter=org.apache.kafka.connect.json.JsonConverter
  value.converter=org.apache.kafka.connect.json.JsonConverter
  ```
  Вы запускаете процесс Kafka Connect с файлом конфигурации, указывающим на локальный брокер Kafka.

### 2. **Distributed Mode (Распределенный режим)**

- **Описание**: Несколько воркеров работают вместе, обмениваясь состоянием через внутренние топики Kafka. Это кластерный подход, где задачи
  автоматически распределяются между воркерами.
- **Когда использовать**:
    - Продакшн-среды с высокой нагрузкой.
    - Сценарии, где важна отказоустойчивость и масштабируемость.
- **Плюсы**:
    - Отказоустойчивость: если один воркер выходит из строя, задачи перераспределяются.
    - Масштабируемость: можно добавлять новые воркеры для увеличения производительности.
    - Централизованное управление через REST API.
- **Минусы**:
    - Требует настройки внутренних топиков Kafka.
    - Более сложная инфраструктура.
- **Пример конфигурации**:
  ```properties
  bootstrap.servers=broker1:9092,broker2:9092
  group.id=connect-cluster
  key.converter=io.confluent.connect.avro.AvroConverter
  value.converter=io.confluent.connect.avro.AvroConverter
  config.storage.topic=connect-configs
  offset.storage.topic=connect-offsets
  status.storage.topic=connect-status
  ```
  В этой конфигурации воркеры используют топики Kafka для хранения конфигураций, смещений и статуса задач.

---

## Как работает Kafka Connect: Пример потока данных

Чтобы лучше понять, как Kafka Connect работает, рассмотрим пример:

1. **Сценарий**: Вы хотите передавать данные из PostgreSQL (таблица `users`) в Elasticsearch.
2. **Шаги**:

- **Source Connector**: JDBC Source Connector настроен для чтения новых строк из таблицы `users` (используя инкрементальный столбец,
  например, `last_updated`).
- **Tasks**: Коннектор разбивает работу на 2 задачи, каждая из которых читает часть данных.
- **Converter**: Данные преобразуются в Avro для компактности и поддержки схем.
- **Transformations**: Применяется SMT `Filter`, чтобы исключить пользователей с `status=inactive`.
- **Kafka**: Данные отправляются в топик `users_topic`.
- **Sink Connector**: Elasticsearch Sink Connector читает данные из `users_topic` и записывает их в индекс Elasticsearch.
- **Workers**: В распределенном режиме два воркера обрабатывают задачи для Source и Sink коннекторов.

---

## Что такое коннекторы в Kafka Connect?

Коннекторы — это плагины в Kafka Connect, которые определяют, как данные передаются между Apache Kafka и внешними системами. Они являются
основным строительным блоком для создания потоковых конвейеров данных (data pipelines). Коннекторы делятся на два типа:

1. **Source Connectors**: Извлекают данные из внешних систем (например, баз данных, файловых систем, API) и отправляют их в топики Kafka.
2. **Sink Connectors**: Читают данные из топиков Kafka и записывают их во внешние системы (например, базы данных, облачные хранилища,
   аналитические платформы).

Коннекторы работают в связке с **задачами** (Tasks), которые выполняют фактическую передачу данных, и управляются **воркерами** (Workers) в
Standalone или Distributed режимах. Kafka Connect предоставляет множество готовых коннекторов, но также поддерживает создание кастомных
коннекторов для специфических нужд.

---

## 1. Source Connectors

**Source Connectors** отвечают за извлечение данных из внешних систем и их отправку в топики Kafka. Они идеально подходят для сценариев, где
нужно в реальном времени собирать данные из источников, таких как базы данных, файлы, системы очередей или API.

### Как работают Source Connectors?

- Source Connector подключается к источнику данных (например, базе данных).
- Он разбивает работу на задачи (Tasks), которые параллельно извлекают данные.
- Данные преобразуются (с помощью Converter) в формат, совместимый с Kafka (например, JSON или Avro), и отправляются в указанный топик.

### Примеры Source Connectors:

1. **JDBC Source Connector**:

- **Что делает**: Читает данные из реляционных баз данных (MySQL, PostgreSQL, Oracle и т.д.) через JDBC.
- **Сценарий**: Вы хотите отправлять новые записи из таблицы `orders` в MySQL в топик Kafka `orders_topic` для последующей обработки.
- **Пример конфигурации**:
  ```json
  {
    "name": "jdbc-source-connector",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
      "tasks.max": "2",
      "connection.url": "jdbc:mysql://localhost:3306/mydb",
      "connection.user": "user",
      "connection.password": "pass",
      "table.whitelist": "orders",
      "mode": "timestamp",
      "timestamp.column.name": "updated_at",
      "topic.prefix": "orders_"
    }
  }
  ```
- **Режимы работы**:
    - `incrementing`: Использует автоинкрементный столбец (например, `id`) для отслеживания новых записей.
    - `timestamp`: Использует столбец с временной меткой (например, `updated_at`) для чтения обновленных записей.
    - `bulk`: Полностью читает таблицу (подходит для небольших таблиц).

2. **Debezium Source Connector**:

- **Что делает**: Использует технологию Change Data Capture (CDC) для отслеживания изменений в базах данных (добавление, обновление,
  удаление строк) и отправки их в Kafka.
- **Сценарий**: Вы отслеживаете изменения в таблице `customers` в PostgreSQL, чтобы в реальном времени обновлять данные в аналитической
  системе.
- **Особенности**: Debezium работает с журналами транзакций базы данных (например, WAL в PostgreSQL), что делает его высокоэффективным для
  потоковой обработки.
- **Пример конфигурации**:
  ```json
  {
    "name": "debezium-postgres-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "tasks.max": "1",
      "database.hostname": "localhost",
      "database.port": "5432",
      "database.user": "user",
      "database.password": "pass",
      "database.dbname": "mydb",
      "table.whitelist": "public.customers",
      "topic.prefix": "dbserver1"
    }
  }
  ```

### Когда использовать Source Connectors?

- Для интеграции данных из баз данных, файлов, API или других систем в Kafka.
- Для потоковой обработки событий в реальном времени (например, CDC для баз данных).
- Для создания основы для аналитики или ETL-процессов.

---

## 2. Sink Connectors

**Sink Connectors** читают данные из топиков Kafka и записывают их во внешние системы, такие как базы данных, облачные хранилища, системы
аналитики или очереди сообщений.

### Как работают Sink Connectors?

- Sink Connector читает сообщения из указанного топика Kafka.
- Задачи (Tasks) распределяют работу по записи данных в целевую систему.
- Данные преобразуются из формата Kafka (например, Avro) в формат, подходящий для целевой системы.

### Примеры Sink Connectors:

1. **S3 Sink Connector**:

- **Что делает**: Записывает данные из топиков Kafka в Amazon S3 в виде файлов (например, JSON, Parquet).
- **Сценарий**: Вы хотите архивировать данные из топика `logs_topic` в S3 для долгосрочного хранения или анализа в Amazon Athena.
- **Пример конфигурации**:
  ```json
  {
    "name": "s3-sink-connector",
    "config": {
      "connector.class": "io.confluent.connect.s3.S3SinkConnector",
      "tasks.max": "3",
      "topics": "logs_topic",
      "s3.bucket.name": "my-kafka-bucket",
      "s3.region": "us-east-1",
      "format.class": "io.confluent.connect.s3.format.parquet.ParquetFormat",
      "storage.class": "io.confluent.connect.s3.storage.S3Storage"
    }
  }
  ```

2. **Elasticsearch Sink Connector**:

- **Что делает**: Отправляет данные из Kafka в Elasticsearch для индексации и поиска.
- **Сценарий**: Вы отправляете логи из топика `logs_topic` в Elasticsearch для визуализации в Kibana.
- **Пример конфигурации**:
  ```json
  {
    "name": "elasticsearch-sink-connector",
    "config": {
      "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
      "tasks.max": "2",
      "topics": "logs_topic",
      "connection.url": "http://localhost:9200",
      "type.name": "log",
      "key.ignore": "true"
    }
  }
  ```

### Когда использовать Sink Connectors?

- Для отправки данных из Kafka в системы хранения (S3, HDFS).
- Для интеграции с аналитическими платформами (Elasticsearch, Snowflake).
- Для записи данных в базы данных или системы очередей.

---

## 3. Создание собственных коннекторов

Иногда готовых коннекторов недостаточно для специфических нужд, и требуется разработка **кастомного коннектора**. Kafka Connect
предоставляет API для создания собственных Source и Sink Connectors.

### Основы API Kafka Connect

Для создания коннектора нужно реализовать следующие классы:

1. **Connector**: Определяет конфигурацию и разбиение работы на задачи.

- Для Source: `SourceConnector`
- Для Sink: `SinkConnector`

2. **Task**: Выполняет фактическую работу по извлечению или записи данных.

- Для Source: `SourceTask`
- Для Sink: `SinkTask`

3. **ConfigDef**: Определяет параметры конфигурации коннектора (например, URL источника, имя топика).

### Шаги для создания кастомного коннектора:

1. **Определите цель**:

- Например, Source Connector для чтения данных из REST API или Sink Connector для записи в кастомную базу данных.

2. **Создайте класс Connector**:

- Реализуйте методы:
    - `start(Map<String, String> props)`: Инициализация коннектора с конфигурацией.
    - `taskClass()`: Возвращает класс задачи (SourceTask или SinkTask).
    - `taskConfigs(int maxTasks)`: Разбивает работу на задачи.
    - `stop()`: Останавливает коннектор.
    - `config()`: Определяет параметры конфигурации.

3. **Создайте класс Task**:

- Для SourceTask:
    - `start(Map<String, String> props)`: Инициализация задачи.
    - `poll()`: Извлекает данные из источника и возвращает список `SourceRecord`.
    - `stop()`: Останавливает задачу.
- Для SinkTask:
    - `start(Map<String, String> props)`: Инициализация задачи.
    - `put(Collection<SinkRecord> records)`: Записывает данные в целевую систему.
    - `flush(Map<TopicPartition, OffsetAndMetadata> offsets)`: Подтверждает запись.
    - `stop()`: Останавливает задачу.

4. **Упакуйте коннектор**:

- Скомпилируйте код в JAR-файл.
- Поместите JAR в директорию плагинов Kafka Connect (указанную в `plugin.path`).

5. **Настройте и запустите**:

- Используйте REST API Kafka Connect для добавления коннектора с вашей конфигурацией.

### Пример: Кастомный Source Connector

Предположим, вы хотите создать Source Connector для чтения данных из REST API, возвращающего JSON.

```java
import org.apache.kafka.connect.source.SourceConnector;
import org.apache.kafka.connect.source.SourceTask;
import java.util.List;
import java.util.Map;

public class RestApiSourceConnector extends SourceConnector {

  @Override
  public String version() {
    return "1.0";
  }

  @Override
  public void start(Map<String, String> props) {
    // Инициализация: сохранение конфигурации (например, URL API)
  }

  @Override
  public Class<? extends Task> taskClass() {
    return RestApiSourceTask.class;
  }

  @Override
  public List<Map<String, String>> taskConfigs(int maxTasks) {
    // Разбиение работы на задачи
    return Collections.singletonList(props);
  }

  @Override
  public void stop() {
    // Освобождение ресурсов
  }

  @Override
  public ConfigDef config() {
    return new ConfigDef()
        .define("api.url", ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, "URL of the REST API")
        .define("topic", ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, "Kafka topic to write to");
  }
}
```

### Когда создавать кастомный коннектор?

- Если нет готового коннектора для вашей системы (например, специфический API или проприетарная база данных).
- Если требуется сложная логика обработки данных, недоступная через SMT.
- Если вы хотите интегрировать Kafka Connect с внутренней системой компании.

---

## 4. Популярные коннекторы

Kafka Connect имеет богатую экосистему готовых коннекторов, многие из которых разработаны Confluent или сообществом. Рассмотрим самые
популярные:

### Confluent Community Connectors

Confluent поддерживает множество коннекторов, доступных через **Confluent Hub**. Примеры:

- **JDBC Connector**:
    - **Тип**: Source и Sink.
    - **Применение**: Интеграция с реляционными базами данных (MySQL, PostgreSQL, SQL Server и т.д.).
    - **Особенности**: Поддерживает инкрементное чтение, bulk-режим и запись в таблицы.
- **S3 Connector**:
    - **Тип**: Sink.
    - **Применение**: Хранение данных в Amazon S3 в форматах JSON, Avro, Parquet.
    - **Особенности**: Поддерживает партиционирование данных по времени или другим ключам.
- **HDFS Connector**:
    - **Тип**: Sink.
    - **Применение**: Запись данных в Hadoop HDFS.
    - **Особенности**: Подходит для интеграции с экосистемой Hadoop (Hive, Spark).

### Debezium

- **Тип**: Source.
- **Что делает**: Реализует Change Data Capture (CDC) для баз данных, таких как MySQL, PostgreSQL, MongoDB, Oracle.
- **Применение**: Отслеживание изменений в базе данных (вставки, обновления, удаления) и отправка их в Kafka в виде событий.
- **Особенности**:
    - Высокая производительность за счет чтения журналов транзакций.
    - Поддержка схем через Confluent Schema Registry.
    - Пример: Отслеживание изменений в таблице `products` для синхронизации с витриной данных.

### MirrorMaker

- **Тип**: Source и Sink.
- **Что делает**: Реплицирует данные между кластерами Kafka.
- **Применение**: Используется для создания географически распределенных систем или резервного копирования данных.
- **Особенности**:
    - MirrorMaker 2.0 (MM2) улучшает оригинальный MirrorMaker, добавляя поддержку синхронизации топиков, партиций и оффсетов.
    - Пример: Репликация топика `events` из кластера в дата-центре A в кластер в дата-центре B.

### Где найти коннекторы?

- **Confluent Hub**: Центральный репозиторий с сотнями коннекторов (https://www.confluent.io/hub/).
- **GitHub**: Многие коннекторы, включая кастомные, доступны в открытых репозиториях.
- **Maven Central**: Для добавления зависимостей в кастомные проекты.

