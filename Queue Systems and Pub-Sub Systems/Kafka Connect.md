### 1. **Основы и архитектура**
- **Что такое Kafka Connect**: Это фреймворк для передачи данных между Kafka и внешними системами. Он позволяет создавать конвейеры данных (data pipelines) для потоковой обработки.
- **Компоненты**:
    - **Connectors**: Плагины, которые определяют, как данные извлекаются (Source Connectors) или записываются (Sink Connectors) в Kafka.
    - **Tasks**: Единицы работы, выполняющие фактическую передачу данных.
    - **Workers**: Процессы, которые запускают коннекторы и задачи. Работают в standalone или distributed режимах.
    - **Converter**: Преобразует данные между форматами (например, JSON, Avro).
    - **Transformations**: Простые преобразования данных (Single Message Transforms, SMT), например, фильтрация или изменение структуры.
- **Режимы работы**:
    - **Standalone**: Один процесс для небольших систем или тестирования.
    - **Distributed**: Масштабируемый кластер для продакшн-сред.

### 2. **Коннекторы**
- **Source Connectors**: Извлекают данные из внешних систем (например, JDBC для баз данных, Debezium для CDC).
- **Sink Connectors**: Записывают данные из Kafka во внешние системы (например, S3, Elasticsearch).
- **Создание собственных коннекторов**: Понимание API Kafka Connect для разработки кастомных коннекторов (SourceConnector, SinkConnector, Task).
- **Популярные коннекторы**:
    - Confluent Community Connectors (например, JDBC, S3, HDFS).
    - Debezium для Change Data Capture (CDC).
    - MirrorMaker для репликации данных между кластерами Kafka.

### 3. **Конфигурация**
- **Конфигурация Worker’ов**:
    - `bootstrap.servers`: Подключение к Kafka.
    - `key.converter` и `value.converter`: Форматы данных (JSON, Avro, Protobuf).
    - `offset.storage.topic`: Топик для хранения смещений задач.
- **Конфигурация коннекторов**:
    - Параметры, специфичные для каждого коннектора (например, `table.whitelist` для JDBC).
    - Настройка производительности: `tasks.max`, `batch.size`.
- **Тюнинг производительности**:
    - Балансировка задач между воркерами.
    - Настройка количества партиций и реплик в Kafka.

### 4. **Обработка данных**
- **Single Message Transforms (SMT)**:
    - Примеры: `InsertField`, `ReplaceField`, `TimestampConverter`.
    - Как комбинировать трансформации для обработки данных на лету.
- **Схемы данных**:
    - Работа с Avro и Schema Registry для строгой типизации.
    - Обработка эволюции схем (schema evolution).
- **Обработка ошибок**:
    - Dead Letter Queues (DLQ) для проблемных сообщений.
    - Настройка `errors.tolerance` и `errors.log.enable`.

    
### 10. **Сценарии использования**
- **ETL-пайплайны**: Потоковая передача данных из баз данных в аналитические системы.
- **Логирование**: Передача логов в Elasticsearch или S3.
- **Репликация данных**: Синхронизация баз данных через Debezium.
- **Интеграция IoT**: Обработка потоков данных от устройств.
!