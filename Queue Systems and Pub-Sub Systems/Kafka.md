Apache Kafka — это распределенная стриминговая платформа, спроектированная для построения высокопроизводительных конвейеров данных,
потоковой аналитики и систем интеграции. В основе ее архитектуры лежит модель распределенного, реплицируемого лога коммитов (distributed
replicated commit log). Рассмотрим ее компоненты и механизмы в деталях.

#### Кластер и его компоненты: Broker, Producer, Consumer

* **Broker (Брокер):** Это серверный узел Kafka. Множество брокеров формируют **Kafka-кластер**. Каждый брокер является независимым
  процессом и управляет подмножеством партиций. Брокеры отвечают за прием записей от продюсеров, их сохранение на диск и обслуживание
  запросов на чтение от консьюмеров. Координация внутри кластера (отслеживание состояния брокеров, выбор контроллера, хранение метаданных,
  таких как конфигурации топиков и ACL) исторически осуществлялась с помощью Apache ZooKeeper. В современных версиях Kafka переходит на
  собственный протокол кворума на базе Raft (KRaft), избавляясь от внешней зависимости.

* **Producer (Продюсер):** Клиентское приложение, которое публикует записи (records) в топики Kafka.
    * **Жизненный цикл:** Продюсер создает объект `ProducerRecord`, который содержит имя топика, ключ (опционально), значение и заголовки (
      headers). Перед отправкой ключ и значение сериализуются в байтовые массивы с помощью указанных сериализаторов (
      например, `StringSerializer`, `AvroSerializer`).
    * **Механизм отправки:** Продюсер отправляет запросы на запись брокеру, который является **лидером** для целевой партиции.
    * **Гарантии доставки и конфигурация `acks`:**
        * `acks=0`: Продюсер не ждет подтверждения от брокера. Максимальная производительность, но возможна потеря данных.
        * `acks=1`: Продюсер ждет подтверждения только от лидера партиции. Данные не потеряются, если лидер не упадет сразу после
          подтверждения, но до репликации. Это компромисс по умолчанию.
        * `acks=all` (или `-1`): Продюсер ждет подтверждения от лидера и всех реплик из набора ISR (In-Sync Replicas). Максимальная гарантия
          сохранности данных, но самая высокая задержка.
    * **Идемпотентность:** При включении (`enable.idempotence=true`) продюсер гарантирует, что сообщения не будут дублироваться в случае
      повторных отправок из-за сетевых ошибок.

* **Consumer (Консьюмер):** Клиентское приложение, которое подписывается на один или несколько топиков и считывает из них записи.
    * **Жизненный цикл:** Консьюмер работает в цикле опроса (`poll loop`). Метод `consumer.poll()` запрашивает у брокера пачку записей с
      последнего коммиченного офсета. Получив записи, приложение их обрабатывает.
    * **Управление офсетами:** Консьюмер должен "коммитить" (сохранять) офсеты обработанных сообщений. Это сообщает Kafka, с какого места
      продолжать чтение в следующий раз. Существует два основных способа:
        * **Auto-commit (`enable.auto.commit=true`):** Консьюмер автоматически коммитит офсеты через заданный
          интервал (`auto.commit.interval.ms`). Это просто, но может привести либо к потере сообщений (если приложение упадет после коммита,
          но до обработки), либо к дублированию (если обработка завершилась, но приложение упало до коммита). Гарантия "at-most-once" или "
          at-least-once".
        * **Manual commit:** Приложение явно вызывает `consumer.commitSync()` или `consumer.commitAsync()`. Это дает полный контроль над
          процессом и позволяет реализовать семантику "at-least-once" более надежно и даже "exactly-once" в связке с транзакционными
          системами.

#### Структура данных: Topic, Partition, Offset, Record

* **Topic (Топик):** Логическое имя для потока записей. Физически топик представляет собой набор партиций.

* **Partition (Партиция):** Основная единица параллелизма и хранения в Kafka.
    * **Структура:** Каждая партиция является упорядоченным, неизменяемым (immutable) логом. Записи в него добавляются строго в конец (
      append-only).
    * **Распределение данных:** Продюсер определяет, в какую партицию отправить запись:
        * **По ключу:** По умолчанию используется хэш от ключа: `hash(key) % numPartitions`. Это гарантирует, что все записи с одинаковым
          ключом попадут в одну и ту же партицию, сохраняя порядок для данной сущности.
        * **Без ключа:** Записи распределяются по партициям по принципу Round-robin (по кругу) для равномерной нагрузки.
    * **Репликация и отказоустойчивость:** Для каждой партиции в кластере выбирается один брокер-**лидер** (leader) и ноль или более
      брокеров-**последователей** (followers). Все запросы на чтение и запись для партиции проходят через лидера. Последователи асинхронно
      копируют данные с лидера. Набор реплик, которые не слишком отстали от лидера, называется **In-Sync Replicas (ISR)**. Если лидер
      падает, один из брокеров в ISR избирается новым лидером.

* **Offset (Смещение):** Уникальный, монотонно возрастающий `long` идентификатор для каждой записи внутри партиции. Консьюмер использует
  офсет для отслеживания своей позиции чтения.

* **Record (Запись):** Атомарная единица данных в Kafka. Состоит из:
    * `key` (byte[])
    * `value` (byte[])
    * `timestamp` (long)
    * `headers` (коллекция пар ключ-значение)

#### Масштабирование чтения: Consumer Group и Rebalancing

* **Consumer Group (Группа потребителей):** Набор консьюмеров, которые совместно читают данные из одного или нескольких топиков. Каждая
  группа имеет уникальный `group.id`.
    * **Принцип работы:** Kafka распределяет партиции топика между консьюмерами внутри одной группы. Ключевое правило: **одна партиция может
      быть назначена только одному консьюмеру из группы в один момент времени**. Если число консьюмеров превышает число партиций, лишние
      консьюмеры будут простаивать.
    * **Независимость групп:** Разные группы консьюмеров читают топики независимо друг от друга. Каждая группа хранит свой собственный набор
      офсетов.

* **Rebalancing (Ребалансировка):** Процесс перераспределения партиций между консьюмерами в группе. Он запускается, когда:
    * В группу добавляется новый консьюмер.
    * Консьюмер покидает группу (штатно завершает работу или "умирает" по таймауту сессии).
    * Изменяется количество партиций в топике, на который подписана группа.
    * **Механизм:** Процессом управляет брокер, называемый **Group Coordinator**. Когда происходит событие, триггерящее ребалансировку,
      координатор отзывает все партиции у всех членов группы. Затем, используя выбранную стратегию (`partition.assignment.strategy`), он
      переназначает партиции между активными членами. В классическом протоколе это приводило к полной остановке обработки ("
      stop-the-world"). Современные версии Kafka поддерживают **Incremental Cooperative Rebalancing**, который минимизирует паузы,
      перераспределяя только те партиции, которые были затронуты изменениями.

#### Фундаментальная концепция: The Log

Основополагающая идея Kafka — это не очередь, а **распределенный лог**. Это различие имеет глубокие технические последствия.

* **Append-only & Sequential I/O:** Записи только дописываются в конец файла лога на диске. Это соответствует модели последовательного
  доступа к диску (sequential I/O), которая на порядки быстрее случайного доступа (random I/O), используемого традиционными СУБД для
  обновления записей. Kafka активно использует **page cache** операционной системы для кеширования как операций записи, так и чтения, что
  минимизирует физические дисковые операции.
* **Immutability и Retention Policy:** Записи не удаляются после прочтения консьюмером. Они удаляются на основе **политики удержания (
  retention policy)**, которая может быть настроена по:
    * **Времени (`retention.ms`):** Удалять сегменты лога старше N миллисекунд.
    * **Размеру (`retention.bytes`):** Удалять старейшие сегменты, если общий размер лога партиции превышает N байт.
* **Log Compaction (Сжатие лога):**
  Это альтернативный механизм удержания данных. Вместо удаления всех старых записей, сжатие гарантирует, что для каждого уникального ключа в
  партиции будет сохранена как минимум последняя запись. Все предыдущие записи с тем же ключом со временем удаляются. Это идеально подходит
  для:
    * **Change Data Capture (CDC):** Потоки изменений из баз данных, где нас интересует только последнее состояние строки.
    * **Хранение состояний:** Например, текущие настройки пользователя или баланс счета. Топик превращается в своего рода материализованное
      представление (materialized view).

#### Репликация (Replication): Сердце отказоустойчивости Kafka

Ни один серьезный разговор о Kafka не обходится без репликации. Это механизм, который гарантирует, что ваши данные не исчезнут вместе с
падением одного или даже нескольких серверов.

* **Leader и Follower (Лидер и Последователь):**
  Каждая партиция в Kafka имеет одного **лидера** и ноль или более **последователей**. Это не демократия, а строгая иерархия. Абсолютно все
  операции записи и, по умолчанию, все операции чтения для конкретной партиции проходят через ее лидера. Роль последователей (followers)
  проста и критически важна: они пассивно копируют данные с лидера, стараясь не отставать. Если лидер выходит из строя, один из
  последователей будет избран новым лидером, и система продолжит работу.

* **In-Sync Replicas (ISR):**
  Это, возможно, самый важный концепт в обеспечении долговечности данных. ISR — это множество реплик (включая самого лидера), которые "идут
  в ногу" с лидером. "Не сильно отстали" — это настраиваемый параметр (`replica.lag.time.max.ms`). Если реплика-последователь не запрашивала
  новые данные у лидера дольше этого времени, она исключается из ISR.
  **Почему это критично?** Новый лидер может быть избран **только** из числа реплик, состоящих в ISR на момент падения старого лидера. Это
  гарантирует, что новый лидер обладает всеми сообщениями, которые были подтверждены клиентам.

* **Связка `acks` и `min.insync.replicas`:**
  Теперь соединим это с настройками продюсера. Параметр `acks` определяет, какого уровня подтверждения ждет продюсер перед тем, как считать
  сообщение успешно отправленным.
    * `acks=0`: Продюсер отправляет сообщение и не ждет ответа. Максимальная производительность, но нет никаких гарантий. Сообщение может
      быть потеряно в сети или если брокер упадет в момент получения.
    * `acks=1`: Продюсер ждет подтверждения только от **лидера** партиции. Это хороший баланс: сообщение точно записано на один сервер.
      Однако, если лидер подтвердит запись, но упадет до того, как данные скопируются на последователей, данные будут потеряны.
    * `acks=all` (или `-1`): Продюсер ждет, пока лидер не получит подтверждение от **всех реплик, находящихся в ISR**. Это самый надежный
      режим. Запись считается успешной только тогда, когда она сохранена на нескольких машинах.

  А что если в ISR остался только сам лидер? Тогда `acks=all` превращается в `acks=1`. Чтобы этого не допустить, есть настройка на стороне
  брокера: `min.insync.replicas`. Она задает минимальное количество реплик в ISR, необходимое для приема записей с `acks=all`. Если вы
  установите `min.insync.replicas=2`, а в ISR останется только одна реплика, то продюсер с `acks=all` получит ошибку `NotEnoughReplicas`.
  Это ваша страховка от записи в не-отказоустойчивый кластер.

#### Хранение данных на диске: Эффективность в деталях

Kafka создавалась с расчетом на то, что данные хранятся на обычных, вращающихся дисках (HDD), и при этом достигается производительность,
сравнимая с системами в оперативной памяти. Как?

* **Log Segments (Лог-сегменты):**
  Партиция на диске — это не один гигантский файл, а директория с набором файлов-сегментов. Когда текущий сегмент достигает определенного
  размера (`segment.bytes`) или возраста (`segment.ms`), он закрывается для записи, и создается новый. Это гениально простое решение
  позволяет:
    1. **Эффективно применять политики удержания:** Чтобы удалить старые данные, Kafka не нужно искать их внутри файла. Она просто удаляет
       целиком старые файлы-сегменты.
    2. **Эффективно проводить сжатие лога (Log Compaction):** Очистка происходит посегментно, не затрагивая активный сегмент, в который идет
       запись.

* **Индексы:**
  Чтобы не сканировать многогигабайтный файл сегмента в поисках нужного сообщения, Kafka использует два типа индексов для каждой партиции:
    1. **Offset Index (`.index`):** Сопоставляет логический офсет сообщения с его физической позицией в файле сегмента.
    2. **Time Index (`.timeindex`):** Сопоставляет временную метку сообщения с офсетом.
       Когда консьюмер запрашивает данные с определенного офсета, Kafka по индексу быстро находит нужный сегмент и позицию в нем, начиная
       чтение оттуда.

* **Zero-Copy (Нулевое копирование):**
  Это ключевая оптимизация производительности на уровне ОС. Когда консьюмер запрашивает данные, Kafka может отдать их, практически не
  задействуя собственный процессор. Вместо стандартного пути "диск -> page cache ОС -> буфер приложения Kafka -> сокет", данные с помощью
  системного вызова `sendfile()` отправляются напрямую из **page cache** операционной системы в сетевой сокет. Это исключает два этапа
  копирования данных и переключение контекста, что кардинально снижает нагрузку на CPU и повышает пропускную способность.

#### Управление кластером: От ZooKeeper к KRaft

* **Роль ZooKeeper (историческая):**
  Долгое время Kafka зависела от внешнего сервиса консенсуса — Apache ZooKeeper. Он был хранилищем всех критически важных метаданных
  кластера: списка брокеров, конфигураций топиков и прав доступа (ACL), текущих лидеров для всех партиций, членов консьюмер-групп. ZK был
  надежен, но являлся отдельной сложной системой для развертывания и обслуживания, а также потенциальным узким местом в масштабировании
  кластера (особенно по количеству партиций).

* **KRaft (Kafka Raft): Будущее без ZooKeeper:**
  Современные версии Kafka внедряют режим KRaft. В этом режиме Kafka избавляется от зависимости от ZooKeeper. Роль хранилища метаданных
  берет на себя сама Kafka. Метаданные хранятся в специальном, высокореплицированном внутреннем топике (`__cluster_metadata`), а консенсус
  между брокерами для управления этими данными достигается с помощью протокола **Raft**. Это значительно упрощает архитектуру, развертывание
  и эксплуатацию Kafka, а также повышает ее масштабируемость и производительность.

* **Controller Broker (Брокер-контроллер):**
  В любом Kafka-кластере всегда есть один "главный" брокер, который называется **контроллером**. Он избирается из числа всех брокеров. Его
  задачи — быть "мозгом" кластера:
    * Управлять состоянием лидеров и последователей для всех партиций.
    * Инициировать выборы нового лидера, когда текущий становится недоступен.
    * Уведомлять другие брокеры об изменениях в кластере (например, о создании нового топика или падении брокера).


#### Святой Грааль: Гарантии доставки и Exactly-Once Semantics (EOS)

Любая распределенная система сталкивается с ненадёжностью сети. Это порождает три возможных сценария доставки сообщений:

* **At most once (не более одного раза):** Продюсер отправил сообщение (acks=0) и не ждет ответа от брокера. Если пакет потерялся, сообщение
  тоже.
  Быстро, но ненадежно.
* **At least once (не менее одного раза):** Продюсер отправляет сообщение и ждет подтверждения (acks=1 (по умолчанию) или acks=all, плюс
  retries > 0). Если подтверждение не пришло (например, из-за таймаута), он отправляет сообщение повторно. Это гарантирует доставку, но
  может порождать дубликаты.
* **Exactly once (строго один раз):** Сообщение доставляется ровно один раз, без потерь и дублей, даже в условиях сбоев.

Достижение EOS — сложнейшая задача в распределенных системах. В Kafka она реализована через два мощных механизма, работающих в тандеме:

1. **Идемпотентные продюсеры `enable.idempotence=true`:** (Автоматически устанавливает acks=all и retries в большое значение)
    1. **Producer ID (PID):** При первом подключении брокер присваивает продюсеру уникальный ID (PID).
    2. **Sequence Number:** Продюсер ведет счетчик (порядковый номер) для **каждой партиции**, в которую он пишет. Номер начинается с 0 и
       увеличивается с каждым новым сообщением.
    3. Каждое сообщение отправляется на брокер вместе с его PID и порядковым номером.
    4. Брокер хранит у себя последний полученный порядковый номер для каждого PID/партиции.
    5. **Логика на брокере:**
        * Если приходит сообщение с номером `N+1` после `N`, оно принимается.
        * Если приходит сообщение с номером `N`, когда `N` уже был обработан, брокер игнорирует это сообщение (понимая, что это дубликат от
          повторной отправки) и просто отправляет `ack`.
        * Если приходит сообщение с номером `N+2`, когда ожидался `N+1`, брокер вернет ошибку.
          **Ограничение:** Идемпотентность работает в рамках одной сессии продюсера и для одной партиции.

2. **Транзакции:**
   Это механизм, который расширяет идемпотентность на атомарную запись в **несколько партиций и топиков**.

    * **Конфигурация продюсера:** В дополнение к идемпотентности, указывается `transactional.id`. Это уникальный, стабильный идентификатор,
      который позволяет брокеру "узнать" продюсера даже после его перезапуска.
   * **Конфигурация консьюмера:** `isolation.level=read_committed`. Это заставляет консьюмера читать только те сообщения, которые являются
     частью успешно завершенной (скоммиченной) транзакции.
   * **Механизм:**
       1. **Инициализация:** Продюсер вызывает `producer.initTransactions()`.
       2. **Начало:** Продюсер вызывает `producer.beginTransaction()`.
       3. **Работа:** Продюсер выполняет свою логику:
           * Отправляет сообщения в топик `B`.
           * Отправляет сообщения в топик `C`.
           * Отправляет информацию о прочитанных офсетах из топика `A` в специальный топик `__consumer_offsets` (это тоже происходит в рамках
             транзакции).
             На этом этапе все отправленные сообщения физически записаны в логи, но помечены как "незакоммиченные" и не видны консьюмерам
             с `isolation.level=read_committed`.
       4. **Завершение (Commit):** Продюсер вызывает `producer.commitTransaction()`.
           * **Двухфазный коммит:** Специальный брокер (Transaction Coordinator) записывает в логи всех затронутых партиций специальные
             маркеры `COMMIT`.
           * Только после этого все сообщения транзакции становятся видимыми для консьюмеров.
       5. **Отмена (Abort):** Если в процессе работы происходит сбой или продюсер вызывает `producer.abortTransaction()`, Transaction
          Coordinator записывает маркеры `ABORT`. Консьюмеры, встретив такие маркеры, просто проигнорируют все сообщения внутри этой
          транзакции.

#### Zero-Copy 

**Zero-Copy** — это техника оптимизации в Kafka, которая позволяет передавать данные с диска напрямую в сеть, минуя память самого приложения (брокера Kafka). Это достигается с помощью системного вызова операционной системы, такого как `sendfile()` в Linux.

### Традиционный подход (4 копирования)

1.  **Диск → Память ядра ОС:** Данные копируются с диска в буфер ядра (page cache).
2.  **Память ядра → Память приложения:** Данные копируются из буфера ядра в память приложения (пространство пользователя, User Space).
3.  **Память приложения → Сетевой буфер ядра:** Данные копируются обратно в другой буфер ядра, связанный с сетевым сокетом.
4.  **Сетевой буфер ядра → Сеть:** Данные отправляются по сети.

**Проблема:** Шаги 2 и 3 — лишние. Они нагружают CPU и забивают память приложения (JVM heap), что вызывает частые и долгие паузы на сборку мусора (Garbage Collection).

### Подход Zero-Copy (2 копирования)

Kafka использует `sendfile()`, чтобы дать команду ядру ОС сделать все самостоятельно:

1.  **Диск → Память ядра ОС:** Данные копируются с диска в буфер ядра (page cache).
2.  **Память ядра ОС → Сеть:** Ядро напрямую копирует данные из своего буфера в сетевую карту для отправки.

**Ключевое отличие:** Данные **никогда не попадают в память приложения Kafka**.

### Главные выгоды Zero-Copy:

*   **Снижение нагрузки на CPU:** Исключаются два лишних копирования данных, которые выполнял бы процессор.
*   **Эффективное использование памяти:** Данные не попадают в JVM Heap, что резко снижает давление на сборщик мусора и предотвращает паузы в работе брокера.
*   **Высокая пропускная способность:** Позволяет Kafka обрабатывать трафик со скоростью, ограниченной только производительностью диска и сети.
* 