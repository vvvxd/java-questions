# Тестирование приложений Kafka Streams

Узнайте, как тестировать приложения Kafka Streams для создания надежных конвейеров потоковой обработки.

Тестирование приложений Kafka Streams имеет решающее значение для обеспечения их надежности, масштабируемости и корректности. Приложения Kafka Streams предназначены для обработки и анализа потоков данных в реальном времени, что критически важно для многих бизнес-операций. Без надлежащего тестирования могут возникнуть ошибки в логике обработки потоков, что приведет к расхождениям и потере данных. Тестирование помогает выявлять и устранять ошибки перед развертыванием приложения в производстве, а также гарантирует, что приложение может обрабатывать различные типы и объемы данных, выдавая точные и последовательные результаты.

## API тестирования Kafka Streams

Kafka Streams предоставляет тестовые утилиты для выполнения модульных тестов конвейеров потоковой обработки без использования внешнего или встроенного кластера Kafka. Ниже приведен обзор API тестирования Kafka Streams.

### Класс `TestInputTopic`

Класс `TestInputTopic` позволяет создавать фиктивную входную тему для модульного тестирования приложений Kafka Streams. Он используется для подачи входных данных в приложение, чтобы проверить его поведение в различных сценариях.

```java
// Пример использования TestInputTopic
TestInputTopic<String, String> inputTopic = driver.createInputTopic(INPUT_TOPIC, Serdes.String().serializer(), Serdes.String().serializer());
inputTopic.pipeInput("key1", "value1");
```

**Объяснение**:
- Экземпляр `TestInputTopic` представляет входную тему.
- Метод `pipeInput` позволяет отправлять записи в виде пар ключ-значение или списка.

### Класс `TestOutputTopic`

Класс `TestOutputTopic` создает фиктивную выходную тему для чтения и проверки выходных данных приложения Kafka Streams.

```java
// Пример использования TestOutputTopic
TestOutputTopic<String, String> outputTopic = driver.createOutputTopic(OUTPUT_TOPIC, Serdes.String().deserializer(), Serdes.String().deserializer());
List<String> outputValues = outputTopic.readValuesToList();
```

**Объяснение**:
- Экземпляр `TestOutputTopic` используется для потребления выходных данных с помощью методов, таких как `readValue` или `readValuesToList`.

### Класс `TopologyTestDriver`

Класс `TopologyTestDriver` упрощает тестирование топологий, созданных с помощью `Topology` или `StreamsBuilder`. Он работает без реального брокера Kafka, что обеспечивает быстрые тесты с минимальными накладными расходами.

```java
// Пример использования TopologyTestDriver
Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "test-app");
TopologyTestDriver driver = new TopologyTestDriver(topology, props);
```

**Объяснение**:
- Создает экземпляр `TopologyTestDriver` с заданной топологией и конфигурацией.
- Позволяет создавать `TestInputTopic` и `TestOutputTopic` для подачи входных данных и проверки выходных.

## Выполнение модульных тестов

Ниже приведены примеры кода для тестирования операций Kafka Streams, включая `filter`, `flatMap`, `count` и `count` с хранилищем состояний.

### KafkaStreamsOperations.java

```java
package com.example;

import java.util.Arrays;
import java.util.stream.Collectors;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.KeyValueMapper;
import org.apache.kafka.streams.kstream.Materialized;

public class KafkaStreamsOperations {

    static String INPUT_TOPIC = "input";
    static String OUTPUT_TOPIC = "output";
    static final String KAFKA_STREAMS_APP_ID = "kafka-streams-testing-app";
    
    static Topology filterTopology() {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> stream = builder.stream(INPUT_TOPIC);
        stream.filter((k, v) -> v.length() > 5).to(OUTPUT_TOPIC);
        return builder.build();
    }

    static Topology flatMapTopology() {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> stream = builder.stream(INPUT_TOPIC);
        stream.flatMap(new KeyValueMapper<String, String, Iterable<? extends KeyValue<? extends String, ? extends String>>>() {
            @Override
            public Iterable<? extends KeyValue<? extends String, ? extends String>> apply(String k, String csv) {
                String[] values = csv.split(",");
                return Arrays.asList(values)
                        .stream()
                        .map(value -> new KeyValue<>(k, value))
                        .collect(Collectors.toList());
            }
        }).to(OUTPUT_TOPIC);
        return builder.build();
    }

    static Topology countTopology() {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> stream = builder.stream(INPUT_TOPIC);
        stream.groupByKey()
                .count()
                .toStream()
                .to(OUTPUT_TOPIC);
        return builder.build();
    }

    static Topology countWithStateStoreTopology() {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> stream = builder.stream(INPUT_TOPIC);
        stream.groupByKey()
                .count(Materialized.as("count-store"));
        return builder.build();
    }
}
```

### KafkaStreamsOperationsTest.java

```java
package com.example;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.TopologyTestDriver;
import org.apache.kafka.streams.test.TestRecord;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import java.util.List;
import java.util.Properties;
import static org.junit.Assert.*;

public class KafkaStreamsOperationsTest {

    private TopologyTestDriver driver;
    private Properties props;

    @Before
    public void setUp() {
        props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, KafkaStreamsOperations.KAFKA_STREAMS_APP_ID);
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    }

    @After
    public void tearDown() {
        if (driver != null) {
            driver.close();
        }
    }

    @Test
    public void testFilterTopologyPass() {
        Topology topology = KafkaStreamsOperations.filterTopology();
        driver = new TopologyTestDriver(topology, props);
        var inputTopic = driver.createInputTopic(KafkaStreamsOperations.INPUT_TOPIC, Serdes.String().serializer(), Serdes.String().serializer());
        var outputTopic = driver.createOutputTopic(KafkaStreamsOperations.OUTPUT_TOPIC, Serdes.String().deserializer(), Serdes.String().deserializer());

        assertTrue(outputTopic.isEmpty());

        inputTopic.pipeInput("key1", "testing");
        assertEquals("testing", outputTopic.readValue());

        inputTopic.pipeInput("key2", "texas");
        assertTrue(outputTopic.isEmpty());
    }

    @Test
    public void testFlatMapTopologyPass() {
        Topology topology = KafkaStreamsOperations.flatMapTopology();
        driver = new TopologyTestDriver(topology, props);
        var inputTopic = driver.createInputTopic(KafkaStreamsOperations.INPUT_TOPIC, Serdes.String().serializer(), Serdes.String().serializer());
        var outputTopic = driver.createOutputTopic(KafkaStreamsOperations.OUTPUT_TOPIC, Serdes.String().deserializer(), Serdes.String().deserializer());

        inputTopic.pipeInput("key1", "value1,value2,value3");
        inputTopic.pipeInput("key2", "value4,value5,value6");

        List<String> outputValues = outputTopic.readValuesToList();
        assertEquals(6, outputValues.size());
        assertEquals("value1", outputValues.get(0));
        assertEquals("value2", outputValues.get(1));
        assertEquals("value3", outputValues.get(2));
        assertEquals("value4", outputValues.get(3));
        assertEquals("value5", outputValues.get(4));
        assertEquals("value6", outputValues.get(5));
        assertTrue(outputTopic.isEmpty());
    }

    @Test
    public void testCountTopologyPass() {
        Topology topology = KafkaStreamsOperations.countTopology();
        driver = new TopologyTestDriver(topology, props);
        var inputTopic = driver.createInputTopic(KafkaStreamsOperations.INPUT_TOPIC, Serdes.String().serializer(), Serdes.String().serializer());
        var outputTopic = driver.createOutputTopic(KafkaStreamsOperations.OUTPUT_TOPIC, Serdes.String().deserializer(), Serdes.Long().deserializer());

        inputTopic.pipeInput("key1", "value1");
        inputTopic.pipeInput("key2", "value2");
        inputTopic.pipeInput("key1", "value3");
        inputTopic.pipeInput("key2", "value4");
        inputTopic.pipeInput("key3", "value5");

        assertEquals(Long.valueOf(2), outputTopic.readKeyValue().value);
        assertEquals(Long.valueOf(2), outputTopic.readKeyValue().value);
        assertEquals(Long.valueOf(1), outputTopic.readKeyValue().value);
        assertTrue(outputTopic.isEmpty());
    }

    @Test
    public void testCountWithStateStoreTopologyPass() {
        Topology topology = KafkaStreamsOperations.countWithStateStoreTopology();
        driver = new TopologyTestDriver(topology, props);
        var inputTopic = driver.createInputTopic(KafkaStreamsOperations.INPUT_TOPIC, Serdes.String().serializer(), Serdes.String().serializer());
        var store = driver.getKeyValueStore("count-store");

        inputTopic.pipeInput("key1", "value1");
        inputTopic.pipeInput("key2", "value2");
        inputTopic.pipeInput("key1", "value3");

        assertEquals(Long.valueOf(2), store.get("key1"));
        assertEquals(Long.valueOf(1), store.get("key2"));
        assertNull(store.get("key3"));
    }
}
```

### Тестирование сценариев отказа

Каждая операция также имеет отрицательные тестовые случаи для проверки обработки ошибок.

#### Отрицательный тест для `filter`

```bash
# Отрицательный тестовый случай для filter
inputTopic.pipeInput("key1", "short");
assertTrue(outputTopic.isEmpty());
```

**Ожидаемая ошибка**:
- Тест подтверждает, что значения короче 6 символов не проходят фильтр.

#### Отрицательный тест для `flatMap`

```bash
# Отрицательный тестовый случай для flatMap
inputTopic.pipeInput("key1", "");
assertTrue(outputTopic.isEmpty());
```

**Ожидаемая ошибка**:
- Пустая строка не должна создавать выходные записи.

#### Отрицательный тест для `count`

```bash
# Отрицательный тестовый случай для count
inputTopic.pipeInput("key1", "value1");
assertNotEquals(Long.valueOf(2), outputTopic.readKeyValue().value);
```

**Ожидаемая ошибка**:
- Проверяет, что подсчет не равен неверному значению.

#### Отрицательный тест для `count` с хранилищем состояний

```bash
# Отрицательный тестовый случай для count с хранилищем
assertNull(store.get("nonexistent"));
```

**Ожидаемая ошибка**:
- Проверяет, что несуществующий ключ возвращает `null`.

## Код пояснения

### Тестирование `filter` операции

**Код операции (KafkaStreamsOperations.java)**:
- **Строки 18–22**: Метод `filterTopology` создает топологию, фильтрующую значения длиной более 5 символов.
- Используется `StreamsBuilder` для создания `KStream`, к которому применяется операция `filter`.

**Тест (KafkaStreamsOperationsTest.java)**:
- **Строки 41–57**: Тест `testFilterTopologyPass` проверяет, что значения длиннее 5 символов проходят, а короче — нет.
- Используется `TopologyTestDriver` для создания фиктивных тем ввода и вывода.

### Тестирование `flatMap` операции

**Код операции (KafkaStreamsOperations.java)**:
- **Строки 27–44**: Метод `flatMapTopology` разделяет строки, разделенные запятыми, на отдельные записи.
- Применяется `flatMap` с использованием `KeyValueMapper`.

**Тест (KafkaStreamsOperationsTest.java)**:
- **Строки 61–83**: Тест `testFlatMapTopologyPass` проверяет, что входные строки корректно разделяются на отдельные значения.
- Проверяется, что выходная тема содержит ожидаемое количество записей.

### Тестирование `count` операции

**Код операции (KafkaStreamsOperations.java)**:
- **Строки 46–54**: Метод `countTopology` подсчитывает количество записей для каждого ключа.
- Используется `groupByKey` и `count` с последующим выводом в тему.

**Тест (KafkaStreamsOperationsTest.java)**:
- **Строки 87–106**: Тест `testCountTopologyPass` проверяет правильность подсчетов для каждого ключа.

### Тестирование `count` с хранилищем состояний

**Код операции (KafkaStreamsOperations.java)**:
- **Строки 60–65**: Метод `countWithStateStoreTopology` сохраняет подсчеты в хранилище `count-store`.

**Тест (KafkaStreamsOperationsTest.java)**:
- **Строки 110–131**: Тест `testCountWithStateStoreTopologyPass` проверяет данные в хранилище состояний через `KeyValueStore`.

На этом уроке мы изучили ключевые API для тестирования приложений Kafka Streams и рассмотрели их применение на практических примерах.