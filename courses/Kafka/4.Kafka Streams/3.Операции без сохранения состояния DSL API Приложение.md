# Операции без сохранения состояния DSL API: Приложение

Давайте продолжим изучение операций Kafka Streams без сохранения состояния на практическом примере.

## Понять операции без сохранения состояния на примере

Давайте разберемся с операциями без сохранения состояния с помощью приложения, которое будет фильтровать слова на основе критерия и дополнительно преобразовывать результаты.

## Пример кода

Нажмите кнопку «Run» в виджете ниже. Это инициирует процесс сборки и запустит приложение Kafka Streams. После запуска приложения вы должны увидеть сообщение `started kafka streams app`.

После этого выполните действия, описанные ниже:

### KafkaStreamsDSLExample.java

```java
package com.example;

import java.util.Properties;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.common.serialization.Serdes;

public class KafkaStreamsDSLExample {

  public static void main(String[] args) {
    Properties config = new Properties();

    config.put(StreamsConfig.APPLICATION_ID_CONFIG, "dsl-api-demo");
    config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    config.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
    config.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

    StreamsBuilder builder = new StreamsBuilder();
    KStream<String, String> input = builder.stream("input-topic");

    // filter words with length > 5
    KStream<String, String> filtered = input.filter((k, v) -> k.length() > 5);

    // map - each value to upper case
    KStream<String, String> upperCased = filtered.mapValues(value -> value.toUpperCase());

    // terminal operation - peek (inspect each key and value)
    upperCased.peek((k, v) -> System.out.println("[peeked] k-v " + k + ", " + v));

    // terminal operation - to
    upperCased.to("output-topic");

    Topology topology = builder.build();
    KafkaStreams streams = new KafkaStreams(topology, config);

    Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

    System.out.println("started kafka streams app.....");

    streams.start();
  }
}
```

### Kafka Consumer CLI

Нажмите кнопку «+», чтобы открыть новую вкладку терминала, и введите команду для запуска CLI потребителя Kafka. Это будет ждать сообщений от `output-topic`:

```bash
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic output-topic
```

### Kafka Producer CLI

Нажмите кнопку «+», чтобы открыть новую вкладку терминала, и введите следующие команды для отправки сообщений с `input-topic` с помощью производителя консоли Kafka:

```bash
kafka-console-producer.sh --broker-list localhost:9092 --topic input-topic
```

Вы должны увидеть `>` приглашение. Вы можете ввести сообщения, которые хотите отправить в тему (нажмите клавишу «enter» после каждого сообщения, чтобы отправить его в Kafka). Введите сообщения, используя формат `key:value`. Например:

```text
user12345:hello
user1:world
user67890:example
```

### Вывод приложения Kafka Streams

Вернитесь к терминалу потребителя консоли, чтобы убедиться, что сообщения были получены. Вы должны увидеть вывод, аналогичный следующему:

```text
HELLO
EXAMPLE
```

Обратите внимание на следующее относительно сообщений в `output-topic`:

- Значения указаны заглавными буквами.
- Информация для `user1` исключается, так как длина ключа не превышает 5. Поэтому эта запись отфильтровывается.

## Код пояснения

Вот пошаговая разбивка кода:

1. **Строки 4–10**: Импортируем необходимые пакеты.

2. **Строки 15–20**: Создаем объект `Properties` с именем `config`, который содержит параметры конфигурации приложения Kafka Streams, включая идентификатор приложения, сервер начальной загрузки Kafka, а также ключ и значение по умолчанию `Serdes`.

3. **Строка 22**: Создаем объект `StreamsBuilder` для определения топологии приложения Kafka Streams.

4. **Строка 23**: Создаем объект `KStream`, вызывая метод `stream()` объекта `builder`, который считывает сообщения из входной темы Kafka с именем `input-topic`.

5. **Строки 26–35**: Следующие операции без сохранения состояния объединены в цепочку:
    - Вызываем метод `filter()` в потоке `input`, чтобы отфильтровать сообщения, в которых длина ключа меньше или равна 5.
    - Вызываем метод `mapValues()` в потоке `filtered` для преобразования значений в верхний регистр.
    - Вызываем метод `peek()` в потоке `upperCased`, чтобы вывести каждую пару ключ-значение перед ее отправкой в выходную тему Kafka.
    - Вызываем метод `to()` в потоке `upperCased`, чтобы отправить пары ключ-значение в выходную тему Kafka с именем `output-topic`.

6. **Строка 37**: Топология приложения Kafka Streams строится путем вызова метода `build()` на объекте `builder`.

7. **Строка 38**: Создаем новый объект `KafkaStreams` с именем `streams`, передавая объекты `topology` и `config` конструктору.

8. **Строка 40**: Создаем обработчик завершения работы, который вызывает метод `close()` объекта `streams` при завершении работы приложения.

9. **Строка 44**: Вызываем метод `start()` объекта `streams` для запуска приложения Kafka Streams.

В этом уроке мы продолжили обсуждение операций без сохранения состояния в API Kafka Streams DSL и продемонстрировали это на практическом примере.