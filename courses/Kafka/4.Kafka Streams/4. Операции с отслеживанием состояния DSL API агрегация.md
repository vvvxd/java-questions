# Операции с отслеживанием состояния DSL API: Агрегация

Узнайте, как Kafka Streams поддерживает операции агрегации в API DSL.

## Kafka Streams DSL API — операции с отслеживанием состояния

Как обсуждалось ранее, операции с сохранением состояния требуют, чтобы внутреннее состояние поддерживалось в нескольких записях данных. Обычно они используются для более сложных преобразований или агрегаций, которые требуют обработки данных с течением времени, а не на основе записи за записью. Операции с сохранением состояния важны, поскольку они позволяют разработчикам выполнять сложные вычисления над потоками данных, которым требуется контекст и историческая информация.

Вот некоторые из ключевых классов, которые поддерживают операции с отслеживанием состояния в API Kafka Streams DSL:

- **KGroupedStream**: Представляет группировку записей в потоке тем Kafka. Этот класс создается путем вызова метода `groupByKey` на экземпляре `KStream`.
- **KGroupedTable**: Абстракция перегруппированного потока журнала изменений из таблицы с первичным ключом, обычно на другом ключе группировки, чем исходный первичный ключ. Этот класс создается путем вызова метода `groupBy` на экземпляре `KTable`.
- **Materialized**: Предоставляет API-интерфейс Fluent builder для настройки хранилища состояний. Параметры конфигурации включают имя хранилища состояний, ключ и значения Serdes, период хранения и т. д.
- **Хранилища состояний**: Используются для сохранения промежуточных результатов операций с сохранением состояния, которым требуется доступ к ранее обработанным записям, таким как агрегации и объединения. Эти хранилища реализованы как хранилища ключей и значений и могут быть как в памяти, так и на диске. Они разделены и распределены по кластеру для обеспечения отказоустойчивости и масштабируемости.
- **Window**: Оконная обработка позволяет контролировать, как группировать записи, имеющие одинаковый ключ, для операций с отслеживанием состояния, таких как агрегации или объединения, в окна, которые отслеживаются по ключу записи.

## Агрегация

Агрегация — это мощный метод выполнения сложных вычислений над потоковыми данными. Он особенно полезен для приложений, которым требуется аналитика потоковых данных в реальном времени, например, для обнаружения мошенничества, анализа поведения пользователей и предиктивного обслуживания. В API Kafka Streams DSL агрегация — это категория операций с сохранением состояния, которые позволяют выполнять вычисления над потоком данных на основе ключа. Ее цель — создать сводку данных в потоке, которую можно использовать для дальнейшего анализа или для создания вывода в другом потоке или хранилище данных.

После того как записи сгруппированы по ключу с помощью методов `groupByKey` или `groupBy` (и представлены в `KGroupedStream` или `KGroupedTable`), их можно агрегировать с помощью одного из следующих методов: `aggregate`, `count` или `reduce`. Поскольку агрегации являются операциями на основе ключей, они всегда работают со значениями одного и того же ключа.

Давайте подробно рассмотрим агрегацию, рассмотрев каждый из этих методов.

### Функция `aggregate`

```java
// Пример использования функции aggregate
KStream<String, Long> source = builder.stream("input-topic");
KGroupedStream<String, Long> grouped = source.groupByKey();
KTable<String, Long> aggregated = grouped.aggregate(
    () -> 0L, // Начальное значение
    (key, value, aggregate) -> aggregate + value // Функция агрегации
);
```

**Объяснение**:
- Метод `aggregate` принимает два аргумента: начальное значение и функцию-агрегатор.
- Начальное значение используется в качестве отправной точки для операции агрегации, в то время как функция агрегации объединяет текущее значение с предыдущим агрегированным значением.
- В этом примере метод `aggregate` вызывается после группировки данных по ключу с использованием метода `groupByKey` на источнике `KStream`. Начальное значение устанавливается равным 0, а функция агрегации принимает три аргумента: текущий ключ, текущее значение и предыдущее агрегированное значение. Функция агрегации добавляет текущее значение к предыдущему агрегированному значению.
- Операция агрегации приводит к созданию `KTable` с теми же ключами, что и исходный поток, и агрегированными значениями.

```java
// Пример использования перегруженной версии функции aggregate
KStream<String, Long> source = builder.stream("input-topic");
KGroupedStream<String, Long> grouped = source.groupByKey();
KTable<String, Long> aggregated = grouped.aggregate(
    () -> 0L, // Начальное значение
    (key, value, aggregate) -> aggregate + value, // Функция агрегации
    Materialized.with(Serdes.String(), Serdes.Long()) // Хранилище состояний
);
```

**Объяснение**:
- Перегруженная версия метода `aggregate` принимает экземпляр `Materialized`, который используется для указания имени хранилища состояний и любых дополнительных параметров конфигурации.

### Метод `reduce`

Метод `reduce` в Kafka Streams — это операция с сохранением состояния, которая применяет функцию сокращения к значениям каждого ключа в потоке или таблице. Функция сокращения принимает два значения и возвращает одно значение, а результат используется для обновления состояния соответствующего ключа.

```java
// Пример операции reduce
KStream<String, Long> source = builder.stream("input-topic");
KGroupedStream<String, Long> grouped = source.groupByKey();
KTable<String, Long> reduced = grouped.reduce(
    (value1, value2) -> value1 + value2 // Функция сокращения
);
```

**Объяснение**:
- Здесь `reducer` — это функция, которая принимает два значения типа `V` и возвращает одно значение типа `V`. Результатом метода `reduce` является `KTable` с теми же ключами, что и исходный поток, и агрегированными значениями.

```java
// Пример перегруженной версии reduce
Theresulting `KTable` can be used for further analysis or output to another stream or data store.

```java
// Пример реализации max с использованием reduce
KStream<String, Long> source = builder.stream("input-topic");
KGroupedStream<String, Long> grouped = source.groupByKey();
KTable<String, Long> maxValues = grouped.reduce(
    (value1, value2) -> Math.max(value1, value2) // Функция для вычисления максимума
);
```

**Примечание**: Замените `Math.max` на `Math.min`, чтобы вычислить минимальное значение.

```java
// Пример реализации sum с использованием reduce
KStream<String, Long> source = builder.stream("input-topic");
KGroupedStream<String, Long> grouped = source.groupByKey();
KTable<String, Long> sumValues = grouped.reduce(
    (value1, value2) -> value1 + value2 // Функция для вычисления суммы
);
```

### Метод `count`

Подсчет — это распространенная операция в обработке потоков. Например, можно отслеживать количество просмотров страницы или кликов по ссылке.

```java
// Пример реализации count с использованием aggregate
KStream<String, String> source = builder.stream("input-topic");
KGroupedStream<String, String> grouped = source.groupByKey();
KTable<String, Long> counts = grouped.aggregate(
    () -> 0L, // Начальное значение
    (key, value, aggregate) -> aggregate + 1 // Увеличение счетчика
);
```

Однако более удобный способ — использовать метод `count`:

```java
// Пример метода count
KStream<String, String> source = builder.stream("input-topic");
KGroupedStream<String, String> grouped = source.groupByKey();
KTable<String, Long> counts = grouped.count(
    Materialized.with(Serdes.String(), Serdes.Long()) // Хранилище состояний
);
```

**Объяснение**:
- Метод `count` подсчитывает количество записей для каждого ключа в потоке или таблице. Результатом является `KTable`, содержащая обновленные записи с неизмененными ключами и значениями типа `Long`, представляющими последний (скользящий) счетчик для каждого ключа.

## Пример кода

Давайте разберемся с агрегацией на примере приложения, которое подсчитывает количество просмотров товара.

```java
package com.example;

import java.util.Properties;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KGroupedStream;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.KTable;
import org.apache.kafka.streams.kstream.Produced;
import org.apache.kafka.common.serialization.Serdes;

public class KafkaStreamsDSLStatefulOps {

  public static void main(String[] args) {

    Properties configurations = new Properties();
    configurations.put(StreamsConfig.APPLICATION_ID_CONFIG, "counts-app");
    configurations.put(StreamsConfig.APPLICATION_SERVER_CONFIG, "localhost:8080");
    configurations.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    configurations.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
    configurations.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());

    StreamsBuilder builder = new StreamsBuilder();
    KStream<String, String> source = builder.stream("input-topic");

    source.groupByKey()
          .count()
          .toStream()
          .to("output-topic", Produced.with(Serdes.String(), Serdes.Long()));

    KafkaStreams app = new KafkaStreams(builder.build(), configurations);

    app.start();
    System.out.println("Started kafka streams (counts) application");
  }
}
```

### Kafka Consumer CLI

Нажмите кнопку «+», чтобы открыть новую вкладку терминала, и введите команду для запуска Kafka Consumer CLI. Это будет ждать сообщений из темы `output-topic`:

```bash
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic output-topic
```

### Kafka Producer CLI

Нажмите кнопку «+», чтобы открыть новую вкладку терминала, и введите следующие команды для отправки сообщений в тему `input-topic` с помощью консольного продюсера Kafka:

```bash
kafka-console-producer.sh --broker-list localhost:9092 --topic input-topic
```

Вы должны увидеть `>` приглашение. Введите сообщения в формате `key:value`. Например:

```text
product1:view
product1:view
product2:view
```

### Вывод приложения Kafka Streams

Вернитесь к терминалу потребителя консоли, чтобы убедиться, что сообщения были получены. Вы должны увидеть вывод, похожий на следующий:

```text
product1:2
product2:1
```

## Код пояснения

Вот пошаговая разбивка кода:

1. **Строки 3–12**: Импортируем необходимые пакеты.
2. **Строки 18–23**: Создаем объект `Properties` с именем `configurations`, содержащий параметры конфигурации приложения Kafka Streams, включая идентификатор приложения, сервер Kafka bootstrap, ключ и значения по умолчанию `Serdes`.
3. **Строка 25**: Создаем объект `StreamsBuilder` для определения топологии приложения Kafka Streams.
4. **Строка 26**: Создаем объект `KStream`, вызывая метод `stream()` объекта `builder`, который считывает сообщения из входной темы Kafka с именем `input-topic`.
5. **Строки 28–31**: Выполняем следующие операции:
    - Вызываем метод `groupByKey()` для потока `source`, чтобы сгруппировать данные по ключу.
    - Вызываем метод `count()` для подсчета количества вхождений каждого ключа.
    - Преобразуем `KTable` (возвращаемый `count`) в `KStream` с помощью `toStream()`.
    - Отправляем вывод в тему Kafka с именем `output-topic` с помощью метода `to()`.
6. **Строка 33**: Создаем новый объект `KafkaStreams`.
7. **Строка 35**: Вызываем метод `start()` объекта `app` для запуска приложения Kafka Streams.

На этом уроке мы познакомились с вычислениями с отслеживанием состояния API Kafka Streams DSL и операцией агрегации на практическом примере.