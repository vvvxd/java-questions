
### 1. Основы и архитектура Cassandra

#### Распределённая архитектура
Cassandra — это **распределённая NoSQL база данных**, что означает, что данные хранятся на множестве узлов (серверов), работающих как единая система. Она спроектирована так, чтобы не иметь единой точки отказа (single point of failure), что делает её надёжной для критически важных приложений.

- **Модель "ключ-значение" с столбцовым хранением**:
  - Данные в Cassandra организованы в виде пар "ключ-значение", где ключ уникально идентифицирует строку (row), а значение представляет собой набор столбцов (columns). Это гибрид между классическими ключ-значение хранилищами и реляционными базами данных.
  - Столбцовое хранение позволяет эффективно работать с большими наборами данных, где не все столбцы заполнены, экономя место и ускоряя чтение.

- **Кольцевая топология (DHT — Distributed Hash Table)**:
  - Cassandra использует концепцию **распределённой хэш-таблицы (DHT)**, где данные распределяются по узлам в виде виртуального "кольца".
  - Каждый узел отвечает за определённый диапазон хэш-значений (токенов), которые определяют, где хранятся данные. Ключ записи хэшируется, и на основе этого хэша данные распределяются по узлам.
  - Например, если у вас есть кластер из 4 узлов, каждый узел отвечает за часть "кольца", и данные автоматически распределяются по этим узлам.

- **Отсутствие единой точки отказа**:
  - В Cassandra нет главного узла (master), все узлы равноправны (peer-to-peer). Это означает, что любой узел может принимать запросы на чтение или запись, а данные реплицируются между несколькими узлами для обеспечения отказоустойчивости.
  - Если один узел выходит из строя, другие узлы продолжают обслуживать запросы, а данные остаются доступными благодаря репликации.

- **Равноправность узлов (peer-to-peer)**:
  - Все узлы в кластере равны по функциональности: каждый может быть координатором запросов, хранить данные и выполнять операции. Это упрощает масштабирование и управление кластером.
  - Отсутствие мастера/ведомого (master/slave) устраняет узкие места, характерные для традиционных реляционных баз данных.

#### Ключевые принципы

- **Высокая доступность (High Availability)**:
  - Cassandra спроектирована так, чтобы обеспечивать доступ к данным даже при сбоях отдельных узлов или целых дата-центров. Это достигается за счёт репликации данных и распределённой архитектуры.
  - Например, если один узел недоступен, клиент может получить данные с другого узла, где хранится их копия (реплика).

- **Отказоустойчивость**:
  - Данные автоматически реплицируются на несколько узлов (в соответствии с заданным коэффициентом репликации, например, 3 копии). Если один узел выходит из строя, данные всё ещё доступны на других узлах.
  - Cassandra использует механизмы, такие как **hinted handoff** (временное хранение операций на другом узле, если целевой узел недоступен) и **anti-entropy repair** (синхронизация данных между узлами), чтобы поддерживать целостность данных.

- **Масштабируемость**:
  - Cassandra поддерживает **горизонтальное масштабирование**: для увеличения производительности или объёма данных достаточно добавить новые узлы в кластер. Данные автоматически перераспределяются между узлами без необходимости остановки системы.
  - Линейная масштабируемость позволяет кластеру обрабатывать увеличение нагрузки пропорционально количеству узлов.

- **Настраиваемая согласованность (Tunable Consistency)**:
  - Cassandra основана на **CAP-теореме**, которая утверждает, что распределённая система может гарантировать только два из трёх свойств: согласованность (Consistency), доступность (Availability) и устойчивость к разделению (Partition Tolerance). Cassandra делает акцент на **AP** (доступность и устойчивость к разделению), жертвуя строгой согласованностью.
  - Пользователь может настраивать уровень согласованности для операций чтения и записи:
    - **Сильная согласованность**: Например, уровень `QUORUM` требует, чтобы большинство узлов с репликами подтвердили операцию.
    - **Слабая согласованность**: Уровень `ONE` требует подтверждения только от одного узла, что ускоряет операции, но может привести к чтению устаревших данных.
  - Это позволяет адаптировать Cassandra под разные сценарии: от систем, где важна скорость (например, логирование), до систем, где важна точность данных (например, финансовые приложения).

#### Компоненты системы

- **Кластер**:
  - Кластер — это совокупность всех узлов Cassandra, работающих вместе для хранения и обработки данных. Один кластер может охватывать несколько дата-центров.

- **Кольцо**:
  - Логическая структура, представляющая распределение данных между узлами. Каждый узел в кольце отвечает за определённый диапазон хэш-значений (токенов).
  - Данные распределяются по кольцу на основе хэша ключа записи.

- **Ноды (узлы)**:
  - Отдельные серверы в кластере, которые хранят данные и обрабатывают запросы. Каждый узел отвечает за часть данных и может быть координатором запросов.

- **Дата-центры**:
  - Группы узлов, физически расположенные в одном месте (например, в одном регионе). Cassandra поддерживает мультидатацентровые конфигурации, что позволяет реплицировать данные между географически разнесёнными локациями для повышения доступности и отказоустойчивости.

- **Координатор**:
  - Узел, который принимает запрос клиента (чтение или запись) и направляет его к нужным узлам, где хранятся данные. Координатор выбирается автоматически на основе топологии кластера и запроса.

- **Реплики**:
  - Копии данных, хранящиеся на разных узлах. Количество реплик задаётся параметром **replication factor** (например, 3 означает, что каждая запись хранится на трёх узлах).
  - Реплики распределяются по кольцу так, чтобы минимизировать риск потери данных при сбоях.

- **Токены**:
  - Уникальные значения, определяющие диапазон хэш-ключей, за которые отвечает узел. Токены формируют "кольцо" и определяют, где хранятся данные.

- **Партиции**:
  - Логические единицы хранения данных, которые распределяются по узлам. Каждая партиция соответствует строке данных, идентифицируемой уникальным ключом (partition key). Хэш ключа определяет, на каком узле (или узлах) будет храниться партиция.

---

### Пример работы системы
1. **Запись данных**:
  - Клиент отправляет запрос на запись данных в Cassandra.
  - Координатор (любой узел кластера) принимает запрос, вычисляет хэш ключа записи и определяет, на каких узлах (в соответствии с replication factor) эти данные должны быть сохранены.
  - Данные записываются на указанные узлы, а координатор возвращает подтверждение клиенту в зависимости от выбранного уровня согласованности (например, `ONE`, `QUORUM`).

2. **Чтение данных**:
  - Клиент отправляет запрос на чтение.
  - Координатор определяет, на каких узлах находятся реплики данных, и запрашивает их.
  - В зависимости от уровня согласованности (например, `QUORUM`) координатор собирает ответы от нескольких узлов, чтобы гарантировать актуальность данных.

---

### Преимущества и особенности
- **Высокая производительность**: Cassandra оптимизирована для операций записи благодаря использованию лога (commit log) и memtables, что делает её идеальной для приложений с интенсивной записью (например, логи, временные ряды).
- **Гибкость модели данных**: Столбцовое хранение позволяет динамически добавлять столбцы без изменения схемы.
- **Глобальная репликация**: Поддержка мультидатацентровых конфигураций делает Cassandra подходящей для географически распределённых систем.
- **Простота масштабирования**: Добавление узлов не требует сложной настройки, а данные автоматически перераспределяются.


### 2. **Модель данных**
Продолжаем разбор Apache Cassandra, сосредоточившись на ключевых концепциях, особенностях и принципах дизайна данных. Я объясню всё подробно и понятно, опираясь на указанные пункты, чтобы дать целостное представление о структуре и подходах к работе с Cassandra.

---

### Ключевые концепции

#### Keyspace
- **Keyspace** — это аналог базы данных в реляционных СУБД. Это контейнер верхнего уровня, в котором хранятся таблицы (Column Families) и задаются настройки репликации для данных.
- Основные атрибуты Keyspace:
  - **Имя**: Уникальное название Keyspace (например, `my_app`).
  - **Replication Strategy**: Определяет, как данные будут реплицироваться между узлами.
    - **SimpleStrategy**: Используется для одного дата-центра, задаёт количество реплик (replication factor). Например, `replication_factor: 3` означает, что данные хранятся на трёх узлах.
    - **NetworkTopologyStrategy**: Используется для мультидатацентровых конфигураций, где можно указать количество реплик для каждого дата-центра.
  - **Durable Writes**: Определяет, записываются ли изменения в commit log на диск (для обеспечения надёжности).

#### Column Family/Table
- **Column Family** (или таблица в терминах CQL) — это структура данных, аналогичная таблице в реляционных базах, но с большей гибкостью.
- Особенности:
  - Каждая строка (row) в таблице идентифицируется уникальным **первичным ключом**.
  - Строки содержат **динамические столбцы**: количество и имена столбцов могут варьироваться между строками, в отличие от строгой схемы реляционных баз.
  - Каждая строка состоит из набора пар "имя столбца — значение", что делает Cassandra похожей на столбцовую базу данных.

#### Первичный ключ (Primary Key)
- **Первичный ключ** уникально идентифицирует каждую строку в таблице и состоит из двух частей:
  1. **Partition Key (ключ партиции)**:
    - Определяет, на каком узле (или узлах) кластера будут храниться данные.
    - Хэш partition key вычисляется для распределения строки по узлам в кольце (согласно DHT).
    - Пример: в таблице `users` с `PRIMARY KEY (user_id)`, `user_id` — это partition key.
  2. **Clustering Key (кластерный ключ)**:
    - Используется для сортировки данных внутри одной партиции.
    - Определяет порядок строк в партиции на диске, что влияет на эффективность запросов.
    - Пример:
      ```cql
      CREATE TABLE my_app.posts (
          user_id uuid,
          post_id uuid,
          content text,
          created_at timestamp,
          PRIMARY KEY (user_id, post_id)
      );
      ```
      Здесь `user_id` — partition key, а `post_id` — clustering key. Все посты одного пользователя хранятся в одной партиции, отсортированные по `post_id`.
- **Составной ключ**:
  - Первичный ключ может включать несколько столбцов, например:
    ```cql
    PRIMARY KEY ((user_id, region), post_id)
    ```
    Здесь `(user_id, region)` — составной partition key, а `post_id` — clustering key.
- Partition key критически важен, так как он определяет, как данные распределяются и где хранятся.

#### Разница между статическими и динамическими столбцами
- **Статические столбцы**:
  - Объявляются с ключевым словом `STATIC` и принадлежат всей партиции, а не отдельной строке.
  - Используются для данных, которые одинаковы для всех строк в одной партиции (например, метаданные пользователя, общие для всех его записей).
  - Пример:
    ```cql
    CREATE TABLE my_app.user_posts (
        user_id uuid,
        username text STATIC,
        post_id uuid,
        content text,
        PRIMARY KEY (user_id, post_id)
    );
    ```
    Здесь `username` — статический столбец, общий для всех постов одного `user_id`.
- **Динамические столбцы**:
  - Обычные столбцы, которые могут варьироваться между строками в одной партиции.
  - Позволяют гибко добавлять новые столбцы в строки без изменения схемы.
  - Пример: в таблице `user_posts` столбец `content` может быть заполнен только для некоторых постов.

---

### Особенности

#### Отсутствие JOIN-ов
- Cassandra **не поддерживает JOIN-ы**, характерные для реляционных баз данных, чтобы обеспечить высокую производительность и масштабируемость.
- Вместо этого используется **денормализация**:
  - Данные моделируются так, чтобы запросы выполнялись с минимальным количеством операций чтения.
  - Например, если в реляционной базе вы бы использовали JOIN для связи таблиц `users` и `posts`, в Cassandra вы создадите таблицу, где все нужные данные уже находятся вместе:
    ```cql
    CREATE TABLE my_app.user_posts (
        user_id uuid,
        username text,
        post_id uuid,
        content text,
        PRIMARY KEY (user_id, post_id)
    );
    ```
    Здесь данные о пользователе (`username`) и постах хранятся вместе, чтобы запросы типа "получить все посты пользователя" выполнялись одним чтением.
- Денормализация требует дублирования данных, но это оправдано высокой скоростью чтения.

#### Поддержка коллекций
- Cassandra поддерживает **коллекции** для хранения множественных значений в одном столбце:
  - **List**: Упорядоченный список значений (например, `["item1", "item2"]`).
  - **Set**: Неупорядоченное множество уникальных значений (например, `{"tag1", "tag2"}`).
  - **Map**: Ассоциативный массив (ключ-значение, например, `{"key1": "value1", "key2": "value2"}`).
- Пример:
  ```cql
  CREATE TABLE my_app.users (
      user_id uuid,
      emails list<text>,
      tags set<text>,
      preferences map<text, text>,
      PRIMARY KEY (user_id)
  );
  ```
  Здесь `emails` — список адресов, `tags` — множество тегов, `preferences` — словарь настроек.
- Коллекции удобны для хранения небольших наборов данных, но их использование ограничено (например, размер коллекции не должен быть слишком большим).

#### Встроенные и пользовательские типы данных
- **Встроенные типы данных**:
  - Cassandra поддерживает множество типов, таких как:
    - `text`, `varchar`: Строки.
    - `int`, `bigint`, `float`, `double`: Числовые типы.
    - `timestamp`: Для хранения дат и времени.
    - `uuid`, `timeuuid`: Уникальные идентификаторы.
    - `boolean`, `blob` и др.
- **Пользовательские типы (UDT — User-Defined Types)**:
  - Позволяют создавать сложные структуры данных, аналогичные объектам.
- UDT упрощают работу со сложными данными, но требуют осторожного использования из-за ограничений на обновление.

---

### Дизайн данных

#### Моделирование под запросы (Query-Driven Design)
- В Cassandra данные моделируются **под конкретные запросы**, а не под нормализацию, как в реляционных базах.
- Основной принцип: **один запрос — одна таблица**. Это означает, что структура таблицы создаётся так, чтобы запросы выполнялись с минимальным количеством операций чтения.
- Шаги проектирования:
  1. Определите запросы, которые будет выполнять приложение (например, "получить все посты пользователя по `user_id`").
  2. Создайте таблицу, где partition key и clustering key оптимизированы под этот запрос.
  3. Денормализуйте данные, чтобы избежать необходимости JOIN-ов.

#### Влияние Partition Key на распределение данных
- **Partition Key** определяет, как данные распределяются по узлам кластера:
  - Хэш partition key вычисляется с помощью алгоритма (обычно Murmur3), и результат определяет, на каком узле (или узлах, если есть репликация) будут храниться данные.
  - Хорошо спроектированный partition key обеспечивает равномерное распределение данных, предотвращая "горячие точки" (hot spots), когда один узел перегружен.
- Советы по выбору partition key:
  - **Равномерность**: Выбирайте ключ, который создаёт много уникальных партиций (например, `user_id` лучше, чем `country`, так как пользователей больше, чем стран).
  - **Размер партиции**: Избегайте слишком больших партиций (например, миллионы строк в одной партиции), так как это замедляет чтение.
  - **Частота запросов**: Убедитесь, что запросы по partition key эффективны, так как Cassandra оптимизирована для чтения по ключу партиции.
- Пример плохого partition key:
  - Если использовать `year` как partition key для таблицы логов, все данные за один год окажутся в одной партиции, что приведёт к неравномерной нагрузке.
  - Лучше использовать составной ключ, например, `(year, month)` или `(user_id, timestamp)`.


### 3. **Внутренние механизмы**
Продолжаем разбирать Apache Cassandra, сосредоточившись на хранении данных, репликации, согласованности, партиционировании и обработке отказов. Я объясню каждый пункт подробно и понятно, чтобы вы могли глубоко понять, как работает Cassandra на низком уровне.

---

### Хранение данных

Cassandra использует комбинацию структур в памяти и на диске для обеспечения высокой производительности записи и долговечности данных.

#### SSTables
- **SSTables (Sorted String Tables)** — это неизменяемые (immutable) файлы на диске, в которых Cassandra хранит данные.
- Особенности:
  - После создания SSTable не изменяется. Новые данные записываются в новые SSTables.
  - Каждая SSTable содержит отсортированные строки данных, организованные по первичному ключу (partition key и clustering key).
  - SSTables оптимизированы для быстрого чтения, так как данные в них отсортированы, а для ускорения поиска используются индексы.
- Зачем нужны неизменяемые файлы?
  - Неизменяемость упрощает управление данными и повышает надёжность, так как старые данные не перезаписываются.
  - Однако со временем количество SSTables растёт, что требует процесса компактирования (см. ниже).

#### Memtables и CommitLog
- **Memtable**:
  - Это структура данных в оперативной памяти, куда временно записываются новые данные перед их сбросом на диск.
  - Memtable представляет собой отсортированное дерево (обычно Red-Black Tree), где данные хранятся по первичному ключу.
  - Когда Memtable достигает определённого размера, она сбрасывается на диск в виде новой SSTable.
- **CommitLog**:
  - Это журнал на диске, куда все операции записи сначала сохраняются для обеспечения долговечности (durability).
  - Каждая операция записи сначала записывается в CommitLog (на диск), а затем в Memtable (в память). Это гарантирует, что данные не потеряются при сбое системы.
  - CommitLog организован как append-only файл, что делает запись очень быстрой.
- Процесс записи:
  1. Клиент отправляет запрос на запись.
  2. Данные записываются в CommitLog (на диск) и в Memtable (в память).
  3. Клиенту возвращается подтверждение (в зависимости от уровня согласованности).
  4. Когда Memtable заполняется, она сбрасывается в SSTable, а CommitLog очищается для соответствующих данных.

#### Компактирование (Compaction)
- **Компактирование** — это процесс объединения нескольких SSTables в одну для оптимизации хранения и чтения.
- Зачем нужно компактирование?
  - Со временем данные в SSTables могут дублироваться (например, при обновлении или удалении строк), а количество файлов растёт, что замедляет чтение.
  - Компактирование объединяет данные, удаляет устаревшие записи (tombstones) и оптимизирует пространство.
- Типы компактирования:
  - **SizeTieredCompactionStrategy**:
    - Объединяет SSTables одинакового размера в один большой файл.
    - Подходит для рабочих нагрузок с интенсивной записью.
  - **LeveledCompactionStrategy**:
    - Организует SSTables в уровни (levels), где каждый уровень содержит файлы фиксированного размера.
    - Подходит для рабочих нагрузок с интенсивным чтением, так как минимизирует количество SSTables, которые нужно читать.
  - **TimeWindowCompactionStrategy**:
    - Оптимизирована для временных рядов, где данные группируются по временным окнам.
- Компактирование происходит автоматически в фоновом режиме, но может потреблять ресурсы, поэтому важно правильно выбрать стратегию.

---

### Репликация

Репликация в Cassandra обеспечивает отказоустойчивость и доступность данных, распределяя копии данных по нескольким узлам.

#### Фактор репликации (Replication Factor)
- **Replication Factor** определяет, сколько копий каждой записи будет храниться в кластере.
  - Например, если `replication_factor = 3`, каждая строка данных будет храниться на трёх узлах.
- Фактор репликации задаётся при создании Keyspace:
  ```cql
  CREATE KEYSPACE my_app
  WITH replication = {
    'class': 'SimpleStrategy',
    'replication_factor': 3
  };
  ```
- Чем выше replication factor, тем выше отказоустойчивость, но больше нагрузка на хранение и запись.

#### Стратегии репликации
- **SimpleStrategy**:
  - Используется для кластеров с одним дата-центром.
  - Данные распределяются по кольцу, и каждая запись реплицируется на последующие узлы в соответствии с replication factor.
  - Пример: Если у вас 5 узлов и `replication_factor = 3`, данные с одного узла копируются на два следующих узла в кольце.
- **NetworkTopologyStrategy**:
  - Используется для мультидатацентровых конфигураций.
  - Позволяет задавать количество реплик для каждого дата-центра.
  - Пример:
    ```cql
    CREATE KEYSPACE my_app
    WITH replication = {
      'class': 'NetworkTopologyStrategy',
      'DC1': 2,
      'DC2': 3
    };
    ```
    Здесь в дата-центре `DC1` будет 2 копии данных, а в `DC2` — 3 копии.
- NetworkTopologyStrategy полезна для географически распределённых систем, где важно минимизировать задержки и обеспечивать локальную доступность данных.

---

### Согласованность

Cassandra позволяет настраивать уровень согласованности (Consistency Level) для операций чтения и записи, балансируя между производительностью и точностью данных.

#### Уровни согласованности (Consistency Levels)
- **ONE**: Запрос считается успешным, если хотя бы один узел с репликой подтверждает операцию.
  - Максимальная производительность, минимальная согласованность (возможны устаревшие данные при чтении).
- **QUORUM**: Требуется подтверждение от большинства узлов с репликами (например, 2 из 3 при `replication_factor = 3`).
  - Баланс между производительностью и согласованностью.
- **ALL**: Все узлы с репликами должны подтвердить операцию.
  - Максимальная согласованность, но минимальная производительность и доступность (запрос не выполнится, если хотя бы один узел недоступен).
- **LOCAL_QUORUM**: Требуется подтверждение от большинства узлов в том же дата-центре, где выполняется запрос.
  - Подходит для мультидатацентровых конфигураций, чтобы минимизировать задержки.
- **LOCAL_ONE**: Подтверждение от одного узла в локальном дата-центре.
  - Максимальная производительность для локальных операций.
- Другие уровни: `TWO`, `THREE`, `EACH_QUORUM` и т.д., для специфичных сценариев.

#### Баланс между производительностью и согласованностью
- Cassandra следует **CAP-теореме**, отдавая приоритет **доступности (Availability)** и **устойчивости к разделению (Partition Tolerance)** над строгой согласованностью (Consistency).
- Выбор уровня согласованности зависит от сценария:
  - Для приложений, где важна скорость (например, логирование событий), используется `ONE` или `LOCAL_ONE`.
  - Для критически важных данных (например, финансовые транзакции) предпочтительны `QUORUM` или `ALL`.
- Комбинация уровней согласованности для чтения и записи определяет общую согласованность:
  - Если `Consistency_Level_READ + Consistency_Level_WRITE > Replication_Factor`, достигается строгая согласованность (например, `QUORUM` для чтения и записи при `replication_factor = 3`).

---

### Партиционирование

Партиционирование определяет, как данные распределяются по узлам в кластере.

#### Хэш ключа партиции
- **Partition Key** определяет, на каком узле (или узлах) будут храниться данные.
- Процесс:
  1. Значение partition key хэшируется с помощью алгоритма (обычно Murmur3).
  2. Полученный хэш (токен) сопоставляется с диапазоном токенов, за который отвечает определённый узел в кольце.
  3. Данные записываются на этот узел и его реплики (в соответствии с replication factor).
- Пример:
  - Если partition key — `user_id = 123`, хэш `user_id` может дать токен `56789`, который попадает в диапазон узла A. Тогда данные сохраняются на узле A и его репликах.

#### Токены и их распределение в кольце
- **Токены** — это числовые значения, определяющие диапазоны данных, за которые отвечает узел.
- Каждый узел в кластере назначается одному или нескольким токенам (в зависимости от конфигурации).
- Виртуальные узлы (vnodes):
  - Современные версии Cassandra используют виртуальные узлы, где каждый физический узел отвечает за множество токенов (обычно 256 на узел).
  - Это упрощает балансировку данных при добавлении или удалении узлов.
- Пример распределения:
  - Кластер с 4 узлами, каждый отвечает за диапазон токенов (например, 0–25%, 25–50%, 50–75%, 75–100%).
  - Когда добавляется новый узел, токены перераспределяются, и часть данных автоматически мигрирует на новый узел.

---

### Обработка отказов

Cassandra спроектирована для работы в условиях сбоев узлов или целых дата-центров, используя несколько механизмов.

#### Gossip-протокол
- **Gossip-протокол** — это механизм, с помощью которого узлы обмениваются информацией о своём состоянии и состоянии других узлов в кластере.
- Как работает:
  - Каждый узел периодически отправляет "слухи" (gossip messages) о своём статусе (включён, выключен, перегружен) другим узлам.
  - Узлы обмениваются информацией о топологии кластера, токенах и состоянии сети.
- Преимущества:
  - Позволяет быстро обнаруживать сбои узлов или изменения в кластере.
  - Децентрализован, не требует центрального координатора.

#### Hinted Handoff
- **Hinted Handoff** — механизм для обработки временной недоступности узлов.
- Как работает:
  - Если узел, на который должны быть записаны данные, недоступен, координатор сохраняет "подсказку" (hint) на другом узле.
  - Подсказка содержит информацию о записи и целевом узле.
  - Когда узел становится доступным, подсказки передаются ему для синхронизации данных.
- Пример:
  - При `replication_factor = 3`, если один узел недоступен, координатор сохраняет hint и записывает данные на доступные узлы.
- Ограничения:
  - Подсказки хранятся ограниченное время (по умолчанию 3 часа, задаётся параметром `max_hint_window_in_ms`).

#### Read Repair
- **Read Repair** — механизм восстановления согласованности данных при чтении.
- Как работает:
  - При запросе чтения координатор запрашивает данные у всех узлов с репликами (или их подмножества, в зависимости от уровня согласованности).
  - Если обнаруживаются расхождения (например, устаревшие данные на одном узле), координатор возвращает клиенту актуальную версию и инициирует фоновую синхронизацию (repair) для обновления устаревших реплик.
- Read Repair происходит автоматически и помогает поддерживать согласованность в системах со слабой согласованностью (например, `ONE`).

---

### Пример работы системы
1. **Запись**:
  - Клиент отправляет запрос на запись с `Consistency Level = QUORUM`.
  - Координатор вычисляет хэш partition key, определяет целевые узлы (например, 3 узла при `replication_factor = 3`).
  - Данные записываются в CommitLog и Memtable на каждом узле. Если один узел недоступен, создаётся hint.
  - Подтверждение возвращается, когда 2 из 3 узлов (QUORUM) подтверждают запись.

2. **Чтение**:
  - Клиент запрашивает данные с `Consistency Level = QUORUM`.
  - Координатор запрашивает данные у всех реплик, сравнивает их и возвращает актуальную версию.
  - Если обнаруживаются расхождения, запускается Read Repair.

3. **Сбой узла**:
  - Gossip-протокол сообщает кластеру, что узел недоступен.
  - Запросы перенаправляются на доступные узлы с репликами.
  - Hinted Handoff сохраняет изменения для недоступного узла, а после его восстановления данные синхронизируются.



### 4. **Язык запросов CQL (Cassandra Query Language)**
Продолжаем разбирать Apache Cassandra, сосредоточившись на языке запросов **CQL (Cassandra Query Language)**, его основах, продвинутых возможностях и ограничениях. Я объясню всё подробно, понятно и структурированно, опираясь на указанные пункты, чтобы вы могли эффективно использовать CQL в работе с Cassandra.

---

### Основы CQL

**CQL (Cassandra Query Language)** — это язык запросов, похожий на SQL, но адаптированный под распределённую NoSQL архитектуру Cassandra. Он упрощает работу с данными, но имеет ограничения, связанные с особенностями модели данных Cassandra (например, отсутствие JOIN-ов и ограниченные фильтры).

#### Синтаксис, аналогичный SQL
- CQL использует знакомый SQL-подобный синтаксис, что делает его интуитивным для разработчиков, знакомых с реляционными базами данных.
- Основные отличия:
  - **Нет JOIN-ов**: Данные денормализуются, и запросы проектируются так, чтобы получать все данные из одной таблицы.
  - **Ограниченные фильтры**: Запросы с `WHERE` требуют указания ключа партиции (partition key) или вторичных индексов для эффективности.
  - **Ориентация на производительность**: CQL оптимизирован для быстрого чтения и записи в распределённой среде.

#### Создание и изменение Keyspace и таблиц
- **Создание Keyspace**:
  - Keyspace — это контейнер для таблиц, аналог базы данных в реляционных СУБД.
  - Пример:
    ```cql
    CREATE KEYSPACE my_app
    WITH replication = {
      'class': 'SimpleStrategy',
      'replication_factor': 3
    }
    AND durable_writes = true;
    ```
    Здесь создаётся Keyspace `my_app` с тремя репликами в одном дата-центре.
- **Создание таблицы**:
  - Таблицы определяют структуру данных, включая первичный ключ (partition key и, опционально, clustering key).
  - Пример:
    ```cql
    CREATE TABLE my_app.users (
        user_id uuid,
        name text,
        email text,
        created_at timestamp,
        PRIMARY KEY (user_id)
    );
    ```

#### Работа с INSERT, UPDATE, DELETE, SELECT
- **INSERT**:
  - Добавляет новую строку или обновляет существующую (в Cassandra `INSERT` и `UPDATE` идемпотентны, так как данные перезаписываются по первичному ключу).

- **UPDATE**:
  - Обновляет данные в строке, идентифицируемой первичным ключом.

- **DELETE**:
  - Удаляет строки или отдельные столбцы. Удалённые данные помечаются как **tombstones** (логические удаления, которые позже убираются при компактировании).

- **SELECT**:
  - Извлекает данные из таблицы. Требует указания partition key в `WHERE` для эффективного выполнения.

---

### Продвинутые возможности

Cassandra предоставляет мощные инструменты для работы с данными, включая коллекции, пользовательские типы, индексы и batch-операции.

#### Использование индексов
- **Вторичные индексы (Secondary Indexes)**:
  - Позволяют выполнять запросы по столбцам, не входящим в первичный ключ.
  - Пример:
    ```cql
    CREATE INDEX ON my_app.users (email);
    SELECT * FROM my_app.users WHERE email = 'alice@example.com';
    ```
  - Ограничения:
    - Вторичные индексы менее эффективны, чем запросы по partition key, так как требуют сканирования нескольких узлов.
    - Не рекомендуются для столбцов с высокой кардинальностью (много уникальных значений).
- **SASI (SSTable-Attached Secondary Index)**:
  - Улучшенная версия вторичных индексов, поддерживающая более сложные запросы (например, `LIKE`, диапазоны).
  - Пример:
    ```cql
    CREATE CUSTOM INDEX ON my_app.users (name) USING 'org.apache.cassandra.index.sasi.SASIIndex';
    SELECT * FROM my_app.users WHERE name LIKE 'Ali%';
    ```
- **Материализованные представления (Materialized Views)**:
  - Создают отдельную таблицу, синхронизированную с основной, но с другим первичным ключом.
  - Пример:
    ```cql
    CREATE TABLE my_app.user_posts (
        user_id uuid,
        post_id timeuuid,
        content text,
        PRIMARY KEY (user_id, post_id)
    );

    CREATE MATERIALIZED VIEW my_app.posts_by_id AS
        SELECT post_id, user_id, content
        FROM my_app.user_posts
        WHERE post_id IS NOT NULL AND user_id IS NOT NULL
        PRIMARY KEY (post_id, user_id);
    ```
  - Позволяет выполнять запросы по `post_id` без сканирования всех партиций.

#### Batch-операции
- **Batch** позволяет выполнять несколько операций `INSERT`, `UPDATE` или `DELETE` в одной транзакции.
- Пример:
  ```cql
  BEGIN BATCH
      INSERT INTO my_app.users (user_id, name) VALUES (uuid(), 'Bob');
      INSERT INTO my_app.user_posts (user_id, post_id, content) VALUES (<uuid>, now(), 'Hello');
  APPLY BATCH;
  ```
- Типы batch:
  - **Logged Batch**: Гарантирует атомарность для операций в пределах одной партиции. Используется по умолчанию.
  - **Unlogged Batch**: Не использует журнал (CommitLog) для координации, быстрее, но не гарантирует атомарность.
    ```cql
    BEGIN UNLOGGED BATCH
      ...
    APPLY BATCH;
    ```
- Ограничения:
  - Batch-операции, затрагивающие разные партиции, могут быть медленными, так как требуют координации между узлами.
  - Не рекомендуется для больших объёмов данных (например, тысяч операций), так как это нагружает координатор.
  - Используйте batch только для операций, которые логически связаны и должны быть атомарными.

---

### Ограничения CQL

Cassandra оптимизирована для определённых сценариев, и CQL имеет ограничения, связанные с её распределённой природой.

#### Почему не стоит использовать WHERE без partition key
- Запросы с `WHERE`, не включающие partition key, требуют сканирования всех узлов кластера, что крайне неэффективно.
- Пример неэффективного запроса:
  ```cql
  SELECT * FROM my_app.users WHERE name = 'Alice';
  ```
  Этот запрос сканирует все партиции, что может быть очень медленным в больших кластерах.
- Решение:
  - Используйте partition key в `WHERE`:
    ```cql
    SELECT * FROM my_app.users WHERE user_id = <uuid>;
    ```
  - Создайте вторичный индекс или материализованное представление для запросов по неключевым столбцам.

#### Работа с фильтрами и ALLOW FILTERING
- **ALLOW FILTERING** позволяет выполнять запросы с фильтрами по столбцам, не входящим в индексы или первичный ключ.
- Пример:
  ```cql
  SELECT * FROM my_app.users WHERE age > 25 ALLOW FILTERING;
  ```
- Проблемы:
  - `ALLOW FILTERING` заставляет Cassandra сканировать все партиции, что очень медленно и ресурсоёмко.
  - Используется только для тестирования или небольших наборов данных.
- Рекомендации:
  - Перепроектируйте модель данных, чтобы запросы опирались на partition key или clustering key.
  - Используйте вторичные индексы или материализованные представления для фильтрации.

  
