Elasticsearch, мощного поискового и аналитического движка, необходимо освоить несколько ключевых аспектов, включая его архитектуру, функциональность, настройку и использование. Вот подробное руководство, что нужно изучить и на что обратить внимание:

### 1. Основы Elasticsearch
- **Понимание концепции**: Elasticsearch — это распределённая поисковая и аналитическая система, основанная на Apache Lucene. Она предназначена для полнотекстового поиска, аналитики больших данных и хранения документов в формате JSON.
- **Ключевые термины**:
    - **Индекс (Index)**: Аналог базы данных, содержит набор документов.
    - **Документ (Document)**: Основная единица данных в формате JSON, аналог строки в таблице.
    - **Шард (Shard)**: Разделение индекса на части для распределённой обработки.
    - **Реплика (Replica)**: Копия шарда для повышения отказоустойчивости и производительности.
    - **Кластер (Cluster)**: Группа узлов (nodes), работающих вместе.
    - **Узел (Node)**: Один экземпляр Elasticsearch в кластере.
- **REST API**: Elasticsearch использует HTTP-запросы (GET, POST, PUT, DELETE) для взаимодействия. Изучите основы работы с API через инструменты, такие как `curl`, Postman или Kibana Dev Tools.

### 2. Распределенность Elasticsearch 
Elasticsearch распределяет данные по **шардам** и **репликам**, чтобы работать быстро, справляться с большими объемами данных и не ломаться, если что-то идет не так. Объясню максимально просто:

#### **Что такое шарды?**
- Шарды — это кусочки данных. Представь, что у тебя огромная книга (база данных), и ты разрываешь ее на несколько частей, чтобы быстрее читать.
- Elasticsearch разбивает твои данные (например, документы JSON) на несколько **шардов** и распределяет их по разным серверам (узлам) в кластере.
- Это делает систему **масштабируемой**: если данных становится больше, можно добавить новые серверы, и шарды распределятся по ним. Поиск становится быстрее, потому что каждый сервер обрабатывает только свою часть данных.

**Пример**: У тебя 1 миллион документов. Elasticsearch делит их на 5 шардов (по 200 тысяч в каждом). Каждый шард лежит на своем сервере, и поиск идет одновременно на всех серверах.

#### **Что такое реплики?**
- Реплики — это копии шардов. Если у тебя есть шард с данными, Elasticsearch может создать его копию (реплику) и хранить на другом сервере.
- Это обеспечивает **отказоустойчивость**: если один сервер сломается, данные не потеряются, потому что их копия есть на другом сервере.
- Реплики также ускоряют поиск, потому что запросы могут обрабатываться на копиях данных.

**Пример**: У тебя 5 шардов, и для каждого создана 1 реплика. Если один сервер выключится, Elasticsearch использует копии (реплики) с других серверов, и система продолжает работать.

#### **Как это работает вместе?**
- Когда ты загружаешь данные в Elasticsearch, они автоматически делятся на шарды. Ты можешь настроить, сколько шардов и реплик нужно (например, 5 шардов и 2 реплики для каждого).
- Elasticsearch сам решает, на каких серверах хранить шарды и реплики, чтобы нагрузка распределялась равномерно.
- Если добавляется новый сервер, Elasticsearch перераспределяет шарды, чтобы использовать новый ресурс. Если сервер ломается, система переключается на реплики.

**Простой пример**:
Допустим, у тебя интернет-магазин с 3 серверами. Ты загружаешь данные о товарах, и Elasticsearch:
1. Делит данные на 3 шарда: Шард 1 (футболки), Шард 2 (штаны), Шард 3 (обувь).
2. Создает копии (реплики) каждого шарда и распределяет их так:
    - Сервер 1: Шард 1 + Реплика Шарда 2
    - Сервер 2: Шард 2 + Реплика Шарда 3
    - Сервер 3: Шард 3 + Реплика Шарда 1
3. Если Сервер 1 ломается, Elasticsearch использует Реплику Шарда 1 с Сервера 3, и поиск продолжает работать.

#### **Почему это круто?**
- **Масштабируемость**: Больше данных? Добавляй серверы, и шарды распределятся автоматически.
- **Отказоустойчивость**: Если один сервер падает, данные остаются доступны благодаря репликам.
- **Скорость**: Поиск идет параллельно по всем шардам, а реплики помогают распределять нагрузку.


## 3. Инвертированный индекс

**Инвертированный индекс** — это основа быстрого полнотекстового поиска в Lucene (и, соответственно, в Elasticsearch). Это структура данных, которая позволяет быстро находить документы, содержащие определённые слова. Давайте разберём, как она работает:

#### Что такое инвертированный индекс?
- Обычный индекс в книге — это когда по номеру страницы находишь содержание. Инвертированный индекс работает наоборот: по слову ты находишь, в каких документах оно встречается.
- Lucene разбивает текст документов на слова (термины) и создаёт таблицу, где каждому слову соответствует список документов, в которых оно есть.

#### Как Lucene это делает?
1. **Токенизация**:
    - Lucene берёт текст документа и разбивает его на слова (токены). Например, фраза "Кот ест рыбу" разбивается на токены: "кот", "ест", "рыбу".
    - При этом удаляются знаки препинания, а текст может быть приведён к нижнему регистру ("Кот" → "кот").
    - Можно применять дополнительные фильтры: убирать стоп-слова ("и", "в"), приводить слова к начальной форме (например, "рыбу" → "рыба").

2. **Создание индекса**:
    - Для каждого уникального слова (термина) Lucene создаёт запись в инвертированном индексе.
    - Эта запись содержит:
        - Сам термин (например, "кот").
        - Список документов, где он встречается (например, документ 1, документ 3).
        - Дополнительные данные: позиции слова в документе (для поиска фраз вроде "кот ест"), частота слова и т.д.
    - Пример:
      ```
      Термин    | Документы
      кот       | Док 1, Док 3
      ест       | Док 1, Док 2
      рыба      | Док 1
      ```

3. **Хранение и оптимизация**:
    - Индекс хранится на диске в сжатом виде, чтобы занимать меньше места и быстро читаться.
    - Lucene использует структуры данных (например, словари и списки), оптимизированные для быстрого поиска.
    - Для ускорения работы индекс разбивается на **сегменты** — небольшие независимые куски индекса. Когда добавляются новые документы, создаются новые сегменты, которые позже объединяются для оптимизации.

#### Почему это быстро?
- Инвертированный индекс позволяет искать не по всему тексту, а только по терминам. Например, если ищешь "кот", Lucene сразу смотрит в индекс и получает список документов, где есть это слово.
- Дополнительные данные (позиции, частота) помогают ранжировать результаты (например, документ, где "кот" встречается чаще, может быть выше в выдаче).
- Lucene использует эффективные алгоритмы сжатия и кэширования, чтобы минимизировать обращения к диску.

**Пример**:
Допустим, у тебя три документа:
- Док 1: "Кот ест рыбу"
- Док 2: "Кот спит"
- Док 3: "Рыба плавает"
  Lucene создаст индекс:
```
кот   → [Док 1, Док 2]
ест   → [Док 1]
рыба  → [Док 1, Док 3]
спит  → [Док 2]
плавает → [Док 3]
```
Если искать "кот", Lucene сразу вернёт Док 1 и Док 2. Если искать фразу "кот ест", он проверит позиции слов и вернёт только Док 1.

#### Как индекс распределяется по шардам в Elasticsearch?

Elasticsearch использует Lucene для создания инвертированного индекса, но добавляет к нему распределённую архитектуру с **шардами** и **репликами**, чтобы обрабатывать большие объёмы данных:

#### Что такое шарды в Elasticsearch?
- **Шард** — это независимая часть индекса, которая содержит подмножество данных. Каждый шард — это полноценный Lucene-индекс со своим инвертированным индексом.
- Elasticsearch делит данные на несколько шардов, чтобы:
    - Распределить нагрузку по разным серверам (узлам).
    - Ускорить поиск, так как запросы обрабатываются параллельно на каждом шарде.

#### Как данные распределяются по шардам?
1. **Разделение документов**:
    - Когда ты создаёшь индекс в Elasticsearch, ты указываешь, сколько **первичных шардов** (primary shards) будет. Например, 5 шардов.
    - Каждый документ (JSON-объект) автоматически распределяется в один из шардов на основе **хэш-функции**. Хэш вычисляется по уникальному идентификатору документа (ID) или другому ключу.
    - Формула: `shard = hash(document_id) % number_of_shards`. Это гарантирует, что документы равномерно распределяются по шардам.

2. **Шарды и Lucene**:
    - Каждый шард — это отдельный Lucene-индекс, который содержит свой инвертированный индекс для документов, попавших в этот шард.
    - Например, если у тебя 1 миллион документов и 5 шардов, каждый шард будет содержать около 200 тысяч документов, и каждый шард имеет свой инвертированный индекс, построенный Lucene.

3. **Распределение по узлам**:
    - Шарды распределяются по узлам (серверам) в кластере Elasticsearch. Например, если у тебя 3 узла и 5 шардов, Elasticsearch разместит шарды так, чтобы нагрузка была сбалансирована (например, 2 шарда на одном узле, 2 на другом, 1 на третьем).
    - Если добавляется новый узел, Elasticsearch автоматически перераспределяет шарды, чтобы использовать новый ресурс.

#### Реплики и их роль
- Реплики — это копии первичных шардов. Каждый шард может иметь 0 или больше реплик (например, 1 или 2 реплики на шард).
- Реплики тоже являются Lucene-индексами и содержат те же данные, что и первичный шард.
- Они хранятся на других узлах, чтобы:
    - Обеспечить **отказоустойчивость**: если узел с первичным шардом падает, Elasticsearch использует реплику.
    - Ускорить поиск: запросы могут выполняться на репликах, распределяя нагрузку.

#### Как поиск работает с шардами?
- Когда ты отправляешь поисковый запрос, Elasticsearch:
    1. Рассылает запрос всем первичным шардам (или их репликам) параллельно.
    2. Каждый шард выполняет поиск в своём Lucene-индексе и возвращает результаты.
    3. Elasticsearch собирает результаты, сортирует их (например, по релевантности) и возвращает тебе итоговый список.
- Это делает поиск быстрым, так как шарды обрабатываются независимо, а реплики помогают распределять нагрузку.

Добавление документа в индекс и изменение индекса в **Elasticsearch** — это ключевые процессы, которые обеспечивают хранение и актуализацию данных. Давайте разберём, как документ добавляется в индекс и как индекс изменяется, максимально просто и понятно.


### Как документ добавляется в индекс?

Документ — это единица данных в Elasticsearch, представленная в формате JSON. Например, `{"name": "Кот Мурзик", "age": 3}`. Когда ты добавляешь документ в индекс, Elasticsearch выполняет несколько шагов, чтобы сохранить его и сделать доступным для поиска.

#### Процесс добавления документа
1. **Отправка документа**:
    - Ты отправляешь документ через API (например, `POST` или `PUT`).
    - Пример: Добавление документа в индекс `pets`:
      ```bash
      POST /pets/_doc
      {
        "name": "Кот Мурзик",
        "age": 3,
        "type": "cat"
      }
      ```
    - Если указать ID явно, используй `PUT`:
      ```bash
      PUT /pets/_doc/1
      {
        "name": "Кот Мурзик",
        "age": 3,
        "type": "cat"
      }
      ```

2. **Маршрутизация**:
    - Elasticsearch определяет, в какой **шард** попадёт документ, используя хэш-функцию на основе ID документа:
      ```
      shard = hash(document_id) % number_of_shards
      ```
    - Если ID не указан, Elasticsearch генерирует уникальный ID автоматически.
    - Документ отправляется на узел (Data Node), где хранится соответствующий первичный шард.

3. **Обработка маппингов**:
    - Если индекс уже имеет **маппинг** (схему данных), Elasticsearch проверяет, соответствует ли документ этому маппингу. Например, поле `age` должно быть числом, если маппинг определяет его как `integer`.
    - Если маппинг не задан и включены **динамические маппинги**, Elasticsearch автоматически определяет типы полей:
        - `"name": "Кот Мурзик"` → `text` (с дополнительным полем `name.keyword`).
        - `"age": 3` → `integer`.
    - Если маппинг `strict`, а поле неизвестно, добавление документа завершится ошибкой.

4. **Индексация в Lucene**:
    - Каждый шард — это отдельный индекс **Lucene**. Документ добавляется в Lucene-индекс шарда:
        - Текст в полях (например, `name`) обрабатывается **анализатором** (разбивается на токены, приводится к нижнему регистру, применяются фильтры, такие как `russian_morphology`).
        - Пример: `"Коты едят рыбу"` → `["кот", "есть", "рыба"]` (для анализатора `russian`).
        - Токены добавляются в **инвертированный индекс** Lucene, где каждому токену соответствует список документов.
    - Полный JSON документа сохраняется в хранилище (для извлечения в Fetch Phase).

5. **Запись в сегмент**:
    - Lucene добавляет документ в новый **сегмент** (временный кусок индекса) в памяти.
    - Сегменты периодически записываются на диск (процесс **commit**).
    - Для ускорения индексации Elasticsearch использует **in-memory buffer** и обновляет индекс для поиска через интервал `refresh_interval` (по умолчанию 1 секунда).

6. **Репликация**:
    - После записи в первичный шард документ отправляется на все **реплики** этого шарда (если они настроены).
    - Реплики обновляются синхронно, чтобы гарантировать согласованность данных.

7. **Подтверждение**:
    - После успешной записи в первичный шард и все реплики Elasticsearch возвращает ответ:
      ```json
      {
        "_index": "pets",
        "_id": "1",
        "_version": 1,
        "result": "created",
        "_shards": { "total": 2, "successful": 2, "failed": 0 },
        "status": 201
      }
      ```
      
## 4. Роли узлов

В кластере **Elasticsearch** узлы (сервера) выполняют разные роли, чтобы эффективно управлять данными, поиском и обработкой запросов. Каждая роль имеет свои функции, и узлы могут комбинировать роли в зависимости от настроек. Давайте разберём максимально просто и понятно, что делают **Master**, **Data**, **Ingest** и **Coordinating** узлы.

### **Master Node (Узел управления)**
- **Что делает?**
    - Управляет кластером и его состоянием.
    - Отвечает за **администрирование** кластера: создание/удаление индексов, распределение шардов и реплик по узлам, отслеживание состояния узлов.
    - Не хранит данные и не обрабатывает запросы на поиск или индексацию (это делают другие узлы).
    - Поддерживает **состояние кластера** (cluster state) — информацию о том, какие индексы, шарды и узлы есть в кластере.

- **Зачем нужен?**
    - Обеспечивает стабильность и согласованность работы кластера.
    - Если Master-узел выходит из строя, другой узел с ролью Master-eligible (кандидат на Master) автоматически избирается новым Master'ом.

- **Пример**:
    - Когда ты создаёшь новый индекс, Master-узел решает, на каких узлах разместить шарды и реплики, и обновляет состояние кластера.
    - Если узел отключается, Master перераспределяет шарды на другие узлы.

- **Важно**:
    - Обычно в кластере 3–5 узлов с возможностью быть Master (Master-eligible), чтобы избежать проблем с выбором нового Master'а (split-brain).
    - Master-узел не должен быть перегружен, поэтому ему не дают другие роли (например, Data).
    
### **Data Node (Узел данных)**
- **Что делает?**
    - Хранит данные (шарды индексов) и выполняет операции, связанные с данными: индексация (добавление документов), поиск, агрегации.
    - Каждый Data-узел содержит часть данных кластера (первичные шарды или их реплики).
    - Выполняет "тяжёлую работу": поиск по инвертированным индексам Lucene, обработка запросов на фильтрацию или агрегацию (например, подсчёт или группировка).

- **Зачем нужен?**
    - Основной "рабочий" узел для хранения и обработки данных.
    - Позволяет масштабировать кластер: больше данных — больше Data-узлов.

- **Пример**:
    - Если у тебя индекс с 5 шардами, Data-узлы хранят эти шарды. Когда ты ищешь "кот", Data-узлы выполняют поиск в своих шардах и возвращают результаты.

- **Важно**:
    - Data-узлы требуют много ресурсов (диск, CPU, память), так как они хранят данные и выполняют интенсивные вычисления.
    - Разделяются на подтипы (необязательно):
        - **Hot nodes**: для часто используемых данных, быстрых операций (обычно на SSD).
        - **Warm nodes**: для менее активных данных, хранящихся дольше.
        - **Cold nodes**: для редко запрашиваемых данных (например, архивов).

### **Ingest Node (Узел приёма данных)**
- **Что делает?**
    - Отвечает за **предобработку** документов перед их индексацией.
    - Использует **ingest pipelines** (наборы правил) для преобразования данных: добавление полей, изменение форматов, фильтрация и т.д.
    - Например, может извлечь дату из строки, добавить геолокацию или преобразовать текст.

- **Зачем нужен?**
    - Упрощает подготовку данных, чтобы они были в нужном формате перед сохранением.
    - Снимает нагрузку с внешних приложений, которые отправляют данные в Elasticsearch.

- **Пример**:
    - У тебя лог сервера: `"2025-07-10 error: server down"`. Ingest-узел может:
        - Извлечь дату (`2025-07-10`) в отдельное поле.
        - Добавить метку `"status": "error"`.
        - Затем передать документ в Data-узел для хранения.

- **Важно**:
    - Любой узел может быть Ingest-узлом, но выделение этой роли на отдельные узлы снижает нагрузку на Data-узлы.
    - Если ingest pipelines не используются, эта роль не нужна.
    
### **Coordinating Node (Координирующий узел)**
- **Что делает?**
    - Принимает запросы от клиентов (например, поиск или агрегацию) и координирует их выполнение.
    - Рассылает запросы на нужные Data-узлы, собирает результаты, сортирует их и возвращает клиенту.
    - Не хранит данные и не выполняет индексацию, а только "управляет трафиком".

- **Зачем нужен?**
    - Упрощает взаимодействие клиента с кластером: клиент отправляет запрос одному узлу, а Coordinating-узел разбирается, какие шарды и узлы задействовать.
    - Уменьшает нагрузку на Data-узлы, так как выполняет финальную обработку результатов (например, сортировку или пагинацию).

- **Пример**:
    - Ты ищешь "кот" в индексе с 5 шардами. Coordinating-узел:
        - Отправляет запрос всем Data-узлам, где лежат шарды.
        - Собирает результаты (например, 10 документов от каждого шарда).
        - Сортирует их по релевантности и возвращает тебе топ-10.

- **Важно**:
    - Любой узел в кластере может быть Coordinating-узлом по умолчанию.
    - В больших кластерах создают **выделенные Coordinating-узлы** (без других ролей), чтобы снизить нагрузку на Data- и Master-узлы.

### Как роли работают вместе?
- **Master**: управляет кластером, распределяет шарды, обновляет состояние.
- **Data**: хранит данные и выполняет поиск/агрегации.
- **Ingest**: обрабатывает данные перед сохранением.
- **Coordinating**: принимает запросы клиентов, распределяет их по Data-узлам и собирает результаты.

**Пример работы кластера**:
1. Ты отправляешь документ `{ "name": "Кот Мурзик" }` в Elasticsearch.
2. **Ingest-узел** добавляет поле, например, `"type": "pet"`.
3. **Master-узел** решает, в какой шард пойдёт документ (по хэшу ID).
4. **Data-узел** сохраняет документ в нужный шард и обновляет инвертированный индекс Lucene.
5. При поиске "Мурзик" **Coordinating-узел** отправляет запрос Data-узлам, получает результаты и возвращает их тебе.

### Почему это важно?
- **Разделение ролей** позволяет:
    - **Масштабировать кластер**: добавляй Data-узлы для больших данных, Coordinating-узлы для обработки запросов.
    - **Обеспечить отказоустойчивость**: Master-узлы избираются заново, Data-узлы используют реплики.
    - **Оптимизировать производительность**: Ingest-узлы снимают нагрузку с Data-узлов, а Coordinating-узлы упрощают обработку запросов.
- В небольших кластерах один узел может выполнять все роли. В больших кластерах роли разделяют для эффективности.

**Простой пример**:
- Маленький кластер (1 узел): он одновременно Master, Data, Ingest и Coordinating.
- Большой кластер (10 узлов):
    - 3 узла Master-eligible (один активный Master).
    - 5 Data-узлов для хранения и поиска.
    - 1 Ingest-узел для обработки данных.
    - 1 Coordinating-узел для запросов.

### Итог
- **Master**: мозг кластера, управляет шардами и состоянием.
- **Data**: хранит данные и выполняет поиск.
- **Ingest**: готовит данные перед сохранением.
- **Coordinating**: связывает клиента с кластером, собирает результаты.
  Эти роли делают Elasticsearch гибким, быстрым и надёжным для работы с большими данными!
  
## 5. Роли узлов
Процесс поиска в **Elasticsearch** — это слаженная работа кластера, которая позволяет быстро находить и возвращать релевантные документы. Он состоит из двух основных фаз: **Query Phase** (фаза запроса) и **Fetch Phase** (фаза получения). Также важную роль играет **маршрутизация запросов**, которая определяет, как запросы распределяются по узлам и шардам.

### Общий процесс поиска
Когда ты отправляешь поисковый запрос в Elasticsearch (например, "кот"), он проходит через несколько этапов:
- Запрос поступает на **Coordinating Node** (координирующий узел).
- Координирующий узел распределяет запрос по нужным узлам и шардам.
- Шарды выполняют поиск (с помощью инвертированного индекса Lucene).
- Результаты собираются и возвращаются клиенту.

Этот процесс делится на две фазы: **Query Phase** и **Fetch Phase**.


### Query Phase (Фаза запроса)
Эта фаза отвечает за поиск документов, соответствующих запросу, и определение их релевантности.

#### Что происходит?
1. **Получение запроса**:
    - Клиент отправляет запрос (например, `"кот"`) на Coordinating Node.
    - Coordinating Node разбирает запрос и определяет, какие индексы и шарды нужно задействовать.

2. **Маршрутизация запроса**:
    - Elasticsearch знает, какие шарды содержат данные индекса. Запрос рассылается **всем первичным шардам** или их репликам (реплики используются для распределения нагрузки).
    - Если запрос точечный (например, поиск по конкретному ID документа), Elasticsearch использует хэш ID, чтобы направить запрос только в нужный шард.

3. **Поиск на шардах**:
    - Каждый шард (это отдельный Lucene-индекс) выполняет поиск в своём инвертированном индексе.
    - Lucene ищет термины из запроса (например, "кот") и возвращает список документов, где они встречаются.
    - Для каждого документа рассчитывается **релевантность** (score) на основе алгоритма, например, TF-IDF или BM25 (учитывается частота термина, длина документа и т.д.).
    - Шард возвращает **ограниченный список** наиболее релевантных документов (обычно топ-10 или сколько указано в запросе) с их ID и score.

4. **Сбор результатов**:
    - Coordinating Node собирает результаты от всех шардов.
    - Он объединяет списки документов, сортирует их по релевантности и выбирает итоговый топ (например, 10 лучших документов).

#### Пример:
- У тебя индекс с 3 шардами, и ты ищешь "кот".
- Coordinating Node отправляет запрос всем 3 шардам.
- Шард 1 находит 5 документов с "кот", Шард 2 — 3 документа, Шард 3 — 2 документа.
- Каждый шард возвращает свои топ-5 документов с их score.
- Coordinating Node объединяет 15 документов (5+5+5), сортирует по score и выбирает топ-10.

#### Особенности:
- **Параллелизм**: Шарды обрабатывают запрос одновременно, что ускоряет поиск.
  -18:00
- **Реплики**: Запрос может пойти либо на первичный шард, либо на его реплику (Coordinating Node выбирает автоматически для балансировки нагрузки).

### Fetch Phase (Фаза получения)
После того как в Query Phase определены наиболее релевантные документы, начинается фаза получения, чтобы извлечь полное содержимое этих документов.

#### Что происходит?
1. **Получение документов**:
    - Coordinating Node отправляет запрос на шарды, где хранятся выбранные документы (по их ID).
    - Каждый шард извлекает полный текст документа из хранилища (не только ID и score, но и все поля, например, `{ "name": "Кот Мурзик", "age": 3 }`).

2. **Возвращение результатов**:
    - Шарды отправляют полные документы обратно на Coordinating Node.
    - Coordinating Node выполняет финальную обработку (например, применяет сортировку, фильтры или агрегации) и возвращает результат клиенту.

#### Пример:
- В Query Phase выбраны документы с ID 1, 3, 5.
- В Fetch Phase Coordinating Node запрашивает у шардов полные данные этих документов.
- Шард 1 возвращает документ 1, Шард 2 — документ 3, Шард 3 — документ 5.
- Coordinating Node отправляет клиенту итоговый результат: список документов с их полями.

#### Особенности:
- Fetch Phase обычно быстрее, так как работает с небольшим количеством документов (только те, что прошли Query Phase).
- Если в запросе нужны только ID или агрегации (например, подсчёт), Fetch Phase может не выполняться.

### Маршрутизация запросов
Маршрутизация запросов — это процесс, который определяет, на какие узлы и шарды отправить запрос. Она обеспечивает эффективность и балансировку нагрузки.

#### Как работает маршрутизация?
1. **Определение шардов**:
    - Каждый индекс в Elasticsearch делится на **первичные шарды** и их **реплики**.
    - Для поиска по индексу запрос отправляется **всем шардам** (или их репликам), так как данные распределены по ним.
    - Если запрос точечный (например, по ID документа), Elasticsearch вычисляет хэш ID (`hash(document_id) % number_of_shards`) и отправляет запрос только в нужный шард.

2. **Роль Coordinating Node**:
    - Coordinating Node решает, какие шарды или реплики использовать. Он выбирает реплики случайным образом или по принципу минимальной нагрузки на узлы.
    - Если шард недоступен (например, узел выключен), Coordinating Node перенаправляет запрос на реплику этого шарда.

3. **Балансировка нагрузки**:
    - Реплики позволяют распределять запросы между разными узлами, снижая нагрузку на один узел.
    - Например, если у тебя 3 первичных шарда и 1 реплика на каждый, запрос может пойти на 3 узла (каждый обрабатывает один шард или его реплику).

4. **Отказоустойчивость**:
    - Если узел с первичным шардом недоступен, Coordinating Node использует реплику на другом узле.
    - Master Node следит за состоянием кластера и перераспределяет шарды, если узел выходит из строя.

#### Пример маршрутизации:
- Индекс имеет 3 шарда, каждый с 1 репликой, распределённые по 3 узлам:
    - Узел 1: Шард 1 + Реплика Шарда 2
    - Узел 2: Шард 2 + Реплика Шарда 3
    - Узел 3: Шард 3 + Реплика Шарда 1
- Запрос "кот" отправляется на все шарды:
    - Coordinating Node может выбрать Шард 1 (Узел 1), Шард 2 (Узел 2), Шард 3 (Узел 3) или их реплики (например, Реплика Шарда 1 на Узле 3).
    - Если Узел 1 недоступен, запрос на Шард 1 пойдёт на Реплику Шарда 1 (Узел 3).
    
### Почему это эффективно?
- **Query Phase**:
    - Быстрая, потому что использует инвертированный индекс Lucene и работает только с ID и score.
    - Параллельная обработка на всех шардах ускоряет поиск.
- **Fetch Phase**:
    - Обрабатывает только отобранные документы, что минимизирует нагрузку.
    - Данные возвращаются быстро, так как хранятся в сжатом виде.
- **Маршрутизация**:
    - Обеспечивает равномерную нагрузку на узлы благодаря репликам.
    - Гарантирует отказоустойчивость: данные доступны даже при сбое узла.
    
### Пример полного процесса
1. Ты отправляешь запрос `"кот"` в индекс с 3 шардами и 1 репликой.
2. **Query Phase**:
    - Coordinating Node отправляет запрос всем шардам (или их репликам).
    - Шард 1 находит 5 документов, Шард 2 — 3, Шард 3 — 2.
    - Каждый шард возвращает топ-5 ID с их score.
    - Coordinating Node сортирует все 15 документов и выбирает топ-10.
3. **Fetch Phase**:
    - Coordinating Node запрашивает полные данные для 10 выбранных документов у соответствующих шардов.
    - Шарды возвращают документы, например, `{ "name": "Кот Мурзик", "age": 3 }`.
4. **Результат**:
    - Coordinating Node отправляет клиенту итоговый список из 10 документов, отсортированных по релевантности.

### Итог
- **Query Phase**: находит релевантные документы, используя инвертированный индекс Lucene, и возвращает их ID и score.
- **Fetch Phase**: извлекает полное содержимое отобранных документов.
- **Маршрутизация**: распределяет запросы по шардам и репликам, обеспечивая скорость и отказоустойчивость.
  Этот процесс делает Elasticsearch быстрым, масштабируемым и надёжным даже для больших объёмов данных!
  
## 6. Безопасность
Безопасность в **Elasticsearch** — это важный аспект, особенно для кластеров в продакшене. Для её обеспечения используется модуль **X-Pack Security**, который предоставляет инструменты для аутентификации, авторизации и защиты данных с помощью SSL/TLS. 

### Настройка аутентификации и авторизации (X-Pack Security)

**X-Pack Security** — это модуль, который обеспечивает защиту доступа к кластеру Elasticsearch через аутентификацию (проверку, кто вы) и авторизацию (что вам разрешено делать). Начиная с версии Elasticsearch 8.0, X-Pack Security включён по умолчанию в бесплатной версии (Basic license).[](https://severalnines.com/blog/best-practices-elasticsearch-security/)

#### Аутентификация
- **Что это?** Аутентификация проверяет, что пользователь или приложение имеет право доступа к кластеру. Это может быть пароль, сертификат или токен.
- **Как настроить?**
    1. **Включение X-Pack Security**:
        - Открой файл `elasticsearch.yml` на каждом узле кластера (обычно находится в `/etc/elasticsearch/` или `$ES_HOME/config/`).
        - Добавь строку:
          ```yaml
          xpack.security.enabled: true
          ```
        - Перезапусти кластер. В версиях 8.0+ это включено автоматически при первом запуске.[](https://www.elastic.co/docs/deploy-manage/security/set-up-minimal-security)
    2. **Настройка паролей для встроенных пользователей**:
        - После включения X-Pack используй утилиту `elasticsearch-reset-password` для создания паролей встроенным пользователям (например, `elastic` или `kibana_system`).
        - Пример команды для автоматического создания пароля для пользователя `elastic`:
          ```bash
          ./bin/elasticsearch-reset-password -u elastic
          ```
        - Для ручной установки пароля добавь флаг `-i`:
          ```bash
          ./bin/elasticsearch-reset-password -u elastic -i
          ```
        - Сохрани пароли, они понадобятся для доступа к кластеру и Kibana.[](https://www.elastic.co/docs/deploy-manage/security/set-up-minimal-security)
    3. **Другие способы аутентификации**:
        - **PKI (Public Key Infrastructure)**: Используй сертификаты для аутентификации вместо паролей (подробности ниже в разделе SSL/TLS).
        - **LDAP/Active Directory**: Настрой интеграцию с LDAP для корпоративных систем (требует дополнительных настроек в `elasticsearch.yml`).[](https://severalnines.com/blog/best-practices-elasticsearch-security/)
        - **API ключи**: Для программного доступа создавай API ключи через API или Kibana.

#### Авторизация
- **Что это?** Авторизация определяет, какие действия пользователь может выполнять (например, читать, писать, управлять индексами).
- **Как работает?**
    - Elasticsearch использует **Role-Based Access Control (RBAC)**. Ты создаёшь роли с определёнными привилегиями, а затем назначаешь их пользователям.
    - Пример: роль `kibana_system` даёт доступ к функциям Kibana, а роль `superuser` — полный доступ ко всему кластеру.

- **Настройка**:
    1. **Создание ролей**:
        - Используй Kibana (раздел Management → Security → Roles) или API для создания ролей.
        - Пример API-запроса для создания роли `logstash_writer`:
          ```bash
          POST /_security/role/logstash_writer
          {
            "cluster": ["manage_index_templates", "monitor"],
            "indices": {
              "logstash-*": {
                "privileges": ["write", "delete", "create_index"]
              }
            }
          }
          ```
          Эта роль разрешает запись и удаление в индексах, начинающихся с `logstash-`.[](https://www.elastic.co/blog/configuring-ssl-tls-and-https-to-secure-elasticsearch-kibana-beats-and-logstash)
    2. **Назначение ролей пользователям**:
        - Создай пользователя через Kibana или API:
          ```bash
          POST /_security/user/logstash_writer
          {
            "username": "logstash_writer",
            "roles": ["logstash_writer"],
            "password": "your_secure_password",
            "enabled": true
          }
          ```
        - Пользователь `logstash_writer` теперь имеет доступ только к указанным индексам и привилегиям.[](https://www.elastic.co/blog/configuring-ssl-tls-and-https-to-secure-elasticsearch-kibana-beats-and-logstash)
    3. **Анонимный доступ**:
        - По умолчанию анонимные пользователи имеют минимальные права (например, доступ к информации о себе). Настрой анонимный доступ в `elasticsearch.yml`:
          ```yaml
          xpack.security.authc.anonymous.username: anonymous_user
          xpack.security.authc.anonymous.roles: superuser
          xpack.security.authc.anonymous.authz_exception: true
          ```
          **Внимание**: Использование `superuser` для анонимного доступа опасно и не рекомендуется в продакшене.[](https://dev.to/wangpin34/how-to-disable-ssl-authencation-of-elasticsearch-46je)
    
### Использование SSL/TLS для защиты данных

**SSL/TLS** обеспечивает шифрование данных при передаче между узлами кластера и клиентами (например, Kibana или приложениями). Без шифрования данные могут быть перехвачены. В продакшене TLS обязателен, если включена безопасность.[](https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication)

#### Как настроить SSL/TLS?
1. **Создание сертификатов**:
    - Используй утилиту `elasticsearch-certutil` для создания сертификатов:
        - Создай Certificate Authority (CA):
          ```bash
          ./bin/elasticsearch-certutil ca --ca-dn CN=elastic-ca
          ```
          Это создаёт файл `elastic-stack-ca.p12`.[](https://learn.liferay.com/w/dxp/using-search/installing-and-upgrading-a-search-engine/elasticsearch/securing-elasticsearch)
        - Создай сертификаты для узлов:
          ```bash
          ./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --days 3650
          ```
          Это создаёт файл `elastic-certificates.p12`, содержащий сертификат и ключ для узлов.[](https://medium.com/%40musabdogan/enabling-elasticsearch-xpack-security-on-an-unsecured-cluster-79f6ea4023dd)
        - Скопируй `elastic-certificates.p12` в папку `$ES_PATH_CONF/certs/` на каждом узле (по умолчанию `/etc/elasticsearch/certs/`).

2. **Настройка TLS для транспортного уровня** (между узлами):
    - В `elasticsearch.yml` добавь:
      ```yaml
      xpack.security.transport.ssl.enabled: true
      xpack.security.transport.ssl.verification_mode: certificate
      xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12
      xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12
      ```
    - Если сертификат защищён паролем, добавь пароль в keystore:
      ```bash
      ./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password
      ./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password
      ```
    - Это шифрует коммуникацию между узлами (порт 9300).[](https://www.elastic.co/docs/deploy-manage/security/set-up-basic-security)

3. **Настройка TLS для HTTP уровня** (для клиентов):
    - В `elasticsearch.yml` добавь:
      ```yaml
      xpack.security.http.ssl.enabled: true
      xpack.security.http.ssl.keystore.path: certs/elastic-certificates.p12
      xpack.security.http.ssl.truststore.path: certs/elastic-certificates.p12
      ```
    - Если нужен пароль, добавь его в keystore, как выше.[](https://www.elastic.co/blog/configuring-ssl-tls-and-https-to-secure-elasticsearch-kibana-beats-and-logstash)
    - Теперь клиенты (например, Kibana) должны использовать `https://` вместо `http://` для подключения (порт 9200).

4. **Настройка Kibana для TLS**:
    - В `kibana.yml` укажи:
      ```yaml
      elasticsearch.url: "https://localhost:9200"
      xpack.security.enabled: true
      elasticsearch.ssl.certificate: config/certs/client.cer
      elasticsearch.ssl.key: config/certs/client.key
      elasticsearch.ssl.certificateAuthorities: config/certs/client-ca.cer
      ```
    - Перезапусти Kibana.[](https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication)

5. **Перезапуск кластера**:
    - После настройки TLS требуется полный перезапуск кластера, так как узлы с TLS не могут общаться с узлами без TLS.[](https://medium.com/%40musabdogan/enabling-elasticsearch-xpack-security-on-an-unsecured-cluster-79f6ea4023dd)

#### Советы:
- Для продакшен-кластеров используй `verification_mode: full` для проверки имени хоста (нужны отдельные сертификаты для каждого узла).[](https://www.elastic.co/docs/deploy-manage/security/set-up-basic-security)
- Храни сертификаты в безопасном месте и проверяй права доступа к файлам.
- Если возникают ошибки, проверь логи (`/var/log/elasticsearch/`) и убедись, что пути к сертификатам правильные.[](https://stackoverflow.com/questions/73861951/elastic-search-failed-to-start-invalid-ssl-configuration-for-xpack-security-tr)

### Управление ролями и пользователями

Управление ролями и пользователями в Elasticsearch основано на **RBAC** (Role-Based Access Control) и позволяет гибко настраивать доступ.

#### Создание и управление ролями
- Роли определяют, какие действия разрешены пользователю (например, чтение, запись, управление кластером).
- **Пример создания роли** (через API или Kibana):
    - Создай роль для чтения индекса `logs-*`:
      ```bash
      POST /_security/role/logs_reader
      {
        "indices": {
          "logs-*": {
            "privileges": ["read", "view_index_metadata"]
          }
        }
      }
      ```
    - Роль `logs_reader` даёт доступ только к чтению индексов, начинающихся с `logs-`.
- **Встроенные роли**:
    - `superuser`: полный доступ ко всему.
    - `kibana_system`: для работы Kibana.
    - `logstash_system`: для Logstash.
    - Можно создавать свои роли для точечного управления доступом.[](https://severalnines.com/blog/best-practices-elasticsearch-security/)

#### Создание и управление пользователями
- Пользователи создаются с привязкой к ролям.
- **Пример создания пользователя**:
    - Через API:
      ```bash
      POST /_security/user/analyst
      {
        "password": "secure_password",
        "roles": ["logs_reader"],
        "full_name": "Data Analyst",
        "email": "analyst@example.com"
      }
      ```
    - Через Kibana: Management → Security → Users → Create User.
- **Встроенные пользователи**:
    - `elastic`: суперпользователь для администрирования.
    - `kibana_system`: для подключения Kibana.
    - Пароли для них настраиваются через `elasticsearch-reset-password`.[](https://www.elastic.co/docs/deploy-manage/security/set-up-minimal-security)

#### Управление через PKI
- Если используется PKI-аутентификация (сертификаты), назначь роли сертификатам через **role mapping**:
    - Пример:
      ```bash
      PUT /_security/role_mapping/kibana_certificate_authorization
      {
        "roles": ["kibana_system"],
        "rules": {
          "field": {
            "dn": "CN=something,OU=Consulting Team,DC=mydomain,DC=com"
          }
        },
        "enabled": true
      }
      ```
    - Это позволяет сертификату с указанным DN (Distinguished Name) получить роль `kibana_system`.[](https://www.elastic.co/blog/elasticsearch-security-configure-tls-ssl-pki-authentication)

#### Лучшие практики:
- **Принцип минимальных привилегий**: Давай пользователям только те права, которые им нужны.
- **Регулярное обновление паролей**: Используй `elasticsearch-reset-password` для смены паролей.
- **Включение аудита**: Настрой аудит-логирование в `elasticsearch.yml` для отслеживания событий безопасности:
  ```yaml
  xpack.security.audit.enabled: true
  ```
  Это помогает отслеживать попытки входа и изменения конфигурации.[](https://severalnines.com/blog/best-practices-elasticsearch-security/)

### Итог
- **Аутентификация и авторизация (X-Pack Security)**:
    - Включи `xpack.security.enabled: true` в `elasticsearch.yml`.
    - Настрой пароли для встроенных пользователей (`elastic`, `kibana_system`).
    - Используй RBAC для создания ролей и назначения их пользователям.
- **SSL/TLS**:
    - Создай сертификаты с помощью `elasticsearch-certutil`.
    - Настрой TLS для транспортного (`xpack.security.transport.ssl.enabled`) и HTTP (`xpack.security.http.ssl.enabled`) уровней.
    - Перезапусти кластер после настройки.
- **Управление ролями и пользователями**:
    - Создавай роли с минимальными привилегиями через API или Kibana.
    - Управляй пользователями, привязывая их к ролям.
    - Для PKI используй role mapping для сертификатов.

## 7 **Создание и управление индексами**

Создание и управление индексами в **Elasticsearch** — это основа работы с данными. Индексы определяют, как данные хранятся, индексируются и ищутся. Важными аспектами являются настройка **маппингов** (структуры данных), использование **динамических и статических маппингов**, а также управление **настройками индекса** (количество шардов, реплик, анализаторы). 

### Настройка маппингов (mappings) для определения структуры данных

**Маппинги** — это схема данных в индексе, которая определяет, как Elasticsearch будет обрабатывать поля в документах (например, типы данных, как их индексировать и искать). Это похоже на структуру таблицы в базе данных.

#### Что такое маппинги?
- Маппинги указывают, какие поля есть в документах, их типы (например, `text`, `keyword`, `integer`, `date`) и как они обрабатываются (например, индексировать для поиска или нет).
- Пример: для документа `{ "name": "Кот Мурзик", "age": 3, "birth_date": "2022-01-01" }` маппинг определяет, что `name` — это текст, `age` — целое число, а `birth_date` — дата.

#### Как настроить маппинги?
1. **Создание индекса с маппингом**:
    - Используй API для создания индекса с явным маппингом:
      ```bash
      PUT /pets
      {
        "mappings": {
          "properties": {
            "name": { "type": "text" },           // Для полнотекстового поиска
            "age": { "type": "integer" },         // Для числовых операций
            "birth_date": { "type": "date" },     // Для дат
            "type": { "type": "keyword" }         // Для точного соответствия
          }
        }
      }
      ```
    - Это создаёт индекс `pets` с заданной структурой.

2. **Добавление маппинга в существующий индекс**:
    - Если индекс уже существует, можно добавить новое поле:
      ```bash
      PUT /pets/_mapping
      {
        "properties": {
          "color": { "type": "keyword" }
        }
      }
      ```
    - **Важно**: Нельзя изменить тип уже существующего поля (например, `text` на `keyword`). Для этого нужно создать новый индекс и перенести данные (reindex).

3. **Проверка маппинга**:
    - Узнай текущий маппинг:
      ```bash
      GET /pets/_mapping
      ```

#### Основные типы данных в маппингах:
- `text`: Для полнотекстового поиска (анализируется, разбивается на токены). Пример: `"Кот Мурзик"` → `кот`, `мурзик`.
- `keyword`: Для точного соответствия (не анализируется). Пример: `"Кот Мурзик"` остаётся как есть.
- `integer`, `long`, `float`, `double`: Для чисел.
- `date`: Для дат (например, `"2022-01-01"`).
- `boolean`: Для значений `true`/`false`.
- `object`, `nested`: Для сложных структур (вложенных JSON).

#### Пример:
Если ты добавляешь документ `{ "name": "Кот Мурзик", "age": 3 }` в индекс `pets`, Elasticsearch будет использовать маппинг, чтобы:
- Индексировать `name` как текст для поиска (например, по запросу "мурзик").
- Хранить `age` как число для фильтров (например, `age > 2`).

### Использование динамических и статических маппингов

Elasticsearch поддерживает два подхода к маппингам: **динамические** (автоматические) и **статические** (явно заданные).

#### Динамические маппинги
- **Что это?** Elasticsearch автоматически определяет типы полей на основе данных, если маппинг не задан явно.
- **Как работает?**
    - Если ты добавляешь документ `{ "name": "Кот Мурзик", "age": 3 }` без маппинга, Elasticsearch сам решает:
        - `name` → `text` (с дополнительным полем `name.keyword` для точного соответствия).
        - `age` → `integer`.
    - Это удобно для быстрого старта, но может привести к ошибкам (например, строка `"123"` может быть проиндексирована как число, если первым попался числовой формат).

- **Настройка**:
    - Динамическое создание маппингов включено по умолчанию. Можно управлять в `mappings`:
      ```bash
      PUT /pets
      {
        "mappings": {
          "dynamic": true  // Включить (по умолчанию)
        }
      }
      ```
    - Отключение динамических маппингов:
      ```bash
      PUT /pets
      {
        "mappings": {
          "dynamic": false  // Запрещает добавление новых полей
        }
      }
      ```
    - Ограниченное динамическое создание:
      ```bash
      PUT /pets
      {
        "mappings": {
          "dynamic": "strict"  // Выдаёт ошибку, если добавляется неизвестное поле
        }
      }
      ```

#### Статические маппинги
- **Что это?** Ты явно задаёшь структуру полей при создании индекса.
- **Зачем?**
    - Контроль над типами данных и их обработкой.
    - Предотвращение ошибок, когда данные приходят в неожиданном формате.
    - Оптимизация производительности (динамическое определение может быть медленнее).

- **Пример**:
    - Явно задаём маппинг:
      ```bash
      PUT /pets
      {
        "mappings": {
          "dynamic": "strict",  // Только указанные поля разрешены
          "properties": {
            "name": { "type": "text" },
            "age": { "type": "integer" }
          }
        }
      }
      ```
    - Если добавить документ с неизвестным полем (например, `color`), Elasticsearch выдаст ошибку.

#### Когда использовать?
- **Динамические**: Для прототипов или данных с неизвестной структурой (например, логи).
- **Статические**: Для продакшен-систем, где важна точная структура и производительность.

### Управление настройками индекса (settings)

Настройки индекса определяют, как он будет работать: сколько шардов и реплик, какие анализаторы использовать и другие параметры.

#### Основные настройки
1. **Количество шардов и реплик**:
    - **Шарды**: Делят индекс на части для распределения данных и параллельного поиска.
    - **Реплики**: Копии шардов для отказоустойчивости и ускорения запросов.
    - Пример создания индекса с 3 шардами и 1 репликой:
      ```bash
      PUT /pets
      {
        "settings": {
          "number_of_shards": 3,  // Первичные шарды
          "number_of_replicas": 1 // По одной копии каждого шарда
        },
        "mappings": {
          "properties": {
            "name": { "type": "text" }
          }
        }
      }
      ```
    - **Изменение реплик** (можно на лету):
      ```bash
      PUT /pets/_settings
      {
        "number_of_replicas": 2
      }
      ```
    - **Важно**: Количество шардов нельзя изменить после создания индекса (нужен reindex).

2. **Анализаторы**:
    - Анализаторы определяют, как текст разбивается на токены для полнотекстового поиска.
    - Встроенные анализаторы: `standard` (по умолчанию), `simple`, `whitespace`, `keyword`.
    - Пример настройки анализатора для поля `name` с поддержкой русского языка:
      ```bash
      PUT /pets
      {
        "settings": {
          "analysis": {
            "analyzer": {
              "russian_analyzer": {
                "type": "custom",
                "tokenizer": "standard",
                "filter": ["lowercase", "russian_morphology"]
              }
            }
          }
        },
        "mappings": {
          "properties": {
            "name": {
              "type": "text",
              "analyzer": "russian_analyzer" // Применяем кастомный анализатор
            }
          }
        }
      }
      ```
    - Это позволяет искать, например, "коты" и находить "кот" или "кота" благодаря морфологии.

3. **Другие настройки**:
    - **refresh_interval**: Как часто индекс обновляется для поиска (по умолчанию `1s`):
      ```bash
      PUT /pets/_settings
      {
        "refresh_interval": "30s" // Реже обновлять для экономии ресурсов
      }
      ```
    - **max_result_window**: Максимальное количество результатов в запросе (по умолчанию 10000):
      ```bash
      PUT /pets/_settings
      {
        "max_result_window": 50000
      }
      ```

#### Управление индексом
- **Создание индекса**:
  ```bash
  PUT /pets
  ```
- **Удаление индекса**:
  ```bash
  DELETE /pets
  ```
- **Закрытие/открытие индекса** (для экономии ресурсов):
  ```bash
  POST /pets/_close
  POST /pets/_open
  ```
- **Переиндексация** (для изменения структуры или настроек):
  ```bash
  POST /_reindex
  {
    "source": { "index": "pets" },
    "dest": { "index": "pets_v2" }
  }
  ```

#### Лучшие практики:
- **Шарды**: Используй 1–5 шардов на индекс для небольших данных. Для больших данных рассчитывай ~20–50 ГБ на шард.
- **Реплики**: Минимум 1 реплика для отказоустойчивости. Увеличивай для высоконагруженных систем.
- **Анализаторы**: Настраивай под язык данных (например, `russian_morphology` для русского).
- **Мониторинг**: Проверяй состояние шардов (`GET /_cat/shards`) и производительность.

### Итог
- **Маппинги**: Определяют структуру данных (`text`, `keyword`, `date`) и правила индексации. Задавай их явно для контроля или используй динамические для гибкости.
- **Динамические vs статические маппинги**: Динамические удобны для прототипов, статические — для продакшена.
- **Настройки индекса**: Управляй шардами (для распределения данных), репликами (для отказоустойчивости) и анализаторами (для поиска по языку).
  Эти инструменты делают Elasticsearch гибким и эффективным для хранения и поиска данных!
  
## 8  **Индексирование данных**

Импорт данных в **Elasticsearch** — это важный процесс для заполнения индексов данными. Существует несколько способов импорта: через **API** (включая Bulk API), **Logstash** и **Beats**. 

### Импорт данных через API, Logstash или Beats

#### a) Импорт через API
Elasticsearch предоставляет REST API для добавления данных в индексы. Это самый прямой способ, подходящий для программного управления или небольших объёмов данных.

- **Как работает?**
    - Используй HTTP-запросы (например, `POST` или `PUT`) для отправки документов в индекс.
    - Документы отправляются в формате JSON.
    - Подходит для ручного добавления или интеграции с приложениями.

- **Пример**:
    - Добавление одного документа в индекс `pets`:
      ```bash
      POST /pets/_doc
      {
        "name": "Кот Мурзик",
        "age": 3,
        "type": "cat"
      }
      ```
    - Ответ от Elasticsearch содержит ID документа и статус:
      ```json
      {
        "_index": "pets",
        "_id": "abc123",
        "_version": 1,
        "result": "created"
      }
      ```

- **Когда использовать?**
    - Для добавления отдельных документов.
    - Для интеграции с приложениями (например, через Python с библиотекой `elasticsearch`).
    - Для тестирования или небольших данных.

- **Совет**:
    - Используй явный ID для документов (`PUT /pets/_doc/1`), если нужно контролировать идентификаторы.
    - Для больших объёмов данных используй **Bulk API** (см. ниже).

#### b) Импорт через Logstash
**Logstash** — это мощный инструмент для сбора, обработки и отправки данных в Elasticsearch. Он идеально подходит для работы с потоками данных, такими как логи, метрики или данные из баз.

- **Как работает?**
    - Logstash использует **конвейеры** (pipelines), состоящие из трёх этапов:
        1. **Input**: Сбор данных (из файлов, баз данных, Kafka, HTTP и т.д.).
        2. **Filter**: Обработка данных (например, извлечение полей, преобразование форматов).
        3. **Output**: Отправка данных в Elasticsearch или другие системы.
    - Настраивается через конфигурационный файл (например, `logstash.conf`).

- **Пример конфигурации**:
    - Файл `logstash.conf` для чтения логов из файла и отправки в Elasticsearch:
      ```conf
      input {
        file {
          path => "/var/log/app.log"
          start_position => "beginning"
        }
      }
      filter {
        grok {
          match => { "message": "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:msg}" }
        }
      }
      output {
        elasticsearch {
          hosts => ["http://localhost:9200"]
          index => "applogs-%{+YYYY.MM.dd}"
          user => "elastic"
          password => "your_password"
        }
      }
      ```
    - **Объяснение**:
        - **Input**: Читает логи из файла `/var/log/app.log`.
        - **Filter**: Разбирает строки логов, извлекая `timestamp`, `level` и `msg`.
        - **Output**: Отправляет данные в индекс `applogs-2025.07.10` (индекс создаётся ежедневно).

- **Запуск Logstash**:
  ```bash
  ./bin/logstash -f logstash.conf
  ```

- **Когда использовать?**
    - Для обработки логов (например, Nginx, Apache).
    - Для интеграции с базами данных (через плагин `jdbc`).
    - Для сложной обработки данных (например, парсинг, обогащение).

- **Совет**:
    - Настрой фильтры для очистки и структурирования данных.
    - Используй TLS и аутентификацию для безопасной передачи данных.

#### c) Импорт через Beats
**Beats** — это лёгкие агенты для сбора данных (логи, метрики, сетевые пакеты) и отправки их в Elasticsearch или Logstash.

- **Популярные Beats**:
    - **Filebeat**: Для сбора и отправки логов (например, из файлов).
    - **Metricbeat**: Для сбора метрик (CPU, память, процессы).
    - **Packetbeat**: Для анализа сетевых данных.
    - **Winlogbeat**: Для логов Windows.

- **Как работает?**
    - Beats устанавливаются на серверах, собирают данные и отправляют их в Elasticsearch напрямую или через Logstash для дополнительной обработки.
    - Настраиваются через YAML-файлы.

- **Пример конфигурации Filebeat**:
    - Файл `filebeat.yml`:
      ```yaml
      filebeat.inputs:
      - type: log
        enabled: true
        paths:
          - /var/log/nginx/*.log
      output.elasticsearch:
        hosts: ["http://localhost:9200"]
        index: "nginx-logs-%{+YYYY.MM.dd}"
        username: "elastic"
        password: "your_password"
      ```
    - **Объяснение**:
        - Читает логи Nginx.
        - Отправляет их в индекс `nginx-logs-2025.07.10`.

- **Запуск Filebeat**:
  ```bash
  ./filebeat -e -c filebeat.yml
  ```

- **Когда использовать?**
    - Для лёгкого сбора логов или метрик с серверов.
    - Когда нужна минимальная нагрузка на систему (Beats легче, чем Logstash).
    - Для прямой отправки данных в Elasticsearch без сложной обработки.

- **Совет**:
    - Используй Logstash для сложной обработки, а Beats — для простого сбора.
    - Настрой TLS для безопасной передачи данных.
    
### Пакетная загрузка данных (Bulk API)

**Bulk API** — это способ массовой загрузки или обновления документов в Elasticsearch за один запрос. Он значительно ускоряет импорт больших объёмов данных, так как минимизирует сетевые запросы.

#### Как работает Bulk API?
- Bulk API принимает набор операций (добавление, обновление, удаление) в одном запросе.
- Данные отправляются в формате **NDJSON** (Newline Delimited JSON), где каждая операция состоит из двух строк:
    - **Метаданные**: Указывают действие (`index`, `create`, `update`, `delete`) и параметры (индекс, ID документа).
    - **Данные**: JSON с содержимым документа (для `index`, `create`, `update`).

- **Пример запроса**:
  ```bash
  POST /_bulk
  { "index": { "_index": "pets", "_id": "1" } }
  { "name": "Кот Мурзик", "age": 3, "type": "cat" }
  { "index": { "_index": "pets", "_id": "2" } }
  { "name": "Собака Рекс", "age": 5, "type": "dog" }
  { "delete": { "_index": "pets", "_id": "3" } }
  ```
    - **Объяснение**:
        - Первая строка: добавляет документ с ID `1` в индекс `pets`.
        - Вторая строка: добавляет документ с ID `2`.
        - Третья строка: удаляет документ с ID `3`.

- **Ответ**:
  ```json
  {
    "took": 30,
    "errors": false,
    "items": [
      { "index": { "_index": "pets", "_id": "1", "result": "created", "status": 201 } },
      { "index": { "_index": "pets", "_id": "2", "result": "created", "status": 201 } },
      { "delete": { "_index": "pets", "_id": "3", "result": "not_found", "status": 404 } }
    ]
  }
  ```

#### Типы операций в Bulk API
- `index`: Добавляет или заменяет документ.
- `create`: Добавляет документ, но выдаёт ошибку, если ID уже существует.
- `update`: Обновляет существующий документ (например, частичное обновление полей).
- `delete`: Удаляет документ по ID.

#### Пример обновления:
```bash
POST /_bulk
{ "update": { "_index": "pets", "_id": "1" } }
{ "doc": { "age": 4 } }
```
- Обновляет возраст кота Мурзика до 4 лет.

#### Лучшие практики для Bulk API
- **Размер пакета**: Отправляй 1–10 МБ данных за раз (обычно 100–1000 документов). Слишком большие пакеты могут перегрузить кластер.
- **Параллельные запросы**: Используй несколько потоков для отправки bulk-запросов, чтобы ускорить импорт.
- **Обработка ошибок**: Если одна операция в пакете завершилась с ошибкой, остальные всё равно выполняются. Проверяй поле `errors` в ответе.
- **Оптимизация**:
    - Увеличь `refresh_interval` (например, `30s`) во время массовой загрузки:
      ```bash
      PUT /pets/_settings
      { "refresh_interval": "30s" }
      ```
    - После загрузки верни стандартное значение:
      ```bash
      PUT /pets/_settings
      { "refresh_interval": "1s" }
      ```

#### Когда использовать?
- Для импорта больших объёмов данных (например, загрузка миллионов логов).
- Для оптимизации производительности при массовой индексации.

### Сравнение методов импорта
| Метод        | Плюсы                                                                 | Минусы                                                      | Когда использовать?                              |
|--------------|----------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------|
| **API**      | Простота, гибкость, подходит для небольших данных                     | Медленно для больших объёмов                                | Тестирование, интеграция с кодом, малые данные  |
| **Logstash** | Мощная обработка данных, интеграция с базами, Kafka, фильтры          | Требует настройки, больше ресурсов                         | Обработка логов, сложные данные, потоки         |
| **Beats**    | Лёгкий, минимальная нагрузка, простой для логов и метрик              | Ограниченные возможности обработки                         | Сбор логов/метрик с серверов                   |
| **Bulk API** | Высокая производительность, массовая загрузка                         | Требует форматирования NDJSON, сложнее для ручной работы    | Импорт больших объёмов данных                  |

### Итог
- **API**: Простой способ для добавления отдельных документов через HTTP-запросы.
- **Logstash**: Для сложной обработки и потоковой загрузки данных (логи, базы данных).
- **Beats**: Лёгкие агенты для сбора логов и метрик с минимальной нагрузкой.
- **Bulk API**: Идеально для массовой загрузки данных, используй NDJSON и оптимизируй настройки (например, `refresh_interval`).
  Выбирай метод в зависимости от объёма данных и сложности обработки. Для продакшен-систем комбинируй Logstash/Beats для сбора и Bulk API для массовой загрузки!
  

## 9  **Анализаторы текста**

Анализаторы текста в **Elasticsearch** — это инструменты, которые обрабатывают текст для полнотекстового поиска. Они разбивают текст на части (токены), чтобы Elasticsearch мог эффективно искать и находить совпадения, даже если слова написаны в разных формах (например, "кот" и "кота"). Это особенно важно для поддержки языков, таких как русский, где слова изменяются по падежам, числам и т.д.

### Понимание токенизаторов, фильтров и анализаторов

Анализатор текста в Elasticsearch состоит из трёх компонентов, которые работают последовательно:
- **Character Filters** (символьные фильтры): Предобрабатывают текст, удаляя или заменяя символы.
- **Tokenizer** (токенизатор): Разбивает текст на токены (слова или части слов).
- **Token Filters** (токен-фильтры): Изменяют или фильтруют токены (например, приводят к нижнему регистру, удаляют стоп-слова или применяют морфологию).

#### Как это работает?
1. **Character Filters**:
    - Удаляют или заменяют ненужные символы до токенизации.
    - Примеры:
        - `html_strip`: Удаляет HTML-теги (например, `<p>Кот</p>` → `Кот`).
        - `mapping`: Заменяет символы (например, `ё` → `е`).
    - Используется редко, но полезно для очистки "грязных" данных.

2. **Tokenizer**:
    - Разбивает текст на токены (обычно слова).
    - Примеры токенизаторов:
        - `standard`: Делит текст по пробелам и знакам препинания. Например, `"Кот ест рыбу."` → `["Кот", "ест", "рыбу"]`.
        - `whitespace`: Делит только по пробелам. Например, `"Кот-ест рыбу"` → `["Кот-ест", "рыбу"]`.
        - `keyword`: Не разбивает текст, хранит как один токен. Например, `"Кот ест рыбу"` → `["Кот ест рыбу"]`.

3. **Token Filters**:
    - Преобразуют токены после токенизатора. Примеры:
        - `lowercase`: Приводит к нижнему регистру (`Кот` → `кот`).
        - `stop`: Удаляет стоп-слова (например, "и", "в" для русского).
        - `snowball` или `hunspell`: Приводит слова к начальной форме (например, `кота` → `кот`).
        - `synonym`: Добавляет синонимы (например, `кот` → `кот`, `кошка`).

#### Анализатор
- Анализатор объединяет символьные фильтры, токенизатор и токен-фильтры в один процесс.
- Встроенные анализаторы:
    - `standard`: `standard tokenizer` + `lowercase` + минимальная обработка.
    - `russian`: `standard tokenizer` + `lowercase` + `russian_stop` + `russian_morphology` (для русского языка).
- Пример работы анализатора `russian`:
    - Вход: `"Коты едят рыбу"`
    - Выход: `["кот", "есть", "рыба"]` (приведено к нижнему регистру и начальной форме).

#### Поддержка русского языка
- Русский язык сложен из-за морфологии (падежи, числа, склонения). Для корректной обработки нужен анализатор, который:
    - Приводит слова к начальной форме (например, `котов` → `кот`).
    - Удаляет стоп-слова ("и", "на", "в").
    - Игнорирует регистр.
- Встроенный анализатор `russian` уже включает `russian_morphology` и `russian_stop`, что делает его подходящим для большинства задач.

#### Пример анализа текста
- Используем API для проверки, как работает анализатор:
  ```bash
  POST /_analyze
  {
    "analyzer": "russian",
    "text": "Коты едят рыбу"
  }
  ```
- Ответ:
  ```json
  {
    "tokens": [
      { "token": "кот", "start_offset": 0, "end_offset": 4, "type": "<ALPHANUM>", "position": 0 },
      { "token": "есть", "start_offset": 5, "end_offset": 9, "type": "<ALPHANUM>", "position": 1 },
      { "token": "рыба", "start_offset": 10, "end_offset": 14, "type": "<ALPHANUM>", "position": 2 }
    ]
  }
  ```
- **Объяснение**: Текст разбит на токены, приведён к нижнему регистру, а слово "коты" преобразовано в "кот" благодаря морфологии.

### Настройка кастомных анализаторов для специфических задач

Кастомные анализаторы позволяют настроить обработку текста под конкретные требования, например:
- Поиск с учётом синонимов.
- Игнорирование определённых символов.
- Специфическая обработка текста (например, для доменных имён или хэштегов).

#### Как создать кастомный анализатор?
1. **Определение анализатора**:
    - Анализатор задаётся в настройках индекса (`settings`) при его создании.
    - Укажи символьные фильтры, токенизатор и токен-фильтры.

2. **Пример кастомного анализатора для русского языка**:
    - Задача: Создать анализатор, который:
        - Удаляет HTML-теги.
        - Разбивает текст на слова.
        - Приводит к нижнему регистру.
        - Применяет морфологию для русского языка.
        - Добавляет синонимы (например, `кот` = `кошка`).

    - Код для создания индекса с кастомным анализатором:
      ```bash
      PUT /pets
      {
        "settings": {
          "analysis": {
            "char_filter": {
              "html_clean": {
                "type": "html_strip" //Удаляет HTML-теги
              }
            },
            "analyzer": {
              "custom_russian": {
                "type": "custom",
                "char_filter": ["html_clean"],
                "tokenizer": "standard",
                "filter": [
                  "lowercase",
                  "russian_morphology",
                  "russian_stop",
                  "synonym_filter"
                ]
              }
            },
            "filter": {
              "synonym_filter": {
                "type": "synonym",
                "synonyms": [
                  "кот, кошка",
                  "собака, пёс"
                ]
              },
              "russian_stop": {
                "type": "stop",
                "stopwords": "_russian_"
              },
              "russian_morphology": {
                "type": "hunspell",
                "locale": "ru_RU",
                "dedup": true
              }
            }
          }
        },
        "mappings": {
          "properties": {
            "name": {
              "type": "text",
              "analyzer": "custom_russian"
            },
            "type": { "type": "keyword" }
          }
        }
      }
      ```

3. **Объяснение**:
    - **char_filter**:
        - `html_strip`: Удаляет HTML-теги (например, `<p>Кот</p>` → `Кот`).
    - **tokenizer**:
        - `standard`: Разбивает текст на слова по пробелам и знакам препинания.
    - **filter**:
        - `lowercase`: Приводит к нижнему регистру (`Кот` → `кот`).
        - `russian_stop`: Удаляет стоп-слова ("и", "в").
        - `russian_morphology`: Приводит слова к начальной форме (`коты` → `кот`).
        - `synonym_filter`: Добавляет синонимы (`кот` → `кот`, `кошка`).
    - **mappings**: Поле `name` использует кастомный анализатор `custom_russian`.

4. **Тестирование анализатора**:
    - Проверь, как работает анализатор:
      ```bash
      POST /pets/_analyze
      {
        "analyzer": "custom_russian",
        "text": "<p>Коты едят рыбу</p>"
      }
      ```
    - Ответ:
      ```json
      {
        "tokens": [
          { "token": "кот", "start_offset": 3, "end_offset": 7, "type": "<ALPHANUM>", "position": 0 },
          { "token": "кошка", "start_offset": 3, "end_offset": 7, "type": "SYNONYM", "position": 0 },
          { "token": "есть", "start_offset": 8, "end_offset": 12, "type": "<ALPHANUM>", "position": 1 },
          { "token": "рыба", "start_offset": 13, "end_offset": 17, "type": "<ALPHANUM>", "position": 2 }
        ]
      }
      ```
    - **Результат**: HTML-теги удалены, "коты" преобразовано в "кот" и добавлен синоним "кошка", стоп-слова убраны.

5. **Добавление документа**:
   ```bash
   POST /pets/_doc
   {
     "name": "Коты едят рыбу",
     "type": "cat"
   }
   ```
    - Теперь поиск по "кошка" или "кот" найдёт этот документ.

#### Лучшие практики
- **Тестируй анализаторы**: Используй `_analyze` API для проверки, как текст преобразуется в токены.
- **Учитывай язык**: Для русского используй `russian_morphology` или `hunspell` с локальным словарем.
- **Разделяй анализаторы для индексации и поиска**:
    - Поле `search_analyzer` можно задать отдельно, чтобы поиск был менее строгим.
    - Пример: `autocomplete_analyzer` для индексации, `standard` для поиска.
- **Оптимизация**: Не добавляй слишком много фильтров, чтобы не замедлить индексацию.
- **Закрывай индекс перед изменением анализаторов**:
  ```bash
  POST /pets/_close
  PUT /pets/_settings
  {
    "analysis": { ... }
  }
  POST /pets/_open
  ```

### Итог
- **Токенизаторы, фильтры и анализаторы**:
    - **Character Filters**: Очищают текст (например, убирают HTML).
    - **Tokenizer**: Разбивает текст на токены (например, `standard`, `keyword`).
    - **Token Filters**: Преобразуют токены (`lowercase`, `russian_morphology`, `synonym`).
    - Для русского языка используй `russian` анализатор или кастомный с `russian_morphology`.
- **Кастомные анализаторы**:
    - Настраивай для специфических задач: синонимы, хэштеги, автодополнение.
    - Задавай в `settings` индекса, тестируй через `_analyze`.
- **Пример**: Анализатор `custom_russian` с морфологией и синонимами делает поиск по русскому тексту гибким и точным.
  Эти инструменты позволяют Elasticsearch эффективно обрабатывать текст, особенно для сложных языков, таких как русский!
  
## 10 **Типы запросов**:
В **Elasticsearch** запросы и аналитика данных строятся с использованием **Query DSL** (Domain Specific Language), который позволяет выполнять поиск, фильтрацию и агрегацию данных. 

### Query DSL: Синтаксис запросов

**Query DSL** — это JSON-основанный язык запросов в Elasticsearch, который позволяет гибко искать и фильтровать данные. Запросы делятся на два контекста:
- **Query context**: Используется для полнотекстового поиска, где учитывается релевантность (score). Примеры: `match`, `multi_match`.
- **Filter context**: Используется для точной фильтрации без подсчёта релевантности. Примеры: `term`, `range`.

Запросы отправляются через API, обычно в `_search` эндпоинт:
```bash
POST /pets/_search
{
  "query": { ... }
}
```

#### Основные типы запросов
1. **match**:
    - Используется для полнотекстового поиска с анализом текста (разбивает запрос на токены, учитывает морфологию).
    - Пример: Поиск документов, где в поле `name` есть слово "кот":
      ```bash
      POST /pets/_search
      {
        "query": {
          "match": {
            "name": "кот"
          }
        }
      }
      ```
    - Находит документы с "кот", "коты", "кошка" (если используется анализатор `russian`).

2. **term**:
    - Точный поиск по значению поля (без анализа, подходит для `keyword`).
    - Пример: Поиск документов, где `type` точно равно "cat":
      ```bash
      POST /pets/_search
      {
        "query": {
          "term": {
            "type": "cat"
          }
        }
      }
      ```

3. **bool**:
    - Комбинирует несколько условий (`must`, `should`, `must_not`, `filter`).
    - Пример: Найти документы, где `name` содержит "кот" и `age` больше 2, но не "dog":
      ```bash
      POST /pets/_search
      {
        "query": {
          "bool": {
            "must": [
              { "match": { "name": "кот" } },
              { "range": { "age": { "gt": 2 } } }
            ],
            "must_not": [
              { "term": { "type": "dog" } }
            ]
          }
        }
      }
      ```
    - **must**: Все условия должны быть истинными (AND).
    - **should**: Хотя бы одно условие должно быть истинным (OR, если нет `must`).
    - **must_not**: Условия не должны выполняться (NOT).
    - **filter**: Условия без подсчёта релевантности (см. ниже).

4. **range**:
    - Поиск по диапазону значений (для чисел, дат).
    - Пример: Найти животных старше 2 лет:
      ```bash
      POST /pets/_search
      {
        "query": {
          "range": {
            "age": {
              "gte": 2,
              "lte": 5
            }
          }
        }
      }
      ```
    - `gte` (≥), `lte` (≤), `gt` (>), `lt` (<).

5. **exists**:
    - Проверяет наличие поля в документе.
    - Пример: Найти документы, где есть поле `color`:
      ```bash
      POST /pets/_search
      {
        "query": {
          "exists": {
            "field": "color"
          }
        }
      }
      ```

6. **prefix**, **wildcard**, **regexp**:
    - Для частичного соответствия.
    - Пример: Найти документы, где `name` начинается с "ко":
      ```bash
      POST /pets/_search
      {
        "query": {
          "prefix": {
            "name.keyword": "ко"
          }
        }
      }
      ```
    - `wildcard`: Например, `"ко*"` (любой текст после "ко").
    - **Осторожно**: Эти запросы могут быть медленными на больших данных.
    
### Полнотекстовый поиск: match, multi_match, query_string

Полнотекстовый поиск анализирует текст запроса и документов, чтобы найти релевантные совпадения, учитывая морфологию, синонимы и т.д.

#### a) match
- Используется для поиска по одному полю с анализом текста.
- Пример: Найти документы, где в `name` есть "кот":
  ```bash
  POST /pets/_search
  {
    "query": {
      "match": {
        "name": {
          "query": "кот кошка",
          "operator": "or" // Найти "кот" ИЛИ "кошка"
        }
      }
    }
  }
  ```
- **Параметры**:
    - `operator`: `"or"` (по умолчанию, любое слово) или `"and"` (все слова).
    - `minimum_should_match`: Минимальное количество совпадающих слов (например, `"70%"`).

#### b) multi_match
- Поиск по нескольким полям одновременно.
- Пример: Найти "кот" в полях `name` и `description`:
  ```bash
  POST /pets/_search
  {
    "query": {
      "multi_match": {
        "query": "кот",
        "fields": ["name", "description^2"], // description имеет больший вес
        "type": "best_fields"
      }
    }
  }
  ```
- **Параметры**:
    - `fields`: Список полей для поиска. Можно задавать вес (`^2` увеличивает важность).
    - `type`: `best_fields` (лучшее совпадение), `most_fields` (больше совпадений), `cross_fields` (единый анализ).

#### c) query_string
- Гибкий поиск с использованием синтаксиса запросов (поддерживает операторы `AND`, `OR`, `NOT`, wildcards и т.д.).
- Пример: Найти документы, где в `name` есть "кот" или "кошка", но не "собака":
  ```bash
  POST /pets/_search
  {
    "query": {
      "query_string": {
        "query": "(кот OR кошка) -собака",
        "fields": ["name"]
      }
    }
  }
  ```
- **Когда использовать?**
    - Для сложных запросов, которые пользователи вводят вручную (например, в поисковой строке).
    - **Осторожно**: Может быть опасно, если входные данные не проверяются, из-за сложного синтаксиса.

#### Пример полнотекстового поиска:
- Документы:
  ```json
  {"name": "Кот Мурзик", " поля: ["кот", "мурзик"]}
  {"name": "Собака Рекс", поля: ["собака", "рекс"]}
  ```
- Запрос:
  ```bash
  POST /pets/_search
  {
    "query": {
      "match": { "name": "кот" }
    }
  }
  ```
- Результат: Найдёт только документ с "Кот Мурзик", так как текст анализируется `russian` анализатором.

### Фильтры: Использование для повышения производительности

Фильтры используются в **filter context** и не рассчитывают релевантность (score), что делает их быстрее, чем запросы в query context. Они идеальны для точных условий (например, "возраст = 3" или "тип = cat").

#### Как работают фильтры?
- Фильтры применяются внутри `bool` запроса в секции `filter`:
  ```bash
  POST /pets/_search
  {
    "query": {
      "bool": {
        "filter": [
          { "term": { "type": "cat" } },
          { "range": { "age": { "gte": 2 } } }
        ]
      }
    }
  }
  ```
- **Почему быстрее?**
    - Фильтры не вычисляют score, а возвращают только документы, соответствующие условиям.
    - Elasticsearch использует кэширование для фильтров, что ускоряет повторные запросы.

#### Лучшие практики:
- Используй `filter` для условий, где релевантность не нужна (например, точные значения или диапазоны).
- Комбинируй `filter` и `must` для точной фильтрации с релевантным поиском.
- Для больших данных фильтры значительно ускоряют запросы.

### Агрегации: Metrics, Bucket, Pipeline

**Агрегации** позволяют анализировать данные, подсчитывая метрики, группируя документы или выполняя вычисления над результатами. Они делятся на три типа:
- **Metrics**: Вычисляют значения (сумма, среднее, минимум).
- **Bucket**: Группируют документы по критериям (аналог GROUP BY в SQL).
- **Pipeline**: Выполняют вычисления над результатами других агрегаций.

#### a) Metrics Aggregations
- Вычисляют числовые показатели.
- Примеры:
    - `sum`, `avg`, `min`, `max`, `stats`.
- Пример: Подсчитать средний возраст животных:
  ```bash
  POST /pets/_search
  {
    "query": { "match_all": {} },
    "aggs": {
      "avg_age": {
        "avg": { "field": "age" }
      }
    }
  }
  ```
- Ответ:
  ```json
  {
    "aggregations": {
      "avg_age": { "value": 4.5 }
    }
  }
  ```

#### b) Bucket Aggregations
- Группируют документы по категориям.
- Примеры:
    - `terms`: Группировка по значениям поля.
    - `histogram`: Группировка по числовым диапазонам.
    - `date_histogram`: Группировка по датам.
- Пример: Подсчитать количество животных по типу:
  ```bash
  POST /pets/_search
  {
    "query": { "match_all": {} },
    "aggs": {
      "by_type": {
        "terms": { "field": "type" }
      }
    }
  }
  ```
- Ответ:
  ```json
  {
    "aggregations": {
      "by_type": {
        "buckets": [
          { "key": "cat", "doc_count": 10 },
          { "key": "dog", "doc_count": 5 }
        ]
      }
    }
  }
  ```

#### c) Pipeline Aggregations
- Выполняют вычисления над результатами других агрегаций.
- Примеры:
    - `avg_bucket`, `sum_bucket`, `bucket_script`.
- Пример: Средний возраст по типам животных:
  ```bash
  POST /pets/_search
  {
    "query": { "match_all": {} },
    "aggs": {
      "by_type": {
        "terms": { "field": "type" },
        "aggs": {
          "avg_age": { "avg": { "field": "age" } }
        }
      },
      "overall_avg_age": {
        "avg_bucket": {
          "buckets_path": "by_type>avg_age"
        }
      }
    }
  }
  ```
- **Объяснение**:
    - `by_type`: Группирует по `type` и вычисляет средний возраст (`avg_age`) для каждой группы.
    - `overall_avg_age`: Вычисляет среднее значение всех `avg_age`.

#### Лучшие практики для агрегаций:
- Используй фильтры в `query`, чтобы ограничить данные перед агрегацией.
- Ограничивай количество бакетов в `terms` (например, `"size": 10`), чтобы избежать перегрузки.
- Для сложных аналитик комбинируй `bucket` и `metrics` агрегации.
- Используй `pipeline` для вычислений над агрегациями (например, среднее по группам).

### Итог
- **Query DSL**:
  - `match`, `term`, `bool`, `range` — основные типы запросов.
  - `bool` комбинирует условия (`must`, `should`, `must_not`, `filter`).
- **Полнотекстовый поиск**:
  - `match`: Для анализа текста (например, "кот" найдёт "коты").
  - `multi_match`: Поиск по нескольким полям.
  - `query_string`: Гибкий поиск с синтаксисом `AND`, `OR`, `-`.
- **Фильтры**:
  - Используй в `filter` для точных условий без подсчёта score.
  - Ускоряют запросы за счёт кэширования.
- **Агрегации**:
  - **Metrics**: Подсчёт значений (`avg`, `sum`).
  - **Bucket**: Группировка данных (`terms`, `histogram`).
  - **Pipeline**: Вычисления над агрегациями (`avg_bucket`).
Эти инструменты позволяют гибко искать, фильтровать и анализировать данные в Elasticsearch, делая его мощным для аналитики и поиска!

## 11  **Релевантность поиска**:
Релевантность поиска в **Elasticsearch** — это механизм, который определяет, насколько хорошо документ соответствует поисковому запросу, и выражается через числовое значение **score**. Elasticsearch использует алгоритмы **TF-IDF** и **BM25** для вычисления релевантности, а также предоставляет инструменты для её настройки через **boosting** и **custom scoring**. 

---

### Как Elasticsearch вычисляет score (релевантность)?

Релевантность в Elasticsearch определяется в **query context** (например, при использовании `match` или `multi_match`), где каждому документу присваивается **score** — число, показывающее, насколько он соответствует запросу. Основные алгоритмы для этого — **TF-IDF** (старый подход) и **BM25** (по умолчанию в современных версиях).

#### a) TF-IDF (Term Frequency-Inverse Document Frequency)
TF-IDF — это классический алгоритм для оценки релевантности, который учитывает частоту термина в документе и его редкость в коллекции.

- **Компоненты**:
    - **TF (Term Frequency)**: Как часто термин встречается в документе. Чем чаще (например, "кот" в документе), тем выше релевантность.
    - **IDF (Inverse Document Frequency)**: Как редко термин встречается в индексе. Редкие термины (например, "Мурзик" вместо "кот") считаются более значимыми.
    - **Длина документа**: Более короткие документы получают больший вес, так как термин в них более значим.

- **Формула (упрощённо)**:
  ```
  score = TF * IDF * normalization
  ```
    - `TF`: Количество вхождений термина в документе.
    - `IDF`: `log(общее_количество_документов / количество_документов_с_термином)`.
    - `normalization`: Учёт длины документа и других факторов.

- **Пример**:
    - Запрос: `"кот"`.
    - Документ 1: `"Кот Мурзик ест рыбу"` (3 слова, "кот" встречается 1 раз).
    - Документ 2: `"Кот кот кот спит на диване"` (6 слов, "кот" встречается 3 раза).
    - Если "кот" есть в 10 из 100 документов, то:
        - `TF` для Документа 1: 1/3 ≈ 0.33.
        - `TF` для Документа 2: 3/6 = 0.5.
        - `IDF`: `log(100/10) = 1`.
        - `score` Документа 1: `0.33 * 1 * нормализация`.
        - `score` Документа 2: `0.5 * 1 * нормализация`.
    - Документ 2 получит более высокий score из-за большей частоты термина.

- **Проблемы TF-IDF**:
    - Слишком высокая чувствительность к частоте термина (много повторов → неоправданно высокий score).
    - Меньшая точность для больших коллекций.

#### b) BM25 (Best Matching 25)
BM25 — современный алгоритм, используемый в Elasticsearch по умолчанию (начиная с версии 5.0). Он улучшает TF-IDF, делая релевантность более сбалансированной.

- **Компоненты**:
    - **TF с насыщением**: Частота термина влияет на score, но эффект уменьшается с каждым дополнительным вхождением (например, 10 "кот" не в 10 раз лучше, чем 1 "кот").
    - **IDF**: Как в TF-IDF, редкие термины важнее.
    - **Длина документа**: Короткие документы получают больший вес, но влияние длины ограничено.
    - **Параметры настройки**:
        - `k1`: Контролирует насыщение TF (обычно 1.2–2.0).
        - `b`: Контролирует влияние длины документа (обычно 0.75).

- **Формула (упрощённо)**:
  ```
  score = Σ (IDF * (TF * (k1 + 1)) / (TF + k1 * (1 - b + b * (длина_документа / средняя_длина)))
  ```
    - Это делает BM25 более устойчивым к чрезмерному влиянию длинных документов или частых терминов.

- **Пример**:
    - Запрос: `"кот"`.
    - Документ 1: `"Кот Мурзик"` (2 слова).
    - Документ 2: `"Кот кот кот спит долго на диване"` (7 слов).
    - BM25 даст Документу 1 более высокий score, так как он короче, а "кот" имеет большее относительное значение, несмотря на меньшую частоту.

- **Преимущества BM25**:
    - Более точные результаты за счёт ограничения влияния TF.
    - Лучше работает с большими и разнообразными наборами данных.
    - Настраиваемые параметры (`k1`, `b`) позволяют адаптировать релевантность.

#### Как проверить score?
- Используй параметр `explain` в запросе:
  ```bash
  POST /pets/_search
  {
    "explain": true,
    "query": {
      "match": { "name": "кот" }
    }
  }
  ```
- Ответ покажет, как score рассчитан для каждого документа (включая вклад TF, IDF и длины).


### Настройка релевантности через boosting и custom scoring

Elasticsearch позволяет настраивать релевантность, чтобы приоритетно показывать определённые документы или изменять score на основе пользовательских правил. Основные инструменты — **boosting** и **custom scoring**.

#### a) Boosting
Boosting увеличивает или уменьшает вес определённых полей или условий в запросе, чтобы повлиять на релевантность.

- **Boosting на уровне полей**:
    - Укажи вес (`boost`) для полей в `multi_match`:
      ```bash
      POST /pets/_search
      {
        "query": {
          "multi_match": {
            "query": "кот",
            "fields": ["name^2", "description"], // name важнее в 2 раза
            "type": "best_fields"
          }
        }
      }
      ```
    - Документы, где "кот" найден в `name`, получат score в 2 раза выше, чем для `description`.

- **Boosting на уровне запросов**:
    - Используй `bool` с `boost` для отдельных условий:
      ```bash
      POST /pets/_search
      {
        "query": {
          "bool": {
            "should": [
              { "match": { "name": { "query": "кот", "boost": 2 } } },
              { "match": { "description": { "query": "кот", "boost": 1 } } }
            ]
          }
        }
      }
      ```
    - Совпадения в `name` имеют больший вес.

- **Negative boosting**:
    - Уменьшай score для нежелательных документов:
      ```bash
      POST /pets/_search
      {
        "query": {
          "bool": {
            "must": [
              { "match": { "name": "кот" } }
            ],
            "should": [
              { "match": { "type": "cat" } }
            ],
            "must_not": [
              { "match": { "description": { "query": "больной", "boost": 0.1 } } }
            ]
          }
        }
      }
      ```
    - Документы с "больной" в `description` получат меньший score.

#### b) Custom Scoring
Custom scoring позволяет задавать собственные формулы для вычисления score с помощью **function_score** запроса. Это полезно, когда нужно учитывать дополнительные факторы, такие как числовые поля, даты или веса.

- **Основные функции в function_score**:
    - `weight`: Фиксированный множитель score.
    - `field_value_factor`: Использует значение поля для изменения score.
    - `script_score`: Пользовательский скрипт для вычисления score.
    - `decay_function`: Уменьшает score в зависимости от расстояния от значения (например, даты или геолокации).

- **Пример: Увеличение score для молодых животных**:
    - Допустим, более молодые животные (меньший `age`) должны быть выше в результатах:
      ```bash
      POST /pets/_search
      {
        "query": {
          "function_score": {
            "query": { "match": { "name": "кот" } },
            "functions": [
              {
                "field_value_factor": {
                  "field": "age",
                  "factor": 1,
                  "modifier": "reciprocal" // score = 1/age, меньший возраст → выше score
                }
              }
            ],
            "boost_mode": "multiply" // Умножаем score на функцию
          }
        }
      }
      ```
    - Документ с `age: 2` получит score выше, чем с `age: 5`.

- **Пример: Скрипт для custom scoring**:
    - Допустим, score зависит от `age` и `weight`:
      ```bash
      POST /pets/_search
      {
        "query": {
          "function_score": {
            "query": { "match": { "name": "кот" } },
            "functions": [
              {
                "script_score": {
                  "script": {
                    "source": "doc['age'].value * 0.5 + doc['weight'].value * 0.3"
                  }
                }
              }
            ],
            "boost_mode": "replace" // Заменяем score на результат скрипта
          }
        }
      }
      ```
    - Score вычисляется как `age * 0.5 + weight * 0.3`.

- **Пример: Decay function для актуальности по дате**:
    - Допустим, более новые записи (поле `created_date`) должны иметь больший score:
      ```bash
      POST /pets/_search
      {
        "query": {
          "function_score": {
            "query": { "match": { "name": "кот" } },
            "functions": [
              {
                "gauss": {
                  "created_date": {
                    "origin": "now",
                    "scale": "30d",
                    "decay": 0.5
                  }
                }
              }
            ],
            "boost_mode": "multiply"
          }
        }
      }
      ```
    - Документы, созданные ближе к текущей дате (`now`), получают более высокий score.

#### Лучшие практики для настройки релевантности
- **Boosting**:
    - Используй умеренные значения `boost` (например, 1–5), чтобы избежать искажения результатов.
    - Тестируй разные веса с помощью `_explain` для понимания влияния.
- **Custom Scoring**:
    - Используй `function_score` для сложных сценариев (например, учёт геолокации или популярности).
    - Избегай сложных скриптов в `script_score`, так как они замедляют поиск.
- **BM25 параметры**:
    - Настрой `k1` и `b` в маппингах индекса, если нужно изменить поведение BM25:
      ```bash
      PUT /pets
      {
        "settings": {
          "index": {
            "similarity": {
              "my_bm25": {
                "type": "BM25",
                "k1": 1.2,
                "b": 0.75
              }
            }
          }
        },
        "mappings": {
          "properties": {
            "name": { "type": "text", "similarity": "my_bm25" }
          }
        }
      }
      ```
- **Тестирование**:
    - Используй `_explain` для анализа, почему документ получил определённый score:
      ```bash
      GET /pets/_explain/1
      {
        "query": { "match": { "name": "кот" } }
      }
      ```
    
### Итог
- **Релевантность**:
    - **TF-IDF**: Учитывает частоту термина и его редкость, но чувствителен к повторам.
    - **BM25**: Более сбалансированный, ограничивает влияние частоты и длины документа, используется по умолчанию.
- **Настройка релевантности**:
    - **Boosting**: Увеличивай вес полей или условий (`multi_match`, `bool`).
    - **Custom Scoring**: Используй `function_score` для сложных правил (например, `field_value_factor`, `script_score`, `decay_function`).
- **Практика**:
    - Тестируй с `_explain` для понимания score.
    - Используй умеренные значения boost и оптимизируй скрипты для производительности.
      Эти инструменты позволяют гибко настраивать поиск, чтобы результаты соответствовали бизнес-целям или пользовательским ожиданиям!
      
## 12 **Геопространственные запросы**
Геопространственные запросы в **Elasticsearch** позволяют работать с географическими данными, такими как координаты точек (`geo_point`) или сложные геометрические фигуры (`geo_shape`). Это полезно для задач, связанных с поиском объектов по местоположению, фильтрацией по области или вычислением расстояний. 

### Основы геоданных в Elasticsearch

Elasticsearch поддерживает два основных типа геоданных:
- **`geo_point`**: Для хранения координат (широта и долгота) в формате `{ "lat": 55.75, "lon": 37.61 }`. Используется для точек на карте (например, местоположение магазина).
- **`geo_shape`**: Для хранения сложных геометрических фигур (полигоны, линии, круги). Используется для работы с областями (например, границы города).

Эти типы данных позволяют выполнять геопространственные запросы, такие как поиск точек в радиусе, проверка пересечения с фигурами или сортировка по расстоянию.


### 2. Настройка маппингов для геоданных

Чтобы использовать геопространственные запросы, нужно настроить маппинг индекса для полей с типами `geo_point` или `geo_shape`.

#### a) Маппинг для `geo_point`
- Пример создания индекса с полем `location` типа `geo_point`:
  ```bash
  PUT /places
  {
    "mappings": {
      "properties": {
        "name": { "type": "text" },
        "location": { "type": "geo_point" }
      }
    }
  }
  ```
- Форматы для `geo_point`:
    - Объект: `{ "lat": 55.75, "lon": 37.61 }` (Москва).
    - Строка: `"55.75,37.61"` (широта, долгота).
    - GeoJSON: `[37.61, 55.75]` (долгота, широта).
    - WKT: `"POINT (37.61 55.75)"`.

#### b) Маппинг для `geo_shape`
- Пример создания индекса с полем `area` типа `geo_shape`:
  ```bash
  PUT /cities
  {
    "mappings": {
      "properties": {
        "name": { "type": "text" },
        "area": { "type": "geo_shape" }
      }
    }
  }
  ```
- Форматы для `geo_shape`:
    - GeoJSON: Например, полигон для описания области.
    - Well-Known Text (WKT): Например, `"POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))"`.

#### Пример добавления данных
- Для `geo_point`:
  ```bash
  POST /places/_doc/1
  {
    "name": "Кремль",
    "location": { "lat": 55.751244, "lon": 37.618423 }
  }
  ```
- Для `geo_shape`:
  ```bash
  POST /cities/_doc/1
  {
    "name": "Москва",
    "area": {
      "type": "polygon",
      "coordinates": [[
        [37.3, 55.9], [37.9, 55.9], [37.9, 55.5], [37.3, 55.5], [37.3, 55.9]
      ]]
    }
  }
  ```

### 3. Геопространственные запросы

Elasticsearch поддерживает несколько типов геопространственных запросов для `geo_point` и `geo_shape`. Они могут использоваться в **query context** (с подсчётом score) или **filter context** (без score, быстрее).

#### a) Запросы для `geo_point`
1. **geo_distance**:
    - Находит точки в заданном радиусе от центра.
    - Пример: Найти места в радиусе 10 км от центра Москвы:
      ```bash
      POST /places/_search
      {
        "query": {
          "bool": {
            "filter": {
              "geo_distance": {
                "distance": "10km",
                "location": {
                  "lat": 55.75,
                  "lon": 37.61
                }
              }
            }
          }
        }
      }
      ```
    - Возвращает документы, где `location` находится в пределах 10 км от точки.

2. **geo_bounding_box**:
    - Находит точки в прямоугольной области, заданной верхним левым и нижним правым углами.
    - Пример: Найти места в прямоугольнике вокруг Москвы:
      ```bash
      POST /places/_search
      {
        "query": {
          "bool": {
            "filter": {
              "geo_bounding_box": {
                "location": {
                  "top_left": { "lat": 55.9, "lon": 37.3 },
                  "bottom_right": { "lat": 55.5, "lon": 37.9 }
                }
              }
            }
          }
        }
      }
      ```

3. **geo_polygon**:
    - Находит точки внутри полигона, заданного координатами.
    - Пример:
      ```bash
      POST /places/_search
      {
        "query": {
          "bool": {
            "filter": {
              "geo_polygon": {
                "location": {
                  "points": [
                    { "lat": 55.9, "lon": 37.3 },
                    { "lat": 55.9, "lon": 37.9 },
                    { "lat": 55.5, "lon": 37.9 },
                    { "lat": 55.5, "lon": 37.3 }
                  ]
                }
              }
            }
          }
        }
      }
      ```

4. **Сортировка по расстоянию**:
    - Используй `sort` для упорядочивания по расстоянию от точки:
      ```bash
      POST /places/_search
      {
        "query": { "match_all": {} },
        "sort": [
          {
            "_geo_distance": {
              "location": { "lat": 55.75, "lon": 37.61 },
              "order": "asc",
              "unit": "km"
            }
          }
        ]
      }
      ```
    - Документы сортируются по возрастанию расстояния от центра Москвы.

#### b) Запросы для `geo_shape`
1. **geo_shape**:
    - Проверяет пересечение, включение или другие отношения между фигурами.
    - Пример: Найти города, чьи области пересекаются с заданным полигоном:
      ```bash
      POST /cities/_search
      {
        "query": {
          "bool": {
            "filter": {
              "geo_shape": {
                "area": {
                  "shape": {
                    "type": "polygon",
                    "coordinates": [[
                      [37.3, 55.9], [37.9, 55.9], [37.9, 55.5], [37.3, 55.5], [37.3, 55.9]
                    ]]
                  },
                  "relation": "intersects" // Пересекается
                }
              }
            }
          }
        }
      }
      ```
    - **relation**:
        - `intersects`: Пересекается с фигурой.
        - `within`: Полностью внутри фигуры.
        - `contains`: Содержит фигуру.
        - `disjoint`: Не пересекается.

2. **geo_bounding_box** (для `geo_shape`):
    - Проверяет, попадает ли фигу.~ра в прямоугольник:
      ```bash
      POST /cities/_search
      {
        "query": {
          "bool": {
            "filter": {
              "geo_shape": {
                "area": {
                  "shape": {
                    "type": "envelope",
                    "coordinates": [[37.3, 55.9], [37.9, 55.5]]
                  },
                  "relation": "within"
                }
              }
            }
          }
        }
      }
      ```
    
### 4. Агрегации с геоданными

Elasticsearch поддерживает геопространственные агрегации для анализа данных.

1. **geo_distance** (агрегация):
    - Группирует документы по расстоянию от точки.
    - Пример: Подсчитать, сколько мест в радиусах 5, 10 и 20 км от центра Москвы:
      ```bash
      POST /places/_search
      {
        "query": { "match_all": {} },
        "aggs": {
          "by_distance": {
            "geo_distance": {
              "field": "location",
              "origin": { "lat": 55.75, "lon": 37.61 },
              "unit": "km",
              "ranges": [
                { "to": 5 },
                { "from": 5, "to": 10 },
                { "from": 10, "to": 20 }
              ]
            }
          }
        }
      }
      ```
    - Ответ:
      ```json
      {
        "aggregations": {
          "by_distance": {
            "buckets": [
              { "key": "*-5.0", "doc_count": 2 },
              { "key": "5.0-10.0", "doc_count": 5 },
              { "key": "10.0-20.0", "doc_count": 10 }
            ]
          }
        }
      }
      ```

2. **geo_centroid**:
    - Вычисляет центр масс (среднюю точку) для набора `geo_point`:
      ```bash
      POST /places/_search
      {
        "aggs": {
          "centroid": {
            "geo_centroid": { "field": "location" }
          }
        }
      }
      ```
    - Ответ:
      ```json
      {
        "aggregations": {
          "centroid": {
            "location": { "lat": 55.73, "lon": 37.62 }
          }
        }
      }
      ```

3. **geo_bounds**:
    - Вычисляет прямоугольник, охватывающий все точки:
      ```bash
      POST /places/_search
      {
        "aggs": {
          "bounds": {
            "geo_bounds": { "field": "location" }
          }
        }
      }
      ```
    - Ответ:
      ```json
      {
        "aggregations": {
          "bounds": {
            "top_left": { "lat": 55.9, "lon": 37.3 },
            "bottom_right": { "lat": 55.5, "lon": 37.9 }
          }
        }
      }
      ```

### Итог
- **Типы данных**:
    - `geo_point`: Для координат (широта, долгота).
    - `geo_shape`: Для сложных фигур (полигоны, линии).
- **Запросы**:
    - `geo_distance`: Поиск в радиусе.
    - `geo_bounding_box`: Поиск в прямоугольнике.
    - `geo_polygon`/`geo_shape`: Поиск внутри фигур.
    - Сортировка по расстоянию с `_geo_distance`.
- **Агрегации**:
    - `geo_distance`: Группировка по расстоянию.
    - `geo_centroid`: Центр масс.
    - `geo_bounds`: Охватывающий прямоугольник.
- **Практика**:
    - Используй `filter` для скорости.
    - Настраивай маппинги для точности.
    - Тестируй с Kibana Maps для визуализации.
      Эти инструменты делают Elasticsearch мощным для работы с геоданными, от поиска ближайших точек до анализа географических зон!
      

## 13 **Оптимизация запросов**:

Оптимизация запросов в **Elasticsearch** критически важна для обеспечения высокой производительности, особенно при работе с большими индексами или высоконагруженными системами. Основные подходы включают использование **фильтров вместо запросов** там, где это возможно, и избегание сложных запросов, таких как `wildcard` на больших индексах. 

###  Использование фильтров вместо запросов

В Elasticsearch запросы делятся на два контекста:
- **Query context**: Используется для полнотекстового поиска с вычислением релевантности (score). Примеры: `match`, `multi_match`.
- **Filter context**: Используется для точной фильтрации без вычисления score, что делает его быстрее. Примеры: `term`, `range` в секции `filter`.

#### Почему фильтры быстрее?
- **Без score**: Фильтры не рассчитывают релевантность, а просто возвращают документы, соответствующие условиям.
- **Кэширование**: Elasticsearch автоматически кэширует результаты фильтров (например, `term`, `range`), если они часто используются. Кэш хранится в памяти и ускоряет повторные запросы.
- **Оптимизация**: Фильтры работают с бинарной логикой (да/нет), что снижает вычислительные затраты.

#### Когда использовать фильтры?
- Когда не нужна релевантность (например, точное совпадение или диапазон).
- Для предварительного сужения выборки перед полнотекстовым поиском.
- Для часто повторяющихся условий (например, фильтрация по категории или дате).

#### Пример: Фильтры против запросов
Допустим, у нас индекс `pets` с документами вида:
```json
{"name": "Кот Мурзик", "age": 3, "type": "cat"}
{"name": "Собака Рекс", "age": 5, "type": "dog"}
```

1. **Запрос с вычислением score** (медленнее):
   ```bash
   POST /pets/_search
   {
     "query": {
       "bool": {
         "must": [
           { "match": { "type": "cat" } },
           { "range": { "age": { "gte": 2 } } }
         ]
       }
     }
   }
   ```
    - Проблема: `match` и `range` в `must` вычисляют score, что замедляет запрос.

2. **Фильтр без score** (быстрее):
   ```bash
   POST /pets/_search
   {
     "query": {
       "bool": {
         "filter": [
           { "term": { "type": "cat" } },
           { "range": { "age": { "gte": 2 } } }
         ]
       }
     }
   }
   ```
    - Преимущества:
        - `term` вместо `match` для точного совпадения (без анализа текста).
        - `filter` не вычисляет score и кэширует результаты.
        - Запрос выполняется быстрее, особенно на больших индексах.

3. **Комбинированный подход**:
    - Используй фильтры для сужения выборки, а `match` для полнотекстового поиска:
      ```bash
      POST /pets/_search
      {
        "query": {
          "bool": {
            "filter": [
              { "term": { "type": "cat" } }, // Сначала фильтруем котов
              { "range": { "age": { "gte": 2 } } }
            ],
            "must": [
              { "match": { "name": "мурзик" } } // Затем ищем по имени
            ]
          }
        }
      }
      ```
    - **Эффект**: Фильтры уменьшают количество документов, которые нужно анализировать в `match`, что ускоряет запрос.

#### Лучшие практики для фильтров
- Используй `term` для точных значений (например, `type: "cat"`) вместо `match`.
- Применяй `range` в `filter` для чисел, дат или других диапазонов.
- Для текстовых полей используй `term` с полем `.keyword` (например, `name.keyword`), если не нужен анализ текста.
- Комбинируй `filter` с `must` для баланса скорости и релевантности.
- Проверяй кэширование фильтров через API:
  ```bash
  GET /_stats/request_cache?human
  ```

### 2. Избежание сложных запросов (например, wildcard на больших индексах)

Сложные запросы, такие как `wildcard`, `regexp`, или `prefix`, могут быть ресурсоёмкими, особенно на больших индексах, так как они требуют проверки большого числа документов или токенов в инвертированном индексе. Давайте разберём, как их избегать и какие альтернативы использовать.

#### Проблемы сложных запросов
1. **wildcard**:
    - Пример: `name: ко*` ищет все документы, где `name` начинается на "ко".
    - Проблема: Elasticsearch проверяет все токены в инвертированном индексе, что медленно на больших индексах.
    - Пример проблемного запроса:
      ```bash
      POST /pets/_search
      {
        "query": {
          "wildcard": { "name.keyword": "ко*" }
        }
      }
      ```

2. **regexp**:
    - Пример: `name: k.t` (где `.` — любой символ).
    - Проблема: Требует проверки сложных шаблонов, что замедляет поиск.

3. **prefix**:
    - Похож на `wildcard`, но проверяет только начало строки.
    - Проблема: Тоже может быть медленным, особенно если префикс короткий (например, `"к*"`).

4. **Другие ресурсоёмкие запросы**:
    - Глубокие агрегации (например, `terms` на поле с миллионами уникальных значений).
    - Сложные скрипты в `script_score` или `script_fields`.
    - Запросы с большим `size` (например, возврат 10000 документов).

#### Альтернативы для оптимизации
1. **Для автодополнения (вместо wildcard/prefix)**:
    - Используй анализатор с `edge_ngram` для поддержки автодополнения:
      ```bash
      PUT /pets
      {
        "settings": {
          "analysis": {
            "analyzer": {
              "autocomplete": {
                "type": "custom",
                "tokenizer": "standard",
                "filter": ["lowercase", "autocomplete_filter"]
              }
            },
            "filter": {
              "autocomplete_filter": {
                "type": "edge_ngram",
                "min_gram": 2,
                "max_gram": 20
              }
            }
          }
        },
        "mappings": {
          "properties": {
            "name": {
              "type": "text",
              "analyzer": "autocomplete",
              "search_analyzer": "standard"
            }
          }
        }
      }
      ```
    - Пример запроса:
      ```bash
      POST /pets/_search
      {
        "query": {
          "match": { "name": "ко" }
        }
      }
      ```
    - **Эффект**: `edge_ngram` разбивает "кот" на `["ко", "кот"]` при индексации, что делает поиск по префиксу быстрым.

2. **Для точного совпадения (вместо wildcard/regexp)**:
    - Используй поле `keyword` с `term`:
      ```bash
      POST /pets/_search
      {
        "query": {
          "term": { "name.keyword": "Кот Мурзик" }
        }
      }
      ```
    - Это быстрее, так как не требует анализа текста.

3. **Для сложных шаблонов**:
    - Если нужен частичный поиск, используй `match` с анализатором, который поддерживает морфологию (например, `russian`):
      ```bash
      POST /pets/_search
      {
        "query": {
          "match": { "name": "коты" }
        }
      }
      ```
    - Это найдёт "кот", "коты", "кошка" без использования `wildcard`.

4. **Для агрегаций**:
    - Ограничивай количество бакетов в `terms`:
      ```bash
      POST /pets/_search
      {
        "aggs": {
          "by_type": {
            "terms": {
              "field": "type",
              "size": 10 // Ограничить 10 бакетами
            }
          }
        }
      }
      ```
    - Используй предварительные фильтры, чтобы уменьшить выборку:
      ```bash
      POST /pets/_search
      {
        "query": {
          "bool": {
            "filter": [
              { "range": { "age": { "gte": 2 } } }
            ]
          }
        },
        "aggs": {
          "by_type": {
            "terms": { "field": "type" }
          }
        }
      }
      ```

5. **Для больших результатов**:
    - Используй `search_after` или `scroll` вместо большого `size`:
      ```bash
      POST /pets/_search
      {
        "size": 100,
        "sort": [
          { "age": "asc" },
          { "_id": "asc" }
        ],
        "query": { "match_all": {} },
        "search_after": [5, "last_id"]
      }
      ```
    - `scroll` подходит для экспорта данных:
      ```bash
      POST /pets/_search?scroll=1m
      {
        "size": 1000,
        "query": { "match_all": {} }
      }
      ```

#### Лучшие практики для избегания сложных запросов
- **Избегай wildcard/regexp** на больших индексах. Вместо этого используй анализаторы с `edge_ngram` или `n-gram` для частичного поиска.
- **Ограничивай выборку** фильтрами перед сложными операциями (поиск, агрегации).
- **Используй keyword для точных совпадений** вместо текстовых полей с анализом.
- **Тестируй запросы** на малых данных и проверяй производительность с помощью `_profile`:
  ```bash
  POST /pets/_search
  {
    "profile": true,
    "query": { "match": { "name": "кот" } }
  }
  ```
- **Оптимизируй индекс**:
    - Уменьшай количество первичных шардов для небольших индексов.
    - Используй `force_merge` для оптимизации сегментов (осторожно, ресурсоёмко):
      ```bash
      POST /pets/_forcemerge
      ```

### Итог
- **Фильтры вместо запросов**:
    - Используй `filter` для точных условий (`term`, `range`), чтобы избежать вычисления score.
    - Комбинируй `filter` и `must` для скорости и релевантности.
    - Фильтры кэшируются, что ускоряет повторные запросы.
- **Избежание сложных запросов**:
    - Заменяй `wildcard`/`regexp` анализаторами (`edge_ngram` для автодополнения).
    - Ограничивай выборку фильтрами перед агрегациями.
    - Используй `search_after` или `scroll` для больших результатов.
    - Тестируй с `_profile` и оптимизируй индексы (`force_merge`).
      Эти подходы помогут сделать запросы быстрыми и эффективными даже на больших данных!

## 14 **Управление шардами**:
Управление шардами в **Elasticsearch** — это важный аспект для обеспечения производительности, масштабируемости и отказоустойчивости кластера. Шарды и их реплики определяют, как данные распределяются и хранятся, а такие операции, как `_forcemerge`, помогают оптимизировать индексы. 

### Оптимизация количества шардов и реплик

Шарды — это части индекса, которые распределяют данные по узлам кластера. Реплики — это копии шардов для отказоустойчивости и распределения нагрузки. Неправильная настройка их количества может привести к перегрузке узлов, низкой производительности или избыточному потреблению ресурсов.

#### a) Шарды
- **Первичные шарды** (`number_of_shards`): Делят индекс на части для параллельной обработки и распределения данных.
- **Особенности**:
    - Количество первичных шардов задаётся при создании индекса и **не может быть изменено** без переиндексации.
    - Каждый шард — это отдельный Lucene-индекс, который потребляет ресурсы (память, диск, CPU).
    - Слишком много шардов увеличивает накладные расходы, слишком мало — ограничивает масштабируемость.

- **Рекомендации по количеству шардов**:
    - **Размер данных**: Целевой размер шарда — 20–50 ГБ. Например, для 100 ГБ данных используй 2–5 шардов.
    - **Количество узлов**: Учитывай число Data Nodes. Например, для 3 узлов 3–6 шардов обеспечивают равномерное распределение.
    - **Тип нагрузки**:
        - Для поисковых запросов: 1–5 шардов на индекс для небольших данных.
        - Для аналитики (агрегации): Больше шардов для параллелизма, но не более 1–2 на узел.
    - **Пример создания индекса**:
      ```bash
      PUT /pets
      {
        "settings": {
          "number_of_shards": 3, // 3 первичных шарда
          "number_of_replicas": 1 // 1 реплика на шард
        }
      }
      ```

- **Проверка распределения шардов**:
  ```bash
  GET /_cat/shards/pets?v
  ```
    - Показывает, где находятся шарды и реплики, их размер и состояние.

#### b) Реплики
- **Реплики** (`number_of_replicas`): Копии первичных шардов для отказоустойчивости и распределения нагрузки при чтении.
- **Особенности**:
    - Реплики можно изменять **на лету** без переиндексации.
    - Увеличивают отказоустойчивость: если узел с первичным шардом выходит из строя, реплика становится первичной.
    - Ускоряют поиск, так как запросы могут распределяться между первичными шардами и репликами.
    - Увеличивают потребление диска и ресурсов (каждая реплика — копия шарда).

- **Рекомендации по количеству реплик**:
    - **Минимум 1 реплика** для отказоустойчивости (в продакшене).
    - Для высоконагруженных систем (много поисков): 2–3 реплики для распределения нагрузки.
    - Для аналитики с редкими запросами: 0–1 реплика, чтобы сэкономить ресурсы.
    - Если кластер небольшой (1–2 узла), 1 реплика достаточно.
    - **Пример изменения реплик**:
      ```bash
      PUT /pets/_settings
      {
        "number_of_replicas": 2 // Установить 2 реплики
      }
      ```

- **Проверка состояния реплик**:
  ```bash
  GET /_cat/health?v
  GET /_cat/recovery?v
  ```
    - Убедись, что все реплики синхронизированы (`status: green`).

#### c) Оптимизация шардов и реплик
1. **Слишком много шардов**:
    - Проблема: Увеличиваются накладные расходы на управление (метаданные, память).
    - Решение:
        - Уменьшай количество шардов при создании новых индексов.
        - Для существующих индексов используй `_shrink` API, чтобы уменьшить число шардов:
          ```bash
          POST /pets/_shrink/pets_shrunk
          {
            "settings": {
              "number_of_shards": 1, // Сжать до 1 шарда
              "number_of_replicas": 1
            }
          }
          ```
        - Убедись, что индекс закрыт или только для чтения перед сжатием:
          ```bash
          POST /pets/_close
          ```

2. **Слишком мало шардов**:
    - Проблема: Ограничивает масштабируемость и параллелизм.
    - Решение:
        - Создай новый индекс с большим количеством шардов и переиндексируй данные:
          ```bash
          PUT /pets_v2
          {
            "settings": {
              "number_of_shards": 6,
              "number_of_replicas": 1
            }
          }
          POST /_reindex
          {
            "source": { "index": "pets" },
            "dest": { "index": "pets_v2" }
          }
          ```

3. **Дисбаланс шардов**:
    - Проблема: Некоторые узлы перегружены, если шарды распределены неравномерно.
    - Решение:
        - Включи автоматическую балансировку:
          ```bash
          PUT /_cluster/settings
          {
            "transient": {
              "cluster.routing.allocation.enable": "all"
            }
          }
          ```
        - Проверь распределение:
          ```bash
          GET /_cat/allocation?v
          ```
        - Принудительно переместить шард:
          ```bash
          POST /_cluster/reroute
          {
            "commands": [
              {
                "move": {
                  "index": "pets",
                  "shard": 0,
                  "from_node": "node1",
                  "to_node": "node2"
                }
              }
            ]
          }
          ```

4. **Реплики для нагрузки**:
    - Увеличь реплики для высоконагруженных поисков:
      ```bash
      PUT /pets/_settings
      {
        "number_of_replicas": 3
      }
      ```
    - Мониторь нагрузку с помощью:
      ```bash
      GET /_cat/nodes?v&h=name,load_1m,load_5m,load_15m
      ```
    
### Использование `_forcemerge` для оптимизации индексов

**`_forcemerge`** — это API, который принудительно объединяет сегменты Lucene в индексе, чтобы уменьшить их количество, освободить место и улучшить производительность.

#### Как работает `_forcemerge`?
- Каждый шард — это Lucene-индекс, состоящий из **сегментов** (файлов на диске, где хранятся данные).
- Сегменты создаются при добавлении/обновлении документов. Удалённые документы помечаются, но физически остаются в сегментах.
- `_forcemerge` объединяет сегменты, удаляет помеченные документы и оптимизирует инвертированный индекс.
- Результат:
    - Меньше сегментов → меньше файлов → быстрее поиск.
    - Освобождается дисковое пространство.
    - Уменьшается потребление памяти.

#### Пример использования
- Выполнить принудительное слияние для индекса `pets`, оставляя 1 сегмент на шард:
  ```bash
  POST /pets/_forcemerge?max_num_segments=1
  ```
- **Параметры**:
    - `max_num_segments`: Целевое количество сегментов на шард (обычно 1 для максимальной оптимизации).
    - `only_expunge_deletes`: Удалить только помеченные документы, не объединяя сегменты:
      ```bash
      POST /pets/_forcemerge?only_expunge_deletes=true
      ```

#### Когда использовать `_forcemerge`?
- **После массового удаления**: Если много документов помечено как удалённые, `_forcemerge` освободит место.
- **Для индексов только для чтения**: Например, исторические логи, которые больше не обновляются.
- **Перед долгосрочным хранением**: Чтобы уменьшить размер индекса и улучшить производительность.
- **При низкой производительности**: Если слишком много сегментов замедляет поиск.

#### Ограничения и риски
- **Ресурсоёмкость**: `_forcemerge` потребляет много CPU, I/O и памяти. Выполняй в периоды низкой нагрузки.
- **Не для активных индексов**: Слияние сегментов может замедлить текущие операции записи.
- **Необратимость**: Сегменты объединяются навсегда, что может быть проблемой, если нужно отменить изменения.
- **Ограничение по размеру сегмента**: Lucene не создаёт сегменты больше ~5 ГБ, даже если `max_num_segments=1`.

#### Лучшие практики
1. **Закрой индекс перед слиянием** (если возможно):
   ```bash
   POST /pets/_close
   POST /pets/_forcemerge?max_num_segments=1
   POST /pets/_open
   ```
2. **Мониторь процесс**:
    - Проверяй состояние слияния:
      ```bash
      GET /_cat/segments/pets?v
      ```
    - Проверяй нагрузку на узлы:
      ```bash
      GET /_cat/nodes?v&h=name,disk.used,disk.avail
      ```
3. **Ограничивай слияние**:
    - Для больших индексов выполняй `_forcemerge` по одному индексу за раз.
    - Укажи `max_num_segments=5` вместо `1`, чтобы снизить нагрузку.
4. **Автоматическое управление сегментами**:
    - Elasticsearch сам выполняет слияние сегментов в фоновом режиме. Настрой параметры в `elasticsearch.yml`:
      ```yaml
      indices.forcemerge.max_num_segments: 5
      ```
    - Увеличь лимиты, если нужно:
      ```bash
      PUT /_cluster/settings
      {
        "transient": {
          "indices.forcemerge.max_bytes_per_sec": "50mb"
        }
      }
      ```

#### Пример оптимизации
- Индекс `pets` после массового удаления занимает 10 ГБ, но имеет 100 сегментов на шард.
- Оптимизируем:
  ```bash
  POST /pets/_close
  POST /pets/_forcemerge?max_num_segments=1
  POST /pets/_open
  ```
- Результат: Размер индекса уменьшился до 7 ГБ, сегментов стало 1 на шард, поиск ускорился.

### Итог
- **Оптимизация шардов и реплик**:
    - Задавай **2–5 шардов** для индексов до 100 ГБ, ориентируйся на 20–50 ГБ на шард.
    - Используй **1–2 реплики** для отказоустойчивости и распределения нагрузки.
    - Изменяй реплики на лету, для шардов используй `_shrink` или `_reindex`.
    - Балансируй шарды с помощью `_cluster/reroute` и проверяй распределение.
- **Использование `_forcemerge`**:
    - Выполняй для индексов только для чтения или после массового удаления.
    - Указывай `max_num_segments=1` для максимальной оптимизации.
    - Закрывай индекс перед слиянием и выполняй в периоды низкой нагрузки.
      Эти подходы помогут оптимизировать производительность и снизить потребление ресурсов в Elasticsearch!
      
## 15 **Кэширование**
Кэширование в **Elasticsearch** — это мощный механизм, который значительно ускоряет выполнение запросов и агрегаций за счёт хранения часто используемых данных в памяти. Elasticsearch использует несколько типов кэшей: **кэш запросов**, **кэш полей** и **кэш шардов**. Понимание их работы и правильное управление помогают оптимизировать производительность.

### Кэш запросов (Query Cache)

**Кэш запросов** хранит результаты фильтров (filter context) для повторного использования, что ускоряет запросы, не требующие вычисления релевантности (score).

#### Как работает?
- Кэш запросов применяется к запросам в **filter context** (например, `term`, `range`, `bool.filter`).
- Хранит **битовые множества** (bitsets) — компактное представление документов, соответствующих фильтру.
- Работает на уровне **шарда**: каждый шард кэширует свои результаты.
- Кэш автоматически используется для часто выполняемых фильтров (например, `type: "cat"`).
- **Когда кэш обновляется?**
    - Кэш очищается при обновлении сегментов Lucene (например, после `refresh` или `_forcemerge`).
    - Если данные в индексе изменились (добавлены/удалены документы), кэш становится недействительным для затронутых шардов.

#### Пример
- Запрос с фильтром:
  ```bash
  POST /pets/_search
  {
    "query": {
      "bool": {
        "filter": [
          { "term": { "type": "cat" } },
          { "range": { "age": { "gte": 2 } } }
        ]
      }
    }
  }
  ```
- **Что происходит?**
    - Elasticsearch создаёт битовое множество для документов, где `type: "cat"` и `age >= 2`.
    - Это множество кэшируется в памяти для каждого шарда.
    - При повторном запросе с таким же фильтром Elasticsearch использует кэш вместо сканирования индекса.

#### Управление кэшем запросов
- **Проверка использования кэша**:
  ```bash
  GET /_stats/request_cache?human
  ```
    - Показывает статистику: `hit_count` (попадания в кэш), `miss_count` (промахи), `memory_size` (объём кэша).

- **Настройка размера кэша**:
    - По умолчанию кэш запросов занимает до 1% от кучи (heap) каждого узла.
    - Изменить размер:
      ```bash
      PUT /_cluster/settings
      {
        "transient": {
          "indices.queries.cache.size": "2%" // Увеличить до 2%
        }
      }
      ```

- **Отключение кэша для запроса**:
    - Если кэш не нужен (например, для разовых запросов):
      ```bash
      POST /pets/_search?request_cache=false
      {
        "query": {
          "bool": {
            "filter": [
              { "term": { "type": "cat" } }
            ]
          }
        }
      }
      ```

#### Лучшие практики
- Используй **filter context** для условий, которые часто повторяются (например, фильтрация по категории или дате).
- Избегай кэширования для редко используемых фильтров, чтобы не занимать память.
- Мониторь использование кэша через `_stats/request_cache` и увеличивай размер, если много промахов (`miss_count`).
- Убедись, что `refresh_interval` не слишком мал (например, `1s`), чтобы кэш не очищался слишком часто.

### Кэш полей (Field Data Cache)

**Кэш полей** используется для хранения данных полей в памяти, чтобы ускорить операции сортировки, агрегаций и доступа к `script_fields`. Он применяется к полям типа `text` (для агрегаций) и другим типам данных, требующим быстрого доступа.

#### Как работает?
- Поля типа `text` анализируются (разбиваются на токены) и не подходят для агрегаций или сортировки без преобразования.
- Кэш полей создаёт **in-memory** структуру (fielddata) для таких полей, преобразуя их в формат, пригодный для операций.
- **Проблема**: Кэш полей может потреблять много памяти, особенно для полей с большим количеством уникальных значений.

#### Пример
- Агрегация по полю `name` (тип `text`):
  ```bash
  POST /pets/_search
  {
    "aggs": {
      "by_name": {
        "terms": { "field": "name" }
      }
    }
  }
  ```
- **Что происходит?**
    - Elasticsearch загружает значения поля `name` в fielddata cache.
    - Это позволяет быстро выполнить агрегацию, но требует памяти.

#### Оптимизация с `keyword`
- Поля `text` неэффективны для агрегаций/сортировки. Вместо этого используй поле `.keyword`:
  ```bash
  POST /pets/_search
  {
    "aggs": {
      "by_name": {
        "terms": { "field": "name.keyword" }
      }
    }
  }
  ```
- **Почему лучше?**
    - Поля `keyword` не требуют fielddata, так как хранятся в компактном виде.
    - Это снижает потребление памяти и ускоряет запросы.

#### Управление кэшем полей
- **Включение fielddata** (если нужно для `text`):
    - По умолчанию `fielddata` отключён для полей `text` из-за высокого потребления памяти.
    - Включить:
      ```bash
      PUT /pets/_mapping
      {
        "properties": {
          "name": {
            "type": "text",
            "fielddata": true
          }
        }
      }
      ```
    - **Осторожно**: Включай только при необходимости и для полей с низкой кардинальностью (немного уникальных значений).

- **Проверка использования кэша**:
  ```bash
  GET /_stats/fielddata?human&fields=name
  ```
    - Показывает объём памяти, занятый fielddata.

- **Ограничение размера кэша**:
    - По умолчанию fielddata ограничен 20% от кучи узла.
    - Изменить:
      ```bash
      PUT /_cluster/settings
      {
        "transient": {
          "indices.fielddata.cache.size": "15%"
        }
      }
      ```

#### Лучшие практики
- **Используй `keyword` вместо `text`** для агрегаций, сортировки и скриптов.
- Включай `fielddata` только для полей с низкой кардинальностью (например, категории, а не полные тексты).
- Мониторь использование памяти через `_stats/fielddata` и увеличивай кэш, если нужно.
- Добавляй поле `.keyword` в маппинг при создании индекса:
  ```bash
  PUT /pets
  {
    "mappings": {
      "properties": {
        "name": {
          "type": "text",
          "fields": {
            "keyword": { "type": "keyword" }
          }
        }
      }
    }
  }
  ```

### Кэш шардов (Shard Request Cache)

**Кэш шардов** (иногда называется request cache) хранит результаты целых поисковых запросов (включая агрегации) на уровне шарда. Он используется для запросов, которые возвращают одинаковые результаты при повторном выполнении.

#### Как работает?
- Кэширует **полный результат запроса** (хиты и агрегации) для каждого шарда.
- Работает только для **GET** запросов или `POST` с неизменяемыми данными (например, без `script_score` с динамическими параметрами).
- Кэш становится недействительным при обновлении сегментов (например, после `refresh`).
- По умолчанию включён для всех индексов.

#### Пример
- Запрос с агрегацией:
  ```bash
  POST /pets/_search
  {
    "query": {
      "bool": {
        "filter": [
          { "term": { "type": "cat" } }
        ]
      }
    },
    "aggs": {
      "by_age": {
        "terms": { "field": "age" }
      }
    }
  }
  ```
- **Что происходит?**
    - Результаты запроса и агрегации кэшируются для каждого шарда.
    - Повторный запрос с теми же параметрами использует кэш, что ускоряет выполнение.

#### Управление кэшем шардов
- **Проверка использования**:
  ```bash
  GET /_stats/request_cache?human
  ```
    - Показывает статистику кэша шардов (попадания, промахи, объём).

- **Отключение кэша для индекса**:
  ```bash
  PUT /pets/_settings
  {
    "index.requests.cache.enable": false
  }
  ```

- **Отключение кэша для запроса**:
  ```bash
  POST /pets/_search?request_cache=false
  {
    "query": { "match_all": {} }
  }
  ```

#### Лучшие практики
- Включай кэш шардов для запросов, которые часто повторяются и работают с неизменяемыми данными (например, аналитика по историческим данным).
- Отключай кэш для индексов с частыми обновлениями (`refresh_interval: 1s`), чтобы избежать устаревания данных.
- Мониторь использование через `_stats/request_cache` и увеличивай размер кэша при необходимости.

### Общие рекомендации по кэшированию

1. **Мониторинг кэшей**:
    - Регулярно проверяй статистику кэшей:
      ```bash
      GET /_stats/request_cache,fielddata?human
      ```
    - Следи за `evictions` (вытеснения из кэша). Если их много, увеличь размер кэша или оптимизируй запросы.

2. **Оптимизация памяти**:
    - Кэши используют кучу (heap) узла. Убедись, что heap настроен правильно (обычно 50% от RAM узла, но не более 31 ГБ).
    - Пример настройки heap в `jvm.options`:
      ```bash
      -Xms16g
      -Xmx16g
      ```

3. **Фильтры для кэширования**:
    - Используй `filter context` для условий, которые можно кэшировать (например, `term`, `range`).
    - Пример:
      ```bash
      POST /pets/_search
      {
        "query": {
          "bool": {
            "filter": [
              { "term": { "type": "cat" } },
              { "range": { "age": { "gte": 2 } } }
            ],
            "must": [
              { "match": { "name": "мурзик" } }
            ]
          }
        }
      }
      ```

4. **Избегай ненужного кэширования**:
    - Отключай кэш для разовых запросов или индексов с частыми изменениями:
      ```bash
      POST /pets/_search?request_cache=false
      ```

5. **Оптимизация индекса**:
    - Используй `_forcemerge` для уменьшения числа сегментов, что снижает нагрузку на кэши:
      ```bash
      POST /pets/_forcemerge?max_num_segments=1
      ```
    - Увеличь `refresh_interval` для массовой индексации:
      ```bash
      PUT /pets/_settings
      {
        "refresh_interval": "30s"
      }
      ```

### Итог
- **Кэш запросов**:
    - Хранит битовые множества для фильтров (`term`, `range`).
    - Ускоряет повторяющиеся запросы в filter context.
    - Управляй размером через `indices.queries.cache.size`.
- **Кэш полей**:
    - Используется для агрегаций/сортировки по полям `text`.
    - Предпочитай поля `keyword`, чтобы избежать fielddata.
    - Ограничивай размер через `indices.fielddata.cache.size`.
- **Кэш шардов**:
    - Кэширует результаты запросов и агрегаций на уровне шарда.
    - Включай для неизменяемых индексов, отключай для часто обновляемых.
- **Практика**:
    - Используй фильтры для кэшируемых условий.
    - Применяй `keyword` для агрегаций/сортировки.
    - Мониторь кэши через `_stats` и оптимизируй `refresh_interval` и `_forcemerge`.
      Эти подходы помогут максимально эффективно использовать кэширование в Elasticsearch, ускоряя запросы и снижая нагрузку на кластер!
      

## 16 **Мониторинг**
Мониторинг в **Elasticsearch** — это ключевой процесс для оценки производительности кластера, выявления узких мест и предотвращения сбоев. **Kibana** является основным инструментом для визуализации и анализа метрик, но также можно использовать другие инструменты, такие как Prometheus и Grafana. Анализ метрик, таких как использование CPU, памяти, операций ввода-вывода (I/O) и поисковых задержек, помогает оптимизировать работу кластера.

### Использование Kibana или других инструментов для мониторинга производительности

**Kibana** — это официальный инструмент для визуализации и мониторинга Elasticsearch, который предоставляет удобные дашборды для анализа состояния кластера, узлов, индексов и запросов. Другие инструменты, такие как Prometheus, Grafana или Elastic Stack Monitoring, также могут быть использованы для более гибкого или внешнего мониторинга.

#### a) Мониторинг с помощью Kibana
Kibana интегрируется с Elasticsearch и использует данные из индексов мониторинга (`.monitoring-*`) для построения графиков и дашбордов.

- **Настройка мониторинга в Kibana**:
    1. Убедись, что модуль мониторинга включён в `elasticsearch.yml`:
       ```yaml
       xpack.monitoring.enabled: true
       xpack.monitoring.collection.enabled: true
       ```
    2. В `kibana.yml` включи доступ к мониторингу:
       ```yaml
       xpack.monitoring.ui.enabled: true
       ```
    3. Перезапусти Elasticsearch и Kibana.

- **Основные дашборды в Kibana**:
    - **Overview**: Общее состояние кластера (статус, количество узлов, индексов, шардов).
    - **Nodes**: Метрики по узлам (CPU, память, I/O, JVM heap).
    - **Indices**: Информация об индексах (размер, количество документов, производительность запросов).
    - **Search and Indexing Performance**: Задержки поиска и индексации.

- **Пример использования**:
    1. Открой Kibana → **Stack Monitoring** (в меню слева).
    2. Выбери дашборд **Elasticsearch Overview**:
        - Проверяй **Cluster Status** (green/yellow/red).
        - Анализируй **Search Rate** (количество поисковых запросов в секунду).
        - Следи за **Indexing Rate** (скорость добавления документов).
    3. Перейди в **Nodes** для анализа нагрузки на конкретные узлы (CPU, память, диск).

- **Создание кастомных дашбордов**:
    - Используй **Lens** или **TSVB** (Time Series Visual Builder) для построения графиков на основе метрик.
    - Пример: Создай график для мониторинга задержек поиска:
        1. В Kibana выбери **Visualize** → **Create Visualization** → **Lens**.
        2. Выбери индекс `.monitoring-es-*`.
        3. Добавь метрику `search.query_total` (общее количество поисков) или `search.query_time` (время выполнения).

#### b) Другие инструменты
- **Prometheus + Grafana**:
    - Prometheus собирает метрики через экспортёр (например, `elasticsearch_exporter`).
    - Grafana визуализирует данные в кастомных дашбордах.
    - Установка:
        1. Установи `elasticsearch_exporter`:
           ```bash
           docker run --rm -p 9114:9114 quay.io/prometheuscommunity/elasticsearch-exporter
           ```
        2. Настрой Prometheus для сбора метрик:
           ```yaml
           scrape_configs:
             - job_name: 'elasticsearch'
               static_configs:
                 - targets: ['localhost:9114']
           ```
        3. В Grafana импортируй дашборд для Elasticsearch (например, ID 2322).
    - Преимущества: Гибкость, интеграция с другими системами, долгосрочное хранение метрик.

- **Elastic Stack Monitoring API**:
    - Получай метрики напрямую через API:
      ```bash
      GET /_cluster/health
      GET /_cat/nodes?v&h=name,load_1m,heap.percent,disk.used_percent
      GET /_stats?human
      ```
    - Используй для скриптов автоматизации или интеграции с внешними системами.

- **Другие инструменты**:
    - **Cerebro**: Лёгкий интерфейс для мониторинга узлов и шардов.
    - **ElasticHQ**: Веб-интерфейс для управления и мониторинга кластера.

#### Лучшие практики
- Настрой **Stack Monitoring** в Kibana для быстрого старта.
- Используй Prometheus + Grafana для сложных сценариев или интеграции с другими системами.
- Настрой алерты в Kibana (через **Alerting**) или Grafana для уведомлений о проблемах (например, красный статус кластера).
- Храни метрики мониторинга в отдельном кластере, чтобы не нагружать основной:
  ```yaml
  xpack.monitoring.elasticsearch.url: "http://monitoring-cluster:9200"
  ```


### Анализ метрик (CPU, память, I/O, поисковые задержки)

Для эффективного мониторинга нужно следить за ключевыми метриками, которые влияют на производительность кластера. Вот основные метрики и их интерпретация.

#### a) CPU
- **Что измеряет?** Загрузка процессора на узлах кластера.
- **Как мониторить?**
    - В Kibana: Дашборд **Nodes** → **CPU Usage**.
    - Через API:
      ```bash
      GET /_cat/nodes?v&h=name,load_1m,load_5m,load_15m
      ```
        - `load_1m`, `load_5m`, `load_15m`: Средняя загрузка за 1, 5 и 15 минут.
    - В Prometheus: Метрика `node_cpu_seconds_total`.
- **Норма**:
    - Загрузка CPU < 70–80% в среднем.
    - Пиковые нагрузки до 90% допустимы, но не постоянно.
- **Проблемы и решения**:
    - Высокая загрузка CPU:
        - Проверь запросы с помощью `_profile`:
          ```bash
          POST /pets/_search
          {
            "profile": true,
            "query": { "match": { "name": "кот" } }
          }
          ```
        - Оптимизируй запросы: используй фильтры вместо `match`, избегай `wildcard`/`regexp`.
        - Уменьши количество сегментов с помощью `_forcemerge`:
          ```bash
          POST /pets/_forcemerge?max_num_segments=1
          ```
        - Добавь узлы в кластер для распределения нагрузки.

#### b) Память (JVM Heap)
- **Что измеряет?** Использование кучи JVM (heap) на узлах, где хранятся кэши, метаданные и временные структуры.
- **Как мониторить?**
    - В Kibana: Дашборд **Nodes** → **JVM Heap Usage**.
    - Через API:
      ```bash
      GET /_cat/nodes?v&h=name,heap.percent,heap.max
      ```
        - `heap.percent`: Процент занятой кучи.
        - `heap.max`: Максимальный размер кучи.
    - В Prometheus: Метрика `jvm_memory_bytes_used`.
- **Норма**:
    - Использование кучи < 75%.
    - Частые сборки мусора (GC) указывают на нехватку памяти.
- **Проблемы и решения**:
    - Высокое использование кучи:
        - Увеличь размер кучи в `jvm.options` (до 50% RAM узла, но не более 31 ГБ):
          ```bash
          -Xms16g
          -Xmx16g
          ```
        - Уменьши кэши (query cache, fielddata):
          ```bash
          PUT /_cluster/settings
          {
            "transient": {
              "indices.queries.cache.size": "1%",
              "indices.fielddata.cache.size": "10%"
            }
          }
          ```
        - Используй поля `keyword` вместо `text` для агрегаций, чтобы снизить использование fielddata.
        - Проверь частоту GC:
          ```bash
          GET /_nodes/stats/jvm?human
          ```

#### c) I/O (дисковые операции)
- **Что измеряет?** Скорость чтения/записи на диск (важно для индексации и поиска).
- **Как мониторить?**
    - В Kibana: Дашборд **Nodes** → **Disk I/O**.
    - Через API:
      ```bash
      GET /_cat/nodes?v&h=name,disk.used_percent,disk.io.read_bytes,disk.io.write_bytes
      ```
        - `disk.used_percent`: Процент занятого диска.
        - `disk.io.read_bytes`, `disk.io.write_bytes`: Объём операций чтения/записи.
    - В Prometheus: Метрики `node_disk_read_bytes_total`, `node_disk_write_bytes_total`.
- **Норма**:
    - Диск заполнен < 85% (чтобы избежать отказов).
    - I/O не должен быть узким местом (зависит от типа диска: SSD быстрее HDD).
- **Проблемы и решения**:
    - Высокая нагрузка на I/O:
        - Увеличь `refresh_interval` для снижения частоты записи:
          ```bash
          PUT /pets/_settings
          {
            "refresh_interval": "30s"
          }
          ```
        - Выполни `_forcemerge` для уменьшения числа сегментов:
          ```bash
          POST /pets/_forcemerge?max_num_segments=1
          ```
        - Используй SSD вместо HDD для лучшей производительности.
    - Нехватка места:
        - Удали старые индексы:
          ```bash
          DELETE /old_index
          ```
        - Перемести индексы на узлы с большим объёмом диска:
          ```bash
          POST /_cluster/reroute
          {
            "commands": [
              {
                "move": {
                  "index": "pets",
                  "shard": 0,
                  "from_node": "node1",
                  "to_node": "node2"
                }
              }
            ]
          }
          ```

#### d) Поисковые задержки
- **Что измеряет?** Время выполнения поисковых запросов и агрегаций.
- **Как мониторить?**
    - В Kibana: Дашборд **Search and Indexing Performance** → **Search Latency**.
    - Через API:
      ```bash
      GET /_stats/search?human
      ```
        - `query_total`: Общее количество запросов.
        - `query_time_in_millis`: Общее время выполнения.
        - Средняя задержка = `query_time_in_millis / query_total`.
    - В Prometheus: Метрика `elasticsearch_indices_search_query_time_seconds`.
- **Норма**:
    - Задержки поиска < 100–500 мс для большинства запросов.
    - Зависит от размера индекса и сложности запросов.
- **Проблемы и решения**:
    - Высокие задержки:
        - Оптимизируй запросы: используй `filter` вместо `match`, избегай `wildcard`/`regexp`:
          ```bash
          POST /pets/_search
          {
            "query": {
              "bool": {
                "filter": [
                  { "term": { "type": "cat" } }
                ]
              }
            }
          }
          ```
        - Увеличь количество реплик для распределения нагрузки:
          ```bash
          PUT /pets/_settings
          {
            "number_of_replicas": 2
          }
          ```
        - Профилируй запросы с `_profile`:
          ```bash
          POST /pets/_search
          {
            "profile": true,
            "query": { "match": { "name": "кот" } }
          }
          ```
        - Уменьши количество сегментов с помощью `_forcemerge`.
    
### Итог
- **Мониторинг с Kibana**:
    - Используй **Stack Monitoring** для дашбордов (Overview, Nodes, Indices).
    - Создавай кастомные визуализации с **Lens** или **TSVB**.
    - Настрой алерты для критических метрик.
- **Другие инструменты**:
    - Prometheus + Grafana для гибкости и интеграции.
    - Cerebro или ElasticHQ для простого мониторинга.
- **Анализ метрик**:
    - **CPU**: < 70–80%, оптимизируй запросы или добавь узлы.
    - **Память**: Heap < 75%, используй `keyword` вместо `text`.
    - **I/O**: Диск < 85%, увеличивай `refresh_interval` или используй SSD.
    - **Поисковые задержки**: < 500 мс, профилируй с `_profile` и оптимизируй фильтры.
      Эти подходы помогут поддерживать кластер в здоровом состоянии и быстро реагировать на проблемы!
      

## 17 Экосистема Elastic Stack
**Elastic Stack** — это мощная экосистема инструментов, центром которой является **Elasticsearch**, дополненная **Kibana**, **Logstash**, **Beats**, **Elastic APM** и возможностями **Machine Learning**. Эти компоненты позволяют собирать, хранить, анализировать и визуализировать данные, а также мониторить производительность приложений и выявлять аномалии. 

### Kibana: Инструмент для визуализации данных и управления Elasticsearch

**Kibana** — это веб-интерфейс для работы с Elasticsearch, который предоставляет инструменты для визуализации данных, создания дашбордов, выполнения поисковых запросов и мониторинга кластера.

#### a) Создание дашбордов, визуализаций и поисковых запросов
- **Визуализации**:
    - Kibana поддерживает графики, таблицы, карты и другие типы визуализаций через инструменты **Lens**, **TSVB** (Time Series Visual Builder) и **Vega**.
    - Пример: Создание графика количества поисковых запросов по времени:
        1. В Kibana → **Visualize** → **Create Visualization** → **Lens**.
        2. Выбери индекс `.monitoring-es-*`.
        3. Настрой метрику `Count` по полю `search.query_total` и разбивку по времени (`@timestamp`).
        4. Сохрани визуализацию как "Search Rate".

- **Дашборды**:
    - Дашборды объединяют несколько визуализаций для комплексного мониторинга.
    - Пример: Создание дашборда для мониторинга Elasticsearch:
        1. В Kibana → **Dashboard** → **Create Dashboard**.
        2. Добавь визуализации: "Search Rate", "CPU Usage", "Heap Usage".
        3. Настрой фильтры (например, по узлу или индексу) и сохрани дашборд.

- **Поисковые запросы**:
    - Используй **Discover** для интерактивного поиска по данным.
    - Пример: Найти все документы в индексе `pets`, где `type: "cat"`:
        1. В Kibana → **Discover**.
        2. Выбери индекс `pets`.
        3. Введи запрос `type:cat` в строку поиска.
        4. Сохрани запрос для использования в дашбордах.

#### b) Использование Dev Tools для тестирования запросов
- **Dev Tools** — это консоль в Kibana для отправки запросов к Elasticsearch API.
- Пример: Тестирование запроса для поиска котов старше 2 лет:
  ```bash
  POST /pets/_search
  {
    "query": {
      "bool": {
        "filter": [
          { "term": { "type": "cat" } },
          { "range": { "age": { "gte": 2 } } }
        ]
      }
    }
  }
  ```
- **Автодополнение**: Dev Tools поддерживает автодополнение для синтаксиса Query DSL.
- **Массовая отправка**: Можно выполнять несколько запросов подряд:
  ```bash
  GET /_cat/indices?v
  GET /pets/_mapping
  ```

- **Лучшие практики**:
    - Тестируй сложные запросы в Dev Tools перед внедрением в код.
    - Используй `_explain` для анализа, почему документ попал в выборку:
      ```bash
      GET /pets/_explain/1
      {
        "query": { "match": { "name": "кот" } }
      }
      ```
    - Сохраняй часто используемые запросы в **Console Snippets** для повторного использования.

#### Лучшие практики для Kibana
- Используй **Lens** для быстрого создания визуализаций, **Vega** — для сложных кастомных графиков.
- Настрой **алерты** в Kibana для уведомлений о проблемах (например, CPU > 80%):
  ```bash
  Kibana → Alerting → Create Rule → Metric Threshold
  ```
- Ограничивай доступ через **Spaces** и **Roles** для защиты данных:
  ```bash
  Kibana → Management → Security → Roles
  ```
- Храни метрики мониторинга в отдельном кластере для снижения нагрузки.

### Logstash и Beats: Сбор, обработка и отправка данных в Elasticsearch

**Logstash** и **Beats** — это инструменты для сбора, обработки и передачи данных в Elasticsearch. Они используются для работы с логами, метриками, событиями и другими типами данных.

#### a) Logstash
- **Что это?** ETL-инструмент (Extract, Transform, Load) для обработки данных из различных источников (логи, базы данных, Kafka) и отправки в Elasticsearch или другие системы.
- **Компоненты**:
    - **Input**: Источник данных (файлы, HTTP, Kafka).
    - **Filter**: Обработка данных (парсинг, преобразование, обогащение).
    - **Output**: Назначение (Elasticsearch, файл, stdout).

- **Пример конфигурации**:
    - Обработка логов приложения и отправка в Elasticsearch:
      ```logstash
      input {
        file {
          path => "/var/log/app.log"
          start_position => "beginning"
        }
      }
      filter {
        grok {
          match => { "message": "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:loglevel} %{GREEDYDATA:msg}" }
        }
        date {
          match => ["timestamp", "ISO8601"]
        }
      }
      output {
        elasticsearch {
          hosts => ["http://localhost:9200"]
          index => "app-logs-%{+YYYY.MM.dd}"
        }
        stdout { codec => rubydebug }
      }
      ```
    - **Объяснение**:
        - `input`: Читает логи из файла `/var/log/app.log`.
        - `filter`: Парсит логи с помощью `grok` (разбивает на поля `timestamp`, `loglevel`, `msg`) и преобразует `timestamp` в дату.
        - `output`: Отправляет данные в индекс `app-logs-YYYY.MM.dd` и выводит в консоль.

- **Запуск Logstash**:
  ```bash
  bin/logstash -f config/app-logstash.conf
  ```

#### b) Beats
- **Что это?** Лёгкие агенты для сбора данных (логи, метрики, сетевой трафик) и отправки в Elasticsearch или Logstash.
- **Основные модули**:
    - **Filebeat**: Сбор логов из файлов.
    - **Metricbeat**: Сбор системных метрик (CPU, память, диски).
    - **Packetbeat**: Анализ сетевого трафика.
    - **Winlogbeat**: Сбор событий Windows.
    - **Auditbeat**: Мониторинг аудита и безопасности.

- **Пример: Настройка Filebeat для логов**:
    1. Установи Filebeat и настрой `filebeat.yml`:
       ```yaml
       filebeat.inputs:
       - type: log
         paths:
           - /var/log/app.log
       output.elasticsearch:
         hosts: ["http://localhost:9200"]
         index: "app-logs-%{+YYYY.MM.dd}"
       ```
    2. Запусти Filebeat:
       ```bash
       ./filebeat -e
       ```
    - **Эффект**: Filebeat отправляет логи в Elasticsearch, создавая индексы `app-logs-2025.07.10`.

- **Пример: Metricbeat для системных метрик**:
  ```yaml
  metricbeat.modules:
  - module: system
    metricsets: ["cpu", "memory", "diskio"]
    period: 10s
  output.elasticsearch:
    hosts: ["http://localhost:9200"]
    index: "metricbeat-%{+YYYY.MM.dd}"
  ```
    - Запусти:
      ```bash
      ./metricbeat -e
      ```
    - В Kibana создай дашборд для визуализации CPU и памяти.

#### Logstash vs Beats
- **Logstash**: Для сложной обработки данных (парсинг, обогащение, агрегация). Более тяжёлый, требует больше ресурсов.
- **Beats**: Лёгкие агенты для прямого сбора данных. Меньше возможностей обработки, но проще в настройке.
- **Комбинированный подход**:
    - Beats → Logstash → Elasticsearch:
      ```yaml
      # filebeat.yml
      output.logstash:
        hosts: ["localhost:5044"]
      ```
      ```logstash
      # logstash.conf
      input {
        beats { port => 5044 }
      }
      output {
        elasticsearch { hosts => ["http://localhost:9200"] }
      }
      ```

#### Лучшие практики
- Используй **Beats** для лёгкого сбора данных, **Logstash** — для сложной обработки.
- Настрой **индексы с временными метками** (`index: "logs-%{+YYYY.MM.dd}"`) для упрощения управления данными.
- Включи **ILM (Index Lifecycle Management)** для автоматического удаления старых индексов:
  ```bash
  PUT /_ilm/policy/logs_policy
  {
    "policy": {
      "phases": {
        "delete": {
          "min_age": "30d",
          "actions": {
            "delete": {}
          }
        }
      }
    }
  }
  ```
- Тестируй конфигурации Logstash с помощью `bin/logstash -f config.conf --config.test_and_exit`.

### 3. Elastic APM: Мониторинг производительности приложений

**Elastic APM** (Application Performance Monitoring) — это инструмент для мониторинга производительности приложений, отслеживания запросов, ошибок и задержек. Он интегрируется с приложениями через агентов и отправляет данные в Elasticsearch.

#### Как работает?
- **APM-агенты**: Библиотеки для популярных языков (Java, Python, Node.js, Go, .NET и др.), которые собирают метрики (время запросов, ошибки, транзакции).
- **APM-сервер**: Принимает данные от агентов и отправляет их в Elasticsearch.
- **Kibana**: Визуализирует метрики через дашборд APM.

#### Настройка Elastic APM
1. **Установи APM-сервер**:
    - Скачай и настрой `apm-server.yml`:
      ```yaml
      apm-server.host: "0.0.0.0:8200"
      output.elasticsearch:
        hosts: ["http://localhost:9200"]
      ```
    - Запусти:
      ```bash
      ./apm-server -e
      ```

2. **Добавь агента в приложение** (пример для Python/Flask):
    - Установи библиотеку:
      ```bash
      pip install elastic-apm[flask]
      ```
    - Настрой приложение:
      ```python
      from flask import Flask
      from elastic_apm.contrib.flask import ElasticAPM
      app = Flask(__name__)
      apm = ElasticAPM(app, server_url='http://localhost:8200', service_name='my-flask-app')
      ```
    - Запусти приложение, и данные начнут поступать в APM.

3. **Визуализация в Kibana**:
    - В Kibana → **APM**:
        - Просматривай **Services** (список приложений).
        - Анализируй **Transactions** (время выполнения запросов).
        - Отслеживай **Errors** (логи ошибок).
    - Пример: График задержек запросов к `/api/users`.

#### Пример метрик
- **Транзакции**: Время выполнения HTTP-запроса `/api/users` (200 мс).
- **Ошибки**: Стек вызовов для исключения `NullPointerException`.
- **Зависимости**: Задержки при вызове внешнего сервиса (например, базы данных).

#### Лучшие практики
- Используй APM для критических приложений, чтобы отслеживать узкие места.
- Настрой **алерты** в Kibana для уведомлений о высоких задержках или частых ошибках.
- Минимизируй объём собираемых данных, чтобы снизить нагрузку:
  ```yaml
  apm-server:
    transaction_sample_rate: 0.1 # 10% транзакций
  ```
- Интегрируй с **Distributed Tracing** для анализа цепочек вызовов в микросервисах.

### Machine Learning: Анализ аномалий и прогнозирование

**Machine Learning** (ML) в Elastic Stack — это встроенные алгоритмы для анализа данных, выявления аномалий, прогнозирования и классификации. Требуется лицензия Platinum или выше (или пробная версия).

#### Основные возможности
- **Анализ аномалий**: Обнаружение необычного поведения (например, всплесков запросов или ошибок).
- **Прогнозирование**: Предсказание временных рядов (например, будущей нагрузки на сервер).
- **Классификация и регрессия**: Для задач, таких как определение спама или прогнозирование значений.

#### Настройка ML
1. **Включение ML**:
    - Убедись, что ML включён в `elasticsearch.yml`:
      ```yaml
      xpack.ml.enabled: true
      ```
    - Перезапусти кластер.

2. **Создание ML-задания** (пример: анализ аномалий в логах):
    - В Kibana → **Machine Learning** → **Anomaly Detection** → **Create Job**.
    - Выбери индекс `app-logs-*`.
    - Настрой:
        - Детектор: `count` (подсчёт событий).
        - Временной интервал: `15m` (анализ каждые 15 минут).
        - Поле влияния: `loglevel` (группировка по уровню логов).
    - Запусти задание и просмотри результаты в **Anomaly Explorer**.

3. **Пример: Обнаружение аномалий**:
    - Допустим, в логах обычно 100 событий в час, но вдруг их стало 1000.
    - ML выдаст аномалию с высоким **anomaly score** (0–100).
    - В Kibana → **Anomaly Explorer**:
        - Увидишь всплеск на графике.
        - Поле `loglevel: ERROR` покажет, что аномалия связана с ошибками.

4. **Прогнозирование**:
    - Пример: Прогноз поисковых запросов на основе исторических данных.
    - В Kibana → **Machine Learning** → **Data Frame Analytics** → **Create Job** → **Regression**.
    - Выбери индекс `.monitoring-es-*`, поле `search.query_total` для прогнозирования.
    - Результат: Прогноз на следующие 24 часа.

#### Пример ML-запроса через API
- Создание задания для аномалий:
  ```bash
  PUT /_ml/anomaly_detectors/log_anomalies
  {
    "analysis_config": {
      "bucket_span": "15m",
      "detectors": [
        {
          "function": "count",
          "by_field_name": "loglevel"
        }
      ]
    },
    "data_description": { "time_field": "@timestamp" }
  }
  ```
- Запуск задания:
  ```bash
  POST /_ml/anomaly_detectors/log_anomalies/_open
  ```

#### Лучшие практики
- Используй ML для анализа временных рядов (логи, метрики, APM-данные).
- Настрой **bucket_span** в зависимости от данных (например, `15m` для логов, `1h` для метрик).
- Ограничивай количество детекторов и полей влияния для снижения нагрузки.
- Интегрируй ML с **Alerting** для уведомлений об аномалиях:
  ```bash
  Kibana → Alerting → Create Rule → Anomaly Detection
  ```
- Тестируй ML на исторических данных перед продакшеном.


### Итог
- **Kibana**:
    - Создавай визуализации с **Lens** и дашборды для мониторинга.
    - Используй **Dev Tools** для тестирования запросов и **Discover** для поиска.
- **Logstash и Beats**:
    - **Beats** (Filebeat, Metricbeat) для лёгкого сбора данных.
    - **Logstash** для сложной обработки (парсинг, обогащение).
    - Комбинируй для гибкости.
- **Elastic APM**:
    - Мониторинг транзакций, ошибок и задержек приложений.
    - Интегрируй с Kibana для визуализации и алертов.
- **Machine Learning**:
    - Анализ аномалий и прогнозирование временных рядов.
    - Используй для логов, метрик и APM-данных.
      Эти компоненты делают Elastic Stack универсальным решением для логирования, мониторинга, аналитики и визуализации данных!
      
### 18 Практическое использование
- **Реальные сценарии**:
    - Логи (например, анализ логов с помощью ELK Stack).
    - Поиск по сайту (e-commerce, контентные платформы).
    - Аналитика больших данных (агрегации, временные ряды).
- **Интеграция**:
    - Работа с Elasticsearch через клиентские библиотеки (Java, Python, Node.js).
    - Интеграция с другими системами (Kafka, Spark, Hadoop).
- **Тестирование**:
    - Написание тестов для проверки маппингов, запросов и производительности.
    - Использование инструментов, таких как Rally, для нагрузочного тестирования.

### 19 Отказоустойчивость и масштабирование
- **Резервное копирование и восстановление**:
    - Настройка снапшотов и восстановления через Snapshot API.
- **Масштабирование**:
    - Горизонтальное масштабирование (добавление узлов).
    - Вертикальное масштабирование (увеличение ресурсов узлов).
- **Обработка сбоев**:
    - Настройка автоматического восстановления шардов.
    - Управление отказоустойчивостью через реплики.